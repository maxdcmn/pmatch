"name","email","title","research_area","profile_url","top_abstract"
"Monica Nordberg","","","Computer Science","https://openalex.org/A5006834808","[""Cadmium is widely recognized as an important environmental toxicant that may give rise to kidney dysfunction, bone disease, and cancer in humans and animals. Kidney dysfunction occurs at very low exposures and is often considered as the most sensitive or critical effect. Cadmium exposures of concern occur in many countries. In low- and middle-income countries with small-scale mining, excessive exposure to cadmium and other metals occurs in occupational and environmental settings. This is of particular importance in view of the growing demand for metals in global climate change mitigation. Since the 1970s, the present authors have contributed evidence concerning the role of metallothionein and other factors in influencing the toxicokinetics and toxicity of cadmium, particularly as it relates to the development of adverse effects on kidneys in humans and animals. The findings gave a background to the development of biomarkers employed in epidemiological studies, demonstrating the important role of metallothionein in protection against cadmium-induced kidney dysfunction in humans. Studies in cadmium-exposed population groups demonstrated how biomarkers of kidney dysfunction changed during 8 years after drastic lowering of environmental cadmium exposure. Other epidemiological studies showed the impact of a good zinc status in lowering the prevalence of cadmium-related kidney dysfunction. Increased susceptibility to Cd-induced kidney dysfunction was shown in a population with high exposure to inorganic arsenic when compared with a group with low such exposure. Several national and international organizations have used part of the reviewed information, but the metallothionein-related biomarkers and the interaction effects have not been fully considered. We hope that these data sets will also be included and improve risk assessments and preventive measures."", ""Biomarkers or biological indicators of metal-induced renal toxicity have been employed for several decades in monitoring the early clinical effects of nephrotoxic metals such as lead (Pb), cadmium (Cd), and mercury (Hg). These indicators, which are early biochemical tests for assessing the biochemical responses of the kidney to these metals prior to overt clinical disease, have proven useful for indicating not only metal exposure, but that a sufficient quantity of a given metal is biologically available to produce a cellular response. In order to be really useful, a good biomarker must be highly sensitive, easily measurable, relatively chemical-specific, and interpretable as to whether it is an index of exposure or toxic effect. A major research need is to provide correlative morphological/biochemical data so that the prognostic value to a given biomarker may be determined (Fowler, 1983)."", ""More than one and a half centuries ago, adverse human health effects were reported after use of a cadmium-containing silver polishing agent. Long-term cadmium exposure gives rise to kidney or bone disease, reproductive toxicity and cancer in animals and humans. At present, high human exposures to cadmium occur in small-scale mining, underlining the need for preventive measures. This is particularly urgent in view of the growing demand for minerals and metals in global climate change mitigation. This review deals with a specific part of cadmium toxicology that is important for understanding when toxic effects appear and, thus, is crucial for risk assessment. The discovery of the low-molecular-weight protein metallothionein (MT) in 1957 was an important milestone because, when this protein binds cadmium, it modifies cellular cadmium toxicity. The present authors contributed evidence in the 1970s concerning cadmium binding to MT and synthesis of the protein in tissues. We showed that binding of cadmium to metallothionein in tissues prevented some toxic effects, but that metallothionein can increase the transport of cadmium to the kidneys. Special studies showed the importance of the Cd/Zn ratio in MT for expression of toxicity in the kidneys. We also developed models of cadmium toxicokinetics based on our MT-related findings. This model combined with estimates of tissue levels giving rise to toxicity, made it possible to calculate expected risks in relation to exposure. Other scientists developed these models further and international organizations have successfully used these amended models in recent publications. Our contributions in recent decades included studies in humans of MT-related biomarkers showing the importance of MT gene expression in lymphocytes and MT autoantibodies for risks of Cd-related adverse effects in cadmium-exposed population groups. In a study of the impact of zinc status on the risk of kidney dysfunction in a cadmium-exposed group, the risks were low when zinc status was good and high when zinc status was poor. The present review summarizes this evidence in a risk assessment context and calls for its application in order to improve preventive measures against adverse effects of cadmium exposures in humans and animals.""]"
"Uwe Windhorst","","","Computer Science","https://openalex.org/A5053138224","[""As if the anatomical architecture of the nociceptive and pain system were not complex enough, a huge variety of additional neurotransmitters, neuromodulators and hormones adds on to it. This new dimension will be treated here. The review is structured from two complementary angles: first, the anatomical and neurophysiological perspective, covering key structures from peripheral nociceptors to central brain regions such as the cortex, basal ganglia, hypothalamus, and various brainstem nuclei; and second, the molecular and biochemical perspective, outlining the array of neuroactive substances – including neuropeptides, classical neurotransmitters, and neuromodulators – involved in nociception and acute pain modulation. In selected sections, appropriate case reports are presented to illustrate specific mechanisms or phenomena, and at the end, some clinical syndromes are mentioned to link basic concepts with clinical relevance. Sub-cellular processes and therapeutic approaches are beyond the scope of this review."", ""Pain is – a pain in the neck, isn´t it? Yes and no. Yes, it hurts. Yet, it helps. At least do acute and transient pain. Acute pain resulting from an acute event is a fundamental condition for survival, insofar as it warns against imminent or actual tissue damage, a potentially life-endangering threat. The importance of the physiological, protective role of nociceptive pain is underscored by cases, in which a failure to sense pain such as in the case of congenital insensitivity often leads to self-mutilation, bone fractures, joint deformities, amputations, and even early death. While the goal of acute pain based on nociception thus appears to be clear, its implementation is anything but that because to achieve this goal calls for a number of requirements to be fulfilled. The first is the identification of a noxious stimulus, including its intensity and location on the body surface or within the body. The second is the orchestration of counter-measures, including arousal, emotional and various motor reactions. The third is the mobilization of the required energy as well as cardio-vascular and respiratory responses. All this implies multi-dimensional activations of diverse neural and neuro-muscular systems. This review attempts to describe the structures and mechanisms underlying nociception and pain in quite some detail to emphasize their complexity. It starts with a structural description of the nociceptive and pain system in an ascending order, from peripheral nociceptors to supraspinal structures involved in nociceptive and pain processing. This is followed by a description of the systems organizing descending pain modulation. The focus will here be on acute pain. It turns out that even acute nociception and pain and the underlying neural systems are very complex. There are many reasons for this complexity. First, many neuronal nodes receive multifarious inputs and send multiple outputs to other nodes, which often have additional functions other than nociception and pain. This constitutes an extended, multi-functional, multiple input-multiple output network. Second, individual nodes often have an inhomogeneous structure chracterized by diverse neuron groups and inter-connections. Third, sub-cellular processes are complex, but will not be treated here. It comes as no surprise, then, that we face difficulties in dealing with pain and its clinical consequences, and find appropriate treatments. It will not suffice to manipulate a single screw or only a few."", ""An untrained couch potato involuntarily forced to undergo a long strenuous exercise towards exhaustion will feel it at the end as a discomforting fatigue. A well-trained athlete will feel the same but after a longer time. Patients of various neurological diseases will do so much earlier. All may also finally experience muscle pain and soreness the next day.Muscle fatigue is an exercise-induced reduction in maximal voluntary muscle force. Muscle fatigability varies with muscle use, age and sex. It is commonly divided into two broad categories: peripheral and central fatigue. The first refers to the many processes in the fatiguing muscle(s), the second to subsequent processes in the central nervous system (CNS). Peripheral muscle fatigue can be caused by numerous different mechanisms, ranging from the accumulation of metabolites within muscle fibers to their damage. Central fatigue involves the inadequacy of the CNS to generate and maintain sufficiently strong motor command. There must of course be a mediator relating the two fatigue types, and this is the collection of small-diameter group III (A&amp;delta;) and IV (C) nerve fibers emanating from the muscle and distributing their activation effects throughout the CNS.Muscle fatigue develops during eventually exhausting exercise that goes along with activations of the cardio-vascular and respiratory system to supply oxygen and energy resources. While these functions are supported by a number of sensory afferents, fatigue-activated group III/IV muscle afferents have been proposed to contribute their share to cardio-vascular and ventilatory reflex responses that are mediated in the brainstem. Neural feedback from working skeletal muscle is a vital component in providing a high capacity for endurance exercise because muscle perfusion and O₂ delivery determine the fatigability of skeletal muscle (Amann 2012).The development of chronic muscle fatigue is related to muscle wasting mediated by aging, myopathies, muscle dystrophies, immobilization, insulin resistance, diseases associated with systemic inflammation (arthritis, sepsis, infections), trauma, cardio-vascular (heart failure) and respiratory disorders (chronic obstructive pulmonary disease (COPD)), chronic kidney failure, multiple sclerosis (MS), and, more recently, coronavirus disease 2019 (COVID-19)."", ""The term `motor control&amp;acute; encompasses a wide range of mechanisms thought to be implicated in the organization of movements. Their study, like that of any other scientific field, must use specific notions to get to grips with them. This review is therefore organized along a series of notions that are frequently used in motor control, and we will discuss them with particular emphasis on neurological conditions, which may disrupt normal motor functioning. We will start with a short description of the roles of space and time, in which movements take place. Subsequently we will deal with kinematics and kinetics (dynamics) of movements. Then, we will list the inputs to motoneurons (MNs), which, in different forms, convey signals from the central nervous system (CNS) to skeletal muscles and muscle spindles, including central pattern generators (CPGs), sensory inputs and supraspinal descending fiber systems. Some helpers in movement organization will be introduced, such as internal models and neuronal network models. A relatively brief section on sensory-motor learning will follow because learning abilities are required to endow the sensory-motor system with flexibility and adaptability. The major emphasis is placed on neurological processes, which result from genetic modifications, autoimmune diseases, neurodegenerative diseases, infections in the nervous system as well as vascular and traumatic lesions. These neurological diseases lead to a variety of impairments and symptoms, particularly movement derangements including cerebellar ataxia, spasticity, and (other) movement disorders such as Parkinson&amp;rsquo;s syndromes. The conclusion is that there is no unique top controller that controls &amp;acute;motor control`."", ""In the past, the spinal cord was considered a hard-wired network responsible for spinal reflexes and a conduit for long-range connections. This view has changed dramatically over the past few decades. It is now recognized as a plastic structure that has the potential to adapt to changing environments. While such changes occur under physiological conditions, the most dramatic alterations take place in response to pathological events. Many of the changes that occur following such pathological events are maladaptive, but some appear to help adapt to the new conditions. Although a number of studies have been devoted to elucidating the underlying mechanisms, in humans and animal models, the etiology and pathophysiology of various diseases impacting the spinal cord are still not well understood. In this review, we summarize current understanding and outstanding challenges for a number of diseases, including spinal muscular atrophy (SMA), amyotrophic laterals sclerosis (ALS), and spinal cord injury (SCI), with occasional relations to stroke. In particular, we focus on changes resulting from SCI (and stroke), and various influencing factors such as cause, site and extent of the afflicted damage.""]"
"Jian Zhang","","","Computer Science","https://openalex.org/A5100410082","[""The photocatalytic reduction of carbon dioxide (CO2) to chemicals holds significant importance for mitigating the current energy crisis. Rational design of catalytic centers within well-defined structures can effectively enhance the reaction activity and selectivity. In this study, we constructed interrupted zeolitic boron imidazolate frameworks (BIFs) featuring unsaturated coordination at the central Co ion. The gas adsorption measurement results indicate that BIF-92-Co(FA) modified with terminal formate ligand exhibits excellent stability and high porosity. In the photocatalytic CO2 reduction reaction, BIF-92-Co(FA) achieved a CO production rate of 3866.5 μmol g-1 h-1 and a selectivity of 87.8% for CO as the primary product. This work paves the way for the design and development of efficient catalysts through unsaturated metal sites."", ""Abstract We propose a metal-dielectric hybrid photonic crystal (PhC) slab for generating orbital angular momentum (OAM) vortex beams. The approach exploits multiple bound states in the continuum (BICs) supported by the PhC slab, which serve as polarization vortex center in momentum space. These vortices carry distinct topological charges, leading to the generation of diverse OAM modes. Unlike conventional spiral phase plates, our design scheme is composed of double-layer periodic units and without a distinct optical geometric center. This PhC slab exhibits insensitivity to the position of the incident wave and is conducive to the flexible configuration of the OAM generator. We further investigate a compact OAM antenna system based on the proposed PhC slab lens. Simulation and experimental results demonstrate that OAM modes with topological charges l=-2 (6.5–7.0 GHz) and l=+2 (5.7–6.0 GHz) are generated as a right-handed circularly polarized (RCP) wave is incident on the PhC slab, The OAM mode purity exceeds 95%. Due to the symmetry of the system, OAM modes with opposite topological charges can be achieved for a left-handed circularly polarized (LCP) incident wave. The proposed approach expands the application of OAM vortex beam via BICs."", ""Abstract A high‐density green body is critical for producing fine‐grained ceramics at low sintering temperatures. However, achieving high density from submicron powders is challenging due to their high specific surface area. In this study, an alumina slurry using a super‐fine α‐Al 2 O 3 powder ( d 50 = 180 nm) and a low molecular weight dispersant (CE‐64) was prepared with a high solid loading of 55 vol% and a low viscosity of 0.40 Pa·s. The slurry was post‐treated by pressure filtration at 0.4 MPa. After de‐binding at 750°C, the resulting green bodies exhibited a relative density of 67.4%, 0.9% higher than that of the counterparts formed without pressure filtration. Pressureless sintering at 1300°C in air yielded ceramics with a relative density of 99.97%, an average grain size of 0.70 µm, a flexural strength of 631 MPa, and a hardness of 18.5 GPa. These findings demonstrate the feasibility of achieving high‐performance alumina ceramics with fine‐grained microstructures under simplified sintering conditions through the combined use of high solid loading slurry and low‐pressure filtration forming.""]"
"O. Smirnova","","","Computer Science","https://openalex.org/A5022564127","[""The article updates the problem of maintaining inflation at a consistently low level. The subjects of management of this process have been identified. &#x0D; The study provides price formation mechanisms using the example of the construction industry. The dynamics of the cost of basic materials used in construction is presented and described. Using these examples from the construction industry, the main drivers of local inflation are identified. &#x0D; The concept of “cost-push inflation scissors” is considered. Some features and prerequisites for inflation and its impact on the economic security of the country's macroeconomic system have been identified. &#x0D; The meaning of effective demand and its role in ensuring the economic security of the macroeconomic system of the state is clarified. The mechanism of inflation targeting is described. Conclusions are drawn about possible scenarios for combating inflation and opportunities to minimize the consequences of its impact. The mechanisms and factors influencing demand and inflation are analyzed: quantitative easing, current accounts. An assessment of the issue of means of payment by the Federal Reserve System is given."", ""A time projection chamber (TPC) with micropattern gaseous detector (MPGD) readout is investigated as main tracking device of the International Large Detector (ILD) concept at the planned International Linear Collider (ILC). A prototype TPC equipped with a triple gas electron multiplier (GEM) readout has been built and operated in an electron test beam. The TPC was placed in a 1 T solenoidal field at the DESY II Test Beam Facility, which provides an electron beam up to 6 GeV/c. The performance of the readout modules, in particular the spatial point resolution, is determined and compared to earlier tests. New studies are presented with first results on the separation of close-by tracks and the capability of the system to measure the specific energy loss dE/dx. This is complemented by a simulation study on the optimization of the readout granularity to improve particle identification by dE/dx."", ""Full detector simulation is known to consume a large proportion of computing resources available to the LHC experiments, and reducing time consumed by simulation will allow for more profound physics studies. There are many avenues to exploit, and in this work we investigate those that do not require changes in the GEANT4 simulation suite. In this study, several factors affecting the full GEANT4 simulation execution time are investigated. A broad range of configurations has been tested to ensure consistency of physical results. The effect of a single dynamic library GEANT4 build type has been investigated and the impact of different primary particles at different energies has been evaluated using GDML and GeoModel geometries. Some configurations have an impact on the physics results and are, therefore, excluded from further analysis. Usage of the single dynamic library is shown to increase execution time and does not represent a viable option for optimization. Lastly, the static build type is confirmed as the most effective method to reduce the simulation execution time."", ""A review article on investment vehicles for companies in the modern digital economy. The article presents the main financial instruments available to organizations that allow them to invest free funds to generate additional income. Deposits, bonds, Federal loan bonds, stocks, mutual funds, ETFs and REITs are considered. Their main parameters are analyzed from the point of view of applied use, as well as the possibility of implementing transactions for the placement of funds in these instruments through digital channels. The article provides up-to-date quotes for rates and profitability parameters of instruments. Different banks and their terms of placement are compared on deposits. The main idea of the article is to convey the need to study, select and use specific tools in accordance with the objectives of the organization in order to obtain maximum additional income.""]"
"R. Gonzalez Suarez","","","Computer Science","https://openalex.org/A5106331204","[""Abstract The CODEX- β apparatus is a demonstrator for the proposed future CODEX-b experiment, a long-lived-particle detector foreseen for operation at IP8 during HL-LHC data-taking. The demonstrator project, intended to collect data in 2025, is described, with a particular focus on the design, construction, and installation of the new apparatus."", ""A bstract This paper investigates the search for long-lived dark scalars from exotic Higgs boson decays at the Future Circular Collider in its e + e − stage, FCC-ee, considering an integrated luminosity of 10 . 8 ab −1 collected during the ZH run at a center-of-mass energy $$ \\sqrt{s} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> </mml:math> = 240 GeV. The work considers Zh events where the Z boson decays leptonically and the Higgs boson h decays into two long-lived dark scalars s which further decay into bottom anti-bottom quark pairs. The analysis is performed using a parametrized simulation of the IDEA detector concept and targets dark scalar decays in the tracking volume, resulting in multiple displaced vertices in the final state. The sensitivity towards long-lived dark scalars at FCC-ee is estimated using an event selection requiring two opposite-charge, same-flavor leptons compatible with the Z boson, and at least two displaced vertices in the final state. The selection is seen to efficiently remove the Standard Model background, while retaining sensitivity for dark scalar masses between m s = 20 GeV and m s = 60 GeV and mean proper lifetimes cτ between approximately 10 mm and 10 m. The results show that the search strategy has potential to probe Higgs to dark scalar branching ratios as low as 10 −4 for a mean proper lifetime cτ ≈ 1 m. The results provide the first sensitivity estimate for exotic Higgs decays at FCC-ee with the IDEA detector concept, using the common FCC framework."", ""This study presents a novel method for the definition of signal regions in searches for new physics at collider experiments. By leveraging multi-dimensional histograms with precise arithmetic and utilizing the SparkDensityTree library, it is possible to identify high-density regions within the available phase space, potentially improving sensitivity to very small signals. Inspired by a search for dark mesons at the ATLAS experiment, CMS open data is used for this proof-of-concept intentionally targeting an already excluded signal. Signal regions are defined based on density estimates of signal and background. These preliminary regions align well with the physical properties of the signal while effectively rejecting background events."", ""We examine the possibility to detect new SM-neutral vector bosons (<a:math xmlns:a=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><a:msup><a:mi>Z</a:mi><a:mo>′</a:mo></a:msup></a:math>) that couple exclusively to leptons in the electron-positron mode of the Future Circular Collider (FCC-ee). Focusing on the <c:math xmlns:c=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><c:msup><c:mi>Z</c:mi><c:mo>′</c:mo></c:msup></c:math> production with a radiated photon search channel, we show that the FCC-ee can significantly extend the unprobed parameter space by increasing the exclusion in the coupling by one to two orders of magnitude in the kinematically allowed mass range (from 10 to 365 GeV), with the leading sensitivity being driven by the muon channel. In doing so, it outperforms other proposed lepton collider options such as CLIC and ILC in this range of masses. Further, we discuss the possibility of improving the sensitivity of the FCC-ee to this model through the modification of the dilepton invariant mass resolution and the photon energy resolution. The impact of systematic uncertainties on the expected sensitivities is also studied. Published by the American Physical Society 2025"", ""Abstract Quantum spin Hall (QSH) insulators are materials with nontrivial topological properties, characterized by helical edge currents. In 2D strips, the application of a bias voltage along the edge generates a magnetization that can be measured using quantum sensors and magnetometry techniques. In this work, we calculate the magnetic field in the vicinity of the edge and explore the potential role of nitrogen-vacancy (NV) centers in diamond as local probes for the characterization of QSH edge states in topological insulators. We characterize the magnetic field near the edges produced by both electron currents and spin accumulation at the edge. We focus on identifying the position from the edge at which the effects of spin accumulation become detectable. We observe that a larger gap between the conduction and valence bands, along with a lower Fermi velocity, results in a stronger magnetic field, with the detectable spin accumulation being more concentrated near the edge. Conversely, a smaller gap results in a slight reduction in the magnetic field magnitude, but the field associated with spin accumulation becomes detectable further from the edge. This work provides insights that could be useful for the characterization of topological materials and the development of novel electro-optical devices.&amp;#xD;""]"
"J. Strandberg","","","Computer Science","https://openalex.org/A5106944425","[]"
"T. P. A. Åkesson","","","Computer Science","https://openalex.org/A5031324575","[""Abstract The jet energy calibration and its uncertainties are derived from measurements of the calorimeter response to single particles in both data and Monte Carlo simulation using proton–proton collisions at $$\\sqrt{s} = 13$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> <mml:mo>=</mml:mo> <mml:mn>13</mml:mn> </mml:mrow> </mml:math> TeV collected with the ATLAS detector during Run 2 at the Large Hadron Collider. The jet calibration uncertainty for anti- $$k_T$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msub> <mml:mi>k</mml:mi> <mml:mi>T</mml:mi> </mml:msub> </mml:math> jets with a jet radius parameter of R $$_\\textrm{jet} = 0.4$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mmultiscripts> <mml:mrow/> <mml:mtext>jet</mml:mtext> <mml:mrow/> </mml:mmultiscripts> <mml:mo>=</mml:mo> <mml:mn>0.4</mml:mn> </mml:mrow> </mml:math> and in the central jet rapidity region is about 2.5% for transverse momenta ( $$p_{\\text {T}}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msub> <mml:mi>p</mml:mi> <mml:mtext>T</mml:mtext> </mml:msub> </mml:math> ) of 20 $$\\text {GeV}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mtext>GeV</mml:mtext> </mml:math> , about $$0.5\\%$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mn>0.5</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> for $$p_{\\text {T}} =300$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msub> <mml:mi>p</mml:mi> <mml:mtext>T</mml:mtext> </mml:msub> <mml:mo>=</mml:mo> <mml:mn>300</mml:mn> </mml:mrow> </mml:math> $$\\text {GeV}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mtext>GeV</mml:mtext> </mml:math> and $$0.7\\%$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mn>0.7</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> for $$p_{\\text {T}} =4$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msub> <mml:mi>p</mml:mi> <mml:mtext>T</mml:mtext> </mml:msub> <mml:mo>=</mml:mo> <mml:mn>4</mml:mn> </mml:mrow> </mml:math> $$\\text {TeV}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mtext>TeV</mml:mtext> </mml:math> . Excellent agreement is found with earlier determinations obtained from $$p_\\textrm{T}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msub> <mml:mi>p</mml:mi> <mml:mtext>T</mml:mtext> </mml:msub> </mml:math> -balance based in situ methods ( $$Z/\\gamma $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mi>Z</mml:mi> <mml:mo>/</mml:mo> <mml:mi>γ</mml:mi> </mml:mrow> </mml:math> +jets). The combination of these two independent methods results in the most precise jet energy measurement achieved so far with the ATLAS detector with a relative uncertainty of $$0.3\\%$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mn>0.3</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> at $$p_\\textrm{T}=300$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msub> <mml:mi>p</mml:mi> <mml:mtext>T</mml:mtext> </mml:msub> <mml:mo>=</mml:mo> <mml:mn>300</mml:mn> </mml:mrow> </mml:math> GeV and $$0.6\\%$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mn>0.6</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> at 4 TeV. The jet energy calibration is also derived with the single-particle calorimeter response measurements separately for quark- and gluon-induced jets and furthermore for jets with R $$_\\textrm{jet}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mmultiscripts> <mml:mrow/> <mml:mtext>jet</mml:mtext> <mml:mrow/> </mml:mmultiscripts> </mml:math> varying from 0.2 to 1.0 retaining the correlations between these measurements. Differences between inclusive jets and jets from boosted top-quark decays, with and without grooming the soft jet constituents, are also studied."", ""Abstract A search for a pseudoscalar a produced in association with a top-quark pair, or in association with a single top quark plus a W boson, with the pseudoscalar decaying into b -quarks ( $$a\\rightarrow b\\bar{b}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mi>a</mml:mi> <mml:mo>→</mml:mo> <mml:mi>b</mml:mi> <mml:mover> <mml:mrow> <mml:mi>b</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> </mml:mrow> </mml:math> ), is performed using the full Run 2 data sample using a dileptonic decay mode signature. The search covers pseudoscalar boson masses between 12 and 100 GeV and involves both the kinematic regime where the decay products of the pseudoscalar are reconstructed as two standard b -tagged small-radius jets, or merged into a large-radius jet due to its Lorentz boost. No significant excess relative to expectations is observed. Assuming a branching ratio $$\\text {BR}(a\\rightarrow b\\bar{b})=100\\% $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mtext>BR</mml:mtext> <mml:mo>(</mml:mo> <mml:mi>a</mml:mi> <mml:mo>→</mml:mo> <mml:mi>b</mml:mi> <mml:mover> <mml:mrow> <mml:mi>b</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> <mml:mo>)</mml:mo> <mml:mo>=</mml:mo> <mml:mn>100</mml:mn> <mml:mo>%</mml:mo> </mml:mrow> </mml:math> , the range of pseudoscalar masses between 50 and 80 GeV is excluded at 95% confidence level for a coupling of the pseudoscalar to the top quark of 0.5, while a coupling of 1.0 is excluded at 95% confidence level for the masses considered, with the coupling defined as the strength modifier of the Standard Model Yukawa coupling.""]"
"M. Ellert","","","Computer Science","https://openalex.org/A5031620607","[""Abstract The fast reconstruction of charged particle tracks with high efficiency and track quality is an essential part of the online data selection for the ATLAS experiment at the High Luminosity LHC. Dedicated custom designed hardware boards and software simulations have been developed to assess the feasibility of a Hardware Tracking Trigger (HTT) system. The Pattern Recognition Mezzanine (PRM), as part of the HTT system, has been designed to recognize track candidates in silicon detectors with Associative Memory ASICs and to select and reconstruct tracks using linearized algorithms implemented in an Intel Stratix 10 MX FPGA. The highly parallelized FPGA design makes extensive use of the integrated High-Bandwidth-Memory. In this paper, the FPGA design for the PRM board is presented. Its functionalities have been verified in both simulations and hardware tests on an Intel Stratix 10 MX development kit."", ""The fast reconstruction of charged particle tracks with high efficiency and track quality is an essential part of the online data selection for the ATLAS experiment at the High-Luminosity LHC. Dedicated custom designed hardware boards and software simulations have been developed to assess the feasibility of a Hardware Tracking Trigger (HTT) system. The Pattern Recognition Mezzanine (PRM), as part of the HTT system, has been designed to recognize track candidates in silicon detectors with Associative Memory ASICs and to select and reconstruct tracks using linearized algorithms implemented in an Intel Stratix 10 MX FPGA. The highly parallelized FPGA design makes extensive use of the integrated High-Bandwidth-Memory. In this paper, the FPGA design for the PRM board is presented. Its functionalities have been verified in both simulations and hardware tests on an Intel Stratix 10 MX development kit."", ""The Worldwide LHC Computing Grid (WLCG) is today comprised of a range of different types of resources such as cloud centers, large and small HPC centers, volunteer computing as well as the traditional grid resources. The Nordic Tier 1 (NT1) is a WLCG computing infrastructure distributed over the Nordic countries. The NT1 deploys the Nordugrid ARC-CE, which is non-intrusive and lightweight, originally developed to cater for HPC centers where no middleware could be installed on the worker nodes. The NT1 runs ARC in the native Nordugrid mode which contrary to the Pilot mode leaves jobs data transfers up to ARC. ARCs data transfer capabilities together with the ARC Cache are the most important features of ARC. In this article we will describe the datastaging and cache functionality of the ARC-CE set up as an edge service to an HPC or cloud resource, and show the gain in efficiency this model provides compared to a traditional pilot model, especially for sites with remote storage."", ""General-purpose Computing on Graphics Processing Units (GPGPU) has been introduced to many areas of scientific research such as bioinformatics, cryptography, computer vision, and deep learning. However, computing models in the High-energy Physics (HEP) community are still mainly centred around traditional CPU resources. Tasks such as track fitting, particle reconstruction, and Monte Carlo simulation could benefit greatly from a high-throughput GPGPU computing model, streamlining bottlenecks in analysis turnover. This technical note describes the basis of an implementation of an integrated GPU discovery mechanism in GRID middleware to facilitate GPGPU."", ""General-purpose Computing on Graphics Processing Units (GPGPU) has been introduced to many areas of scientific research such as bioinformatics, cryptography, computer vision, and deep learning. However, computing models in the High-energy Physics (HEP) community are still mainly centred around traditional CPU resources. Tasks such as track fitting, particle reconstruction, and Monte Carlo simulation could benefit greatly from a high-throughput GPGPU computing model, streamlining bottlenecks in analysis turnover. This technical note describes the basis of an implementation of an integrated GPU discovery mechanism in GRID middleware to facilitate GPGPU.""]"
"C. C. Ohm","","","Computer Science","https://openalex.org/A5047441454","[""Relational databases often suffer from uninformative descriptors of table contents, such as ambiguous columns and hard-to-interpret values, impacting both human users and text-to-SQL models. In this paper, we explore the use of large language models (LLMs) to automatically generate detailed natural language descriptions for SQL database columns, aiming to improve text-to-SQL performance and automate metadata creation. We create a dataset of gold column descriptions based on the BIRD-Bench benchmark, manually refining its column descriptions and creating a taxonomy for categorizing column difficulty. Through evaluating several LLMs, we find that incorporating these column descriptions consistently enhances text-to-SQL model performance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral 22Bx8. However, models struggle with columns that exhibit inherent ambiguity, highlighting the need for manual expert input. Notably, Qwen2-generated descriptions, containing by annotators deemed superfluous information, outperform manually curated gold descriptions, suggesting that models benefit from more detailed metadata than humans expect. Future work will investigate the specific features of these high-performing descriptions and explore other types of metadata, such as numerical reasoning and synonyms, to further improve text-to-SQL systems. The dataset, annotations and code will all be made available."", ""In many social networks, besides peer-to-peer communication, people share information via groups. An interesting problem arises in this scenario: for such networks, which are the best groups to start information diffusion so that the number of eventually informed nodes can be maximized? In this study, we formulate a novel information coverage maximization problem in the context of hypergraphs, wherein nodes are connected by arbitrary-size hyperedges (i.e., groups). In contrast to the existing literature on influence maximization, which aims to find authority nodes with high influence, we are interested in identifying the key groups. To address this problem, we present a new information diffusion model for hypergraphs, namely Hypergraph- Independent-Cascade (HIC). HIC generalizes the popular independent cascade model to hypergraphs to allow capturing group-level information diffusion. We prove the NP- hardness of the proposed problem under HIC, and the submodular monotone property of the information coverage function. Further, inspired by the Degree Discount algorithm, we derive a new heuristic method named Influence Discount (InfDis). Extensive experiments provide empirical evidence for the effectiveness and efficiency of our approach."", ""A technique is presented to measure the efficiency with which $c$-jets are mistagged as b-jets (mistagging efficiency) using $t\\bar{t}$ events, where one of the $W$ bosons decays into an electron or muon and a neutrino and the other decays into a quark-antiquark pair. The measurement utilises the relatively large and known $W\\to cs$ branching ratio, which allows a measurement to be made in an inclusive $c$-jet sample. The data sample used was collected by the ATLAS detector at $\\sqrt{s} = 13$ TeV and corresponds to an integrated luminosity of 139 fb$^{-1}$. Events are reconstructed using a kinematic likelihood technique which selects the mapping between jets and $t\\bar{t}$ decay products that yields the highest likelihood value. The distribution of the $b$-tagging discriminant for jets from the hadronic $W$ decays in data is compared with that in simulation to extract the mistagging efficiency as a function of jet transverse momentum. The total uncertainties are in the range 3%-17%. The measurements generally agree with those in simulation but there are some differences in the region corresponding to the most stringent $b$-jet tagging requirement."", ""Heavy-flavour hadron production provides information about the transport properties and microscopic structure of the quark-gluon plasma created in ultra-relativistic heavy-ion collisions. A measurement of the muons from semileptonic decays of charm and bottom hadrons produced in Pb+Pb and $pp$ collisions at a nucleon-nucleon centre-of-mass energy of 5.02 TeV with the ATLAS detector at the Large Hadron Collider is presented. The Pb+Pb data were collected in 2015 and 2018 with sampled integrated luminosities of $208~\\mathrm{\\mu b}^{-1}$ and $38~\\mathrm{\\mu b^{-1}}$, respectively, and $pp$ data with a sampled integrated luminosity of $1.17~\\mathrm{pb}^{-1}$ were collected in 2017. Muons from heavy-flavour semileptonic decays are separated from the light-flavour hadronic background using the momentum imbalance between the inner detector and muon spectrometer measurements, and muons originating from charm and bottom decays are further separated via the muon track's transverse impact parameter. Differential yields in Pb+Pb collisions and differential cross sections in $pp$ collisions for such muons are measured as a function of muon transverse momentum from 4 GeV to 30 GeV in the absolute pseudorapidity interval $|\\eta| < 2$. Nuclear modification factors for charm and bottom muons are presented as a function of muon transverse momentum in intervals of Pb+Pb collision centrality. The measured nuclear modification factors quantify a significant suppression of the yields of muons from decays of charm and bottom hadrons, with stronger effects for muons from charm hadron decays.""]"
"C. Doglioni","","","Computer Science","https://openalex.org/A5050166026","[""The fundamental nature of Dark Matter is a central theme of the Snowmass 2021 process, extending across all frontiers. In the last decade, advances in detector technology, analysis techniques and theoretical modeling have enabled a new generation of experiments and searches while broadening the types of candidates we can pursue. Over the next decade, there is great potential for discoveries that would transform our understanding of dark matter. In the following, we outline a road map for discovery developed in collaboration among the frontiers. A strong portfolio of experiments that delves deep, searches wide, and harnesses the complementarity between techniques is key to tackling this complicated problem, requiring expertise, results, and planning from all Frontiers of the Snowmass 2021 process."", ""Semi-visible jets arise from a hypothetical, strongly interacting ``dark sector'' -- a dark counterpart of quantum chromodynamics whose partial decays back to Standard Model particles introduce new types of collider BSM signature. CMS and ATLAS have have searched for semi-visible jets in the resonant and non-resonant production modes and set constraints on mediator mass values. In this work, indirect constraints on various model parameters, such as dark hadron masses and coupling strengths, are explored using LHC measurements."", ""Abstract In modern High Energy Physics (HEP) experiments, triggers perform the important task of selecting, in real time, the data to be recorded and saved for physics analyses. As a result, trigger strategies play a key role in extracting relevant information from the vast streams of data produced at facilities like the Large Hadron Collider (LHC). As the energy and luminosity of the collisions increase, these strategies must be upgraded and maintained to suit the experimental needs. This whitepaper presents a high-level overview and reviews recent developments of triggering practices employed at the LHC. The general trigger principles applied at modern HEP experiments are highlighted, with specific reference to the current trigger state-of-the-art within the ALICE, ATLAS, CMS and LHCb collaborations. Furthermore, a brief synopsis of the new trigger paradigm required by the upcoming high-luminosity upgrade of the LHC is provided."", ""In modern High Energy Physics (HEP) experiments, triggers perform the important task of selecting, in real time, the data to be recorded and saved for physics analyses. As a result, trigger strategies play a key role in extracting relevant information from the vast streams of data produced at facilities like the Large Hadron Collider (LHC). As the energy and luminosity of the collisions increase, these strategies must be upgraded and maintained to suit the experimental needs. This whitepaper compiled by the SMARTHEP Early Stage Researchers presents a high-level overview and reviews recent developments of triggering practices employed at the LHC. The general trigger principles applied at modern HEP experiments are highlighted, with specific reference to the current trigger state-of-the-art within the ALICE, ATLAS, CMS and LHCb collaborations. Furthermore, a brief synopsis of the new trigger paradigm required by the upcoming high-luminosity upgrade of the LHC is provided.""]"
"T. Johansson","","","Computer Science","https://openalex.org/A5072376954","[""Abstract The Multi-Blade (MB) Boron-10-based neutron detector is the chosen technology for three instruments at the European Spallation Source (ESS): the two ESS reflectometers, ESTIA and FREIA, and the Test Beam Line. A fourth MB detector has been built, installed and commissioned for the user operation of the reflectometer Amor at PSI (Switzerland). Amor can be considered a downscaled version of the ESS reflectometer ESTIA. They are based on the same Selene guide concept, optimized for performing focusing reflectometry on small samples. The experience gained at Amor is invaluable for the future deployment of the MB detector at the ESS. This manuscript describes the MB detector construction and installation at Amor along with the readout electronics chain based on the VMM3a ASIC. The readout chain deployed at Amor is equivalent of that of the ESS, including the readout master module (RMM), event-formation-units (EFUs), Kafka, FileWriter and live visualisation tools."", ""A new generation of experiments is being developed, where the challenge of separating rare signal processes from background at high intensities requires a change of trigger paradigm. At the future PANDA experiment at FAIR, hardware triggers will be abandoned and instead a purely software-based system will be used. This requires novel reconstruction methods with the ability to process data from many events simultaneously. A 4D tracking algorithm based on the cellular automaton has been developed which will utilize the timing information from detector signals. Simulation studies have been performed to test its performance on the foreseen free-streaming data from the PANDA detector. For this purpose, a quality assurance procedure for tracking on free-streaming data was implemented in the PANDA software. The studies show that at higher interaction rates, 4D tracking performs better than the 3D algorithm in terms of efficiency, 84% compared to 77%. The fake track suppression is also greatly improved, compared to the 3D tracking with roughly a 50% decrease in the ghost rate.""]"
"T. Dorigo","","","Computer Science","https://openalex.org/A5037652790","[""In this work we consider the problem of determining the identity of hadrons at high energies based on the topology of their energy depositions in dense matter, along with the time of the interactions. Using GEANT4 simulations of a homogeneous lead tungstate calorimeter with high transverse and longitudinal segmentation, we investigated the discrimination of protons, positive pions, and positive kaons at 100 GeV. The analysis focuses on the impact of calorimeter granularity by progressively merging detector cells and extracting features like energy deposition patterns and timing information. Two machine learning approaches, XGBoost and fully connected deep neural networks, were employed to assess the classification performance across particle pairs. The results indicate that fine segmentation improves particle discrimination, with higher granularity yielding more detailed characterization of energy showers. Additionally, the results highlight the importance of shower radius, energy fractions, and timing variables in distinguishing particle types. The XGBoost model demonstrated computational efficiency and interpretability advantages over deep learning for tabular data structures, while achieving similar classification performance. This motivates further work required to combine high- and low-level feature analysis, e.g., using convolutional and graph-based neural networks, and extending the study to a broader range of particle energies and types."", ""The TomOpt software package is designed to optimise the geometric configuration and the specifications of detectors intended for muon scattering tomography, an imaging technique exploiting cosmic-ray muons. The software employs an end-to-end differentiable pipeline that models the interactions of muons with detectors and scanned volumes, infers properties of the scanned materials, and performs an optimisation cycle minimising a user-defined loss function. This article presents the implementation of a case study related to cargo scanning applications in the context of homeland security."", ""We simulate hadrons impinging on a homogeneous lead tungstate (PbWO4) calorimeter using GEANT4 software to investigate how the resulting light yield and its temporal structure, as detected by an array of light-sensitive sensors, can be processed by a neuromorphic computing system. Our model encodes temporal photon distributions as spike trains and employs a fully connected spiking neural network to estimate the total deposited energy, as well as the position and spatial distribution of the light emissions within the sensitive material. The extracted primitives offer valuable topological information about the shower development in the material, achieved without requiring a segmentation of the active medium. A potential nanophotonic implementation using III-V semiconductor nanowires is discussed. It can be both fast and energy efficient."", ""Recent advances in machine learning have opened new avenues for optimizing detector designs in high-energy physics, where the complex interplay of geometry, materials, and physics processes has traditionally posed a significant challenge. In this work, we introduce the end-to-end. AI Detector Optimization framework (AIDO), which leverages a diffusion model as a surrogate for the full simulation and reconstruction chain, enabling gradient-based design exploration in both continuous and discrete parameter spaces. Although this framework is applicable to a broad range of detectors, we illustrate its power using the specific example of a sampling calorimeter, focusing on charged pions and photons as representative incident particles. Our results demonstrate that the diffusion model effectively captures critical performance metrics for calorimeter design, guiding the automatic search for a layer arrangement and material composition that align with known calorimeter principles. The success of this proof-of-concept study provides a foundation for the future applications of end-to-end optimization to more complex detector systems, offering a promising path toward systematically exploring the vast design space in next-generation experiments.""]"
"Gerhard Andersson","","","Computer Science","https://openalex.org/A5080370996","[""Abstract The Patient Health Questionnaire-9 (PHQ-9) is a widely used tool for assessing depressive symptom severity and as a screening tool in the diagnosis of major depression. Designed as both a diagnostic instrument and a severity index, it is commonly used in primary care and research. However, findings regarding its reliability and validity for these dual purposes have been mixed. This study aimed to review the history of the PHQ-9, evaluate its factorial validity, and temporal measurement invariance using the dynamic fit index cutoffs framework for model fit evaluations. In a clinical sample of 3384 participants, strong correlations were found between items 1 and 2 (i.e., anhedonia and feeling depressed), indicating a substantial overlap in their coverage. Although a unidimensional factor structure was obtained, the model fit was suboptimal when contrasted with the sample dependent dynamic fit indices. Furthermore, measurement invariance between different treatment groups could not be established, strongly indicating that not all respondents use the PHQ-9 in the same manner as the only differentiating factor between groups was their randomized treatment group allocation. Moreover, temporal measurement invariance could not be convincingly established, in turn raising concerns about its comparability across time points. This suggests that observed changes in PHQ-9 scores over treatment weeks may, at least partly, reflect shifts in how participants engage with the scale rather than true changes in depressive symptomatology. In conclusion, our results raise questions about the validity of using the PHQ-9 to index depressive symptom severity and to monitor treatment outcomes over time."", ""Provides tools for assessing and selecting auxiliary variables using LASSO. The package includes functions for variable selection and diagnostics, facilitating survey calibration analysis with emphasis on robust auxiliary vector selection. For more details see Tibshirani (1996) &lt;<a href=\""https://doi.org/10.1111%2Fj.2517-6161.1996.tb02080.x\"" target=\""_top\"">doi:10.1111/j.2517-6161.1996.tb02080.x</a>&gt; and Caughrey and Hartman (2017) &lt;<a href=\""https://doi.org/10.2139%2Fssrn.3494436\"" target=\""_top\"">doi:10.2139/ssrn.3494436</a>&gt;."", ""<sec> <title>BACKGROUND</title> Refugees commonly experience numerous adverse and traumatic events and are therefore at increased risk of mental health problems. Despite the high need for mental health interventions, services tend to be under-utilized by refugees, and various barriers compromise access. Digital, efficient screening, adapted for refugees, could facilitate initial assessment and increase accessibility to mental health services. We developed an internet-based tiered screening procedure, i-TAP, aiming to identify clinically relevant symptoms of major depressive disorder, anxiety disorders, post-traumatic stress disorder (PTSD) and insomnia disorder among refugees. The i-TAP is an adaptive procedure with three tiers aiming to identify general mental distress in tier 1, differentiate between symptoms in tier 2, and assess severity of symptoms in tier 3. Each tier additionally functions as a gateway to further assessment, as a negative outcome terminates the procedure. </sec> <sec> <title>OBJECTIVE</title> The purpose of the this study was to evaluate the diagnostic test accuracy of the i-TAP, using structured clinical assessments as reference standard. </sec> <sec> <title>METHODS</title> In this prospective study, 70 adult participants with a refugee background, literate in Arabic, Dari, Farsi or Swedish, and residing in Sweden, completed the i-TAP on tablets and participated in a subsequent structured diagnostic interview. </sec> <sec> <title>RESULTS</title> Results show that the i-TAP could identify 91.7% of individuals assessed with any psychiatric disorder, and correctly identified 82.1% of all positive cases of depression, anxiety, PTSD and insomnia, with few false negative assessments. Overall test accuracy of the i-TAP ranged between 77.1% and 84.3%, depending on disorder. The tiered design could reduce item burden with maintained accuracy. A vast majority of participants rated the user-experience as positive. In this sample, 36 (51.4%) individuals were assessed with one or more psychiatric disorders and comorbidity was high. </sec> <sec> <title>CONCLUSIONS</title> The i-TAP may be a valid, efficient and feasible screening tool for identification of common psychiatric disorders among refugees. The i-TAP could be implemented as a first screener in various settings, including online and in-person clinical practices. The digital, adaptive, multilanguage format could facilitate early assessment and increase the availability of mental health services for refugees. </sec>"", ""Despite efforts to implement evidence-based guidelines, little is known about the quality of cognitive-behavioural therapy (CBT) for depression, anxiety disorders, obsessive-compulsive disorder, and posttraumatic stress disorder in routine clinical practice. The present study aimed to investigate therapist adherence to CBT, and related aspects of quality of care, from a patient perspective. In a cross-sectional study, 90 participants from 21 routine psychiatric outpatient units in Stockholm completed a web-based survey post-CBT. Participants reported a high degree of therapist adherence to CBT techniques and procedures (M = 3.02 on a scale of 0 to 4), with higher adherence for social anxiety disorder and posttraumatic stress disorder than for depression (p = .002, ω2 = 0.21). Therapist adherence was moderately correlated with patient improvement (τs = .37-.38, ps < .001). Participants reported a high degree of symptom improvement (M = 3.10) and treatment satisfaction (M = 3.38) and received a median of 15 sessions. Also, we found examples of excessive healthcare provision and non-adherence, as 17% far exceeded the recommended number of sessions, 80% had concurrent psychotropic treatments despite lower guideline priority, and 11% took benzodiazepines though not recommended. In conclusion, we found that CBT was delivered with high quality."", ""The use of internet-based treatments has increased significantly in recent years. As remote technologies continue to evolve, psychotherapy research is progressively shifting toward these approaches. Anxiety and depressive disorders are highly prevalent in adolescents, imposing significant personal and societal costs. Identifying effective and scalable treatments for this age group is therefore essential. This study aimed to compare the efficacy of face-to-face and internet-based unified transdiagnostic treatment in reducing symptoms and improving functioning in adolescents with anxiety and depressive disorders. A pilot randomized controlled trial (RCT) with pre-test, post-test, and follow-up assessments was conducted to compare the efficacy of face-to-face and internet-based treatments. Forty-nine adolescents (aged 13-18 years) from Tehran, Alborz, Gilan, and Kerman were randomly assigned to one of three groups: face-to-face treatment, internet-based treatment, or a control group. Assessments were conducted before treatment, after treatment, and at six months post-treatment. A Mixed-model Analysis of Variance was used for data analysis. Both face-to-face and internet-based transdiagnostic treatments demonstrated similar efficacy in reducing anxiety and depression symptoms, improving functioning, decreasing negative affect, and reducing avoidance in adolescents. However, neither treatment significantly improved positive affect or distress tolerance. Additionally, the effects in the internet-based group were maintained through the follow-up phase, while the face-to-face treatment group did not sustain these results by the six-month follow-up. Based on the results of this study, internet-based transdiagnostic treatment is also a viable option for treating anxiety and depressive disorders. The large scale implementation of internet-based transdiagnostic psychotherapy could be an effective strategy to bridge the significant gap between adolescents' mental health needs and the availability of evidence-based treatments for anxiety and depression. https://irct.behdasht.gov.ir/trial/62220, identifier IRCT20220226054129N1.""]"
"Oskar Hansson","","","Computer Science","https://openalex.org/A5049277633","[""The objective of this study was to determine the predictive value of amyloid-positron emission tomography (PET) versus the plasma ratio of phosphorylated tau at threonine 217 (p-tau217) to non-phosphorylated tau217 (%p-tau217) for tau-PET transitions (T- to T+). The added value of combining plasma amyloid-β 42 and amyloid-β 40 (Aβ42/40) and %p-tau217 into an amyloid probability score (APS2) was also assessed. Mayo Clinic Study of Aging (MCSA) participants had plasma markers measured at via mass spectrometry (MS), an amyloid-PET scan, and a tau-PET (meta-temporal region of interest [ROI]) negative scan (standardized uptake value ratio [SUVR] <1.29) at the index (baseline) date, along with one or more follow-up tau-PET scans. The BioFINDER-2 cohort was used for validation. Cox proportional hazards models adjusted for age, sex, and apolipoprotein (APOE) ε4 were used to assess predictors, with scaling to the interquartile range (IQR) for comparability of hazard ratios (HR). Among 255 tau-PET negative MCSA participants (median age: 71.9 years), 37 converted to tau-PET positive (median follow-up time: 3.81 years). Higher %p-tau217 (HR: 1.52 [95% CI: 1.28-1.80]), amyloid-PET centiloid (HR: 1.47 [95% CI: 1.20-1.79]), and APS2 (HR: 1.62 [95% CI: 1.22-2.16]) predicted tau-PET conversion. However, Aβ42/40 (HR: 0.94 [95% CI: 0.54-1.66]) was not associated with tau-PET conversion. In the BioFINDER-2 cohort (605 tau-negative, median age: 70.2), 33 converted to tau-positive (median follow-up time: 2 years), with higher %p-tau217 (HR: 1.80 [95% CI: 1.50-2.17]), amyloid-PET centiloid (HR: 2.29 [95% CI: 1.77-2.97]), and lower Aβ42/40 (HR: 2.38 [95% CI: 1.17-4.83]) predicting conversion. In two cohorts, %p-tau217 was associated with tau-PET conversion, comparable to amyloid-PET. APS2 also predicted conversion in the MCSA cohort, whereas Aβ42/40 predicted conversion in the BioFINDER-2 cohort, which had more individuals with cognitive impairment. ANN NEUROL 2025."", ""The distribution of tau pathology in Alzheimer's disease (AD) shows remarkable inter-individual heterogeneity, including hemispheric asymmetry. However, the factors driving this asymmetry remain poorly understood. Here we explore whether tau asymmetry is linked to i) reduced inter-hemispheric brain connectivity (potentially restricting tau spread), or ii) asymmetry in amyloid-beta (Aβ) distribution (indicating greater hemisphere-specific vulnerability to AD pathology). We include 452 participants from the Swedish BioFINDER-2 cohort with evidence of both Aβ pathology (CSF Aβ42/40 or neocortical Aβ-PET) and tau pathology (temporal tau-PET), categorising them as left asymmetric (n = 102), symmetric (n = 306), or right asymmetric (n = 44) based on temporal lobe tau-PET uptake distribution. We assess edge-wise inter-hemispheric functional (RSfMRI; n = 318) and structural connectivity (dMRI; n = 352) but find no association between tau asymmetry and connectivity. In contrast, we observe a strong association between tau and Aβ laterality patterns based on PET uptake (n = 233; β = 0.632, p < 0.001), which we replicate in three independent cohorts (n = 234; β = 0.535, p < 0.001). In a longitudinal Aβ-positive sample, we show that baseline Aβ asymmetry predicts progression of tau laterality over time (n = 289; β = 0.025, p = 0.028). These findings suggest that tau asymmetry is not associated with a weaker inter-hemispheric connectivity but might reflect hemispheric differences in vulnerability to Aβ pathology, underscoring the role of regional vulnerability in determining the distribution of AD pathology."", ""Developing and validating sensitive visual read algorithms for assessing Alzheimer disease–related tau in tau PET imaging is imperative, considering the implementation of the methodology in clinical practice and trials. Our aim was to compare 2 visual read algorithms for tau PET images to semiquantitative measurements and plasma phospho-tau 217 (p-tau217) status. <b>Methods:</b> In total, 1,654 participants were consecutively recruited from secondary memory clinics in southern Sweden as part of the prospective BioFINDER-2 cohort study (May 2017–September 2023). All participants underwent [<sup>18</sup>F]RO948 scans, and 37 participants underwent an additional [<sup>18</sup>F]flortaucipir scan. PET scans were read visually in accordance with the BioFINDER visual read (BF-VR) protocol and the established visual read method for [<sup>18</sup>F]flortaucipir (FTP-VR). Comparative analyses were conducted with semiquantitative SUV ratios (SUVRs) in the entorhinal cortex (ERC) and a temporal meta-region of interest and with plasma p-tau217 status. The primary endpoints of the study included the accuracy of visual read algorithms in detecting increases in semiquantitative SUVRs and p-tau217. Secondary outcomes included the intrarater and interrater reliabilities of the BF-VR. <b>Results:</b> Both visual read methods exhibited strong concordance with semiquantitative SUVRs. However, the BF-VR method demonstrated superior accuracy for tau in the ERC (93.9%; 95% CI, 92.6%–95.0%) compared with the FTP-VR method (89.9%; 95% CI, 88.3%–91.3%; <i>P</i> &lt; 0.0001). The BF-VRs displayed lower accuracy when detecting tau in the temporal meta-region of interest (91.8%; 95% CI, 90.4%–93.1%) compared with the FTP-VRs (94.7%; 95% CI, 93.6%–95.8%; <i>P</i> = 0.002). Further, the BF-VRs exhibited similar accuracy (0.866; 95% CI, 0.847–0.884) to the FTP-VRs (0.857; 95% CI, 0.837–0.875; <i>P</i> = 0.54) for detection of p-tau217 abnormality. The interrater reliability of the BF-VR algorithm was excellent (weighted Cohen κ, 0.87; range, 0.82–0.93), and intrarater reliability was almost perfect (Cohen κ, 0.94; range, 0.89–0.98). The concordance of the BF-VR assessments of [<sup>18</sup>F]RO948 and [<sup>18</sup>F]flortaucipir images was excellent (Cohen κ, 0.94; range, 0.86–1.00). <b>Conclusion:</b> Visual reads offer a straightforward method for evaluating tau PET scans. The BF-VR algorithm provides a more accurate algorithm for detecting tau uptake in the ERC compared with the established FTP-VR algorithm and exhibits similar performance when using [<sup>18</sup>F]RO948 and [<sup>18</sup>F]flortaucipir images. The BF-VR algorithm further showed excellent interrater and intrarater reliabilities."", ""Plasma biomarkers' utility for predicting incident mild cognitive impairment (MCI) remains unclear. We evaluated associations of plasma Alzheimer's disease (AD) biomarkers and amyloid positron emission tomography (PET) with transitions from cognitively unimpaired (CU) to MCI in the Mayo Clinic Study of Aging (MCSA) and BioFINDER-2 studies. Associations of continuous baseline plasma biomarker levels and amyloid PET Centiloid with progression to MCI, adjusting for age, sex, and education, were evaluated with Cox proportional hazards models. The study included 381 MCSA and 584 BioFINDER-2 participants. Amyloid PET and percent phosphorylated to non-phosphorylated tau217 (%p-tau217) were strong predictors of progression to MCI in both cohorts: hazard ratios of 1.49 and 1.23 in the MCSA and 1.72 and 1.65 in BioFINDER, respectively. Amyloid beta 42/40 was a significant predictor in BioFINDER-2 only (hazard ratio 2.20). Plasma %p-tau217 was associated with progression from CU to MCI in both cohorts, although differences in biomarker associations may be related to differences in the two cohorts. Mass-spectrometry-based plasma phosphorylated tau217 was associated with cognitively unimpaired to mild cognitive impairment (MCI) progression. Plasma amyloid beta 42/40 was a significant predictor in BioFINDER but not the Mayo Clinic Study of Aging (MCSA). Amyloid positron emission tomography (PET) was the strongest predictor of progression to MCI in the MCSA. Plasma had added value to amyloid PET in BioFINDER but not the MCSA. Biomarker performance may vary with cohort and biomarker measurement differences.""]"
"Karl Henrik Johansson","","","Computer Science","https://openalex.org/A5045975901","[]"
"Rajeev Ahuja","","","Computer Science","https://openalex.org/A5037243808","[""Abstract Inventing new approaches to transform abundant plant matter swiftly into valuable products is crucial for sustainable development. Here, the first direct and continuous conversion of lignocellulosic solid mass into synthetic biogas is reported using blue laser irradiation at multi‐kW/cm 2 intensity. It is demonstrated that the on‐demand generation of a biogas jet, achieving rapid (in milliseconds) and continuous production of a flame jet 300 times larger than the mm‐size laser focus. Remarkably, biogas production occurs exclusively during laser illumination, without triggering bulk combustion, and achieves up to 92% mass conversion. A comprehensive phase diagram of the blue light–wood interaction highlighting three distinct regimes accessed is reported by varying laser parameters and provide mechanistic insight using photothermal simulations. Furthermore, the molecular composition of the synthetic biogas is identified and is shown to result from the photothermal decomposition of lignocellulose molecules. The biogas, collected independently, is shown to produce a sustained flame. Successful bio‐gasification of 16 kinds of woods proves that the all‐optical approach is solvent‐free, non‐contact, scalable, and universally applicable."", ""Abstract Hydroxyapatite (HAP) can serve as a critical permeable reactive barrier for the in situ remediation of 90 Sr radionuclides from groundwater through promoting co‐precipitation. The Sr 2+ leaching associated with the structural degradation of Sr‐HAP in the geological field environment is crucial for lowering groundwater radioactivity; thus, evaluating HAP chemical durability is critical. However, the degradation mechanism of Sr‐doped HAP upon water solutions under the influence of surface charge effects remains unclear and requires further investigation. In this work, the surface degradation and penetration diffusion of Sr‐doped HAP are systematically investigated based on density functional theory and dissolution experiments. Some specific issues such as the preparation process of single‐phase ceramics and surface morphology are discussed in details, along with uncovering the relationship between Sr–O bond and Sr leaching activity. The results reveal that the degradation behavior of Sr‐HAP ceramics follows a typical dissolution‐precipitation process. The presence of unsaturated chemical bonds derived from incomplete [SrO 7 ] and [SrO 9 ] coordination polyhedra can enhance the ability of Sr‐HAP to facilitate surface adsorption and contribute to the nonuniform distribution of electronic states at the nanoscale. Importantly, the hydrogen bonds from aqueous solution provide a strong upward pulling force for the surface adsorption sites to render the metastable surface structure rather than water molecule penetration diffusion, leading to the outer layer of the (0 0 1) surface spontaneously twists to form Schottky defects."", ""Abstract The interplay of superconducting and topological states in two-dimensional materials has gained intensive attention for exploring novel quantum phenomena and their applications in quantum computing. However, two-dimensional materials exhibiting both superconductivity and topological phases are exceptionally rare. In this context, we investigated two-dimensional CrH 2&amp;#xD;(chromium dihydride) in P6m2 (hexagonal) and P3m1 (trigonal) symmetries using first-principles calculations. We verified the stability of these phases using phonon dispersion and mechanical stability analyses. Based on our Z 2 invariant&amp;#xD;calculations, CrH 2 is topologically nontrivial for the P6m2 symmetry, while it is topologically trivial for the P3m1 symmetry. Using anisotropic Migdal-Eliashberg equations, we find both phases as single-gap superconductors, with transition temperatures of ∼11K for the hexagonal phase and ∼8K for the trigonal phase. The superconducting properties are attributed to electron-phonon&amp;#xD;coupling between Cr-d orbitals and low-energy phonon modes dominated by Cr vibrations. Our findings offer a promising foundation for further exploration of co-existence of topological and superconducting states in monolayer hydrides and&amp;#xD;their experimental realization."", ""Planar -conjugated borides not only attract fundamental interests in bonding chemistry, but also offers wide-ranging applications in catalysis, bioimaging, and biosensing. To stabilize -conjugated <a:math xmlns:a=\""http://www.w3.org/1998/Math/MathML\""><a:msub><a:mi mathvariant=\""normal\"">B</a:mi><a:mn>6</a:mn></a:msub></a:math> rings without out-of-plane bonding, we propose a design strategy that involves inserting low-compressible <c:math xmlns:c=\""http://www.w3.org/1998/Math/MathML\""><c:mrow><c:mi mathvariant=\""normal\"">C</c:mi><c:msub><c:mi mathvariant=\""normal\"">N</c:mi><c:mn>3</c:mn></c:msub></c:mrow></c:math> triangles into tessellation of <f:math xmlns:f=\""http://www.w3.org/1998/Math/MathML\""><f:msub><f:mi mathvariant=\""normal\"">B</f:mi><f:mn>6</f:mn></f:msub></f:math> rings to create planar borides. Based on this concept, we designed the planar hypercoordinate boride <h:math xmlns:h=\""http://www.w3.org/1998/Math/MathML\""><h:mrow><h:mi>ScC</h:mi><h:msub><h:mi mathvariant=\""normal\"">N</h:mi><h:mn>3</h:mn></h:msub><h:msub><h:mi mathvariant=\""normal\"">B</h:mi><h:mn>6</h:mn></h:msub></h:mrow></h:math>, which features the planar structure and represents a theoretically predicted -conjugated <k:math xmlns:k=\""http://www.w3.org/1998/Math/MathML\""><k:msub><k:mi mathvariant=\""normal\"">B</k:mi><k:mn>6</k:mn></k:msub></k:math> ring. The molecular dynamics simulations suggested that <m:math xmlns:m=\""http://www.w3.org/1998/Math/MathML\""><m:msub><m:mi mathvariant=\""normal\"">B</m:mi><m:mn>6</m:mn></m:msub></m:math> rings in <o:math xmlns:o=\""http://www.w3.org/1998/Math/MathML\""><o:mrow><o:mi>ScC</o:mi><o:msub><o:mi mathvariant=\""normal\"">N</o:mi><o:mn>3</o:mn></o:msub><o:msub><o:mi mathvariant=\""normal\"">B</o:mi><o:mn>6</o:mn></o:msub></o:mrow></o:math> remain intact at a high temperature of 1500 K. The metal adsorption energy of <r:math xmlns:r=\""http://www.w3.org/1998/Math/MathML\""><r:msub><r:mi mathvariant=\""normal\"">B</r:mi><r:mn>6</r:mn></r:msub></r:math> rings in <t:math xmlns:t=\""http://www.w3.org/1998/Math/MathML\""><t:mrow><t:mi>ScC</t:mi><t:msub><t:mi mathvariant=\""normal\"">N</t:mi><t:mn>3</t:mn></t:msub><t:msub><t:mi mathvariant=\""normal\"">B</t:mi><t:mn>6</t:mn></t:msub></t:mrow></t:math> is comparable to borophene, and superior to that of graphene and two-dimensional hexagonal boron nitride. As a result, the designed <w:math xmlns:w=\""http://www.w3.org/1998/Math/MathML\""><w:mrow><w:mi>ScC</w:mi><w:msub><w:mi mathvariant=\""normal\"">N</w:mi><w:mn>3</w:mn></w:msub><w:msub><w:mi mathvariant=\""normal\"">B</w:mi><w:mn>6</w:mn></w:msub></w:mrow></w:math> demonstrates both high stability and chemical activity attributed to <z:math xmlns:z=\""http://www.w3.org/1998/Math/MathML\""><z:mrow><z:mi mathvariant=\""normal\"">C</z:mi><z:msub><z:mi mathvariant=\""normal\"">N</z:mi><z:mn>3</z:mn></z:msub></z:mrow></z:math> triangles and <cb:math xmlns:cb=\""http://www.w3.org/1998/Math/MathML\""><cb:msub><cb:mi mathvariant=\""normal\"">B</cb:mi><cb:mn>6</cb:mn></cb:msub></cb:math> rings, respectively. Consequently, the designed <eb:math xmlns:eb=\""http://www.w3.org/1998/Math/MathML\""><eb:mrow><eb:mi>ScC</eb:mi><eb:msub><eb:mi mathvariant=\""normal\"">N</eb:mi><eb:mn>3</eb:mn></eb:msub><eb:msub><eb:mi mathvariant=\""normal\"">B</eb:mi><eb:mn>6</eb:mn></eb:msub></eb:mrow></eb:math> could serve as a potential sensor for biomolecules. Moreover, the planar ninefold hypercoordinate Sc weakly bonds to the <hb:math xmlns:hb=\""http://www.w3.org/1998/Math/MathML\""><hb:mrow><hb:msub><hb:mi mathvariant=\""normal\"">N</hb:mi><hb:mn>3</hb:mn></hb:msub><hb:msub><hb:mi mathvariant=\""normal\"">B</hb:mi><hb:mn>6</hb:mn></hb:msub></hb:mrow></hb:math> wheel, drives a flat band of the orbital, suggesting that flat band can be generated by splitting of <kb:math xmlns:kb=\""http://www.w3.org/1998/Math/MathML\""><kb:mrow><kb:mi mathvariant=\""normal\"">d</kb:mi><kb:msup><kb:mrow><kb:mi>z</kb:mi></kb:mrow><kb:mn>2</kb:mn></kb:msup></kb:mrow></kb:math> orbital in planar hypercoordinate crystal field. Published by the American Physical Society 2025""]"
"Per Svenningsson","","","Computer Science","https://openalex.org/A5036364090","[""L-DOPA-induced dyskinesia (LID) is a significant and treatment-limiting complication in Parkinson's disease (PD) therapy, yet its mechanisms remain poorly understood. We used high-resolution mass spectrometry imaging to map brain-region-specific alterations of glycerophospholipids and sphingolipids in a female macaque model of PD with and without LID following chronic L-DOPA treatment. LID was associated with depletion of antioxidant plasmalogen phosphatidylcholines in the globus pallidus interna, claustrum, and precentral gyrus-regions critical for motor function-and elevations of polyunsaturated fatty acid-containing glycerophospholipids, indicative of increased membrane fluidity. This lipid profile differed from similarly treated non-dyskinetic animals, suggesting lipid composition mediates differential susceptibility to LID. Lipid alterations correlated strongly with dyskinesia severity, dopamine, and L-DOPA concentrations, supporting a mechanistic link between lipid metabolism, neurotransmitter dysregulation, and LID. This comprehensive spatial lipidomic analysis identifies region-specific lipid dysregulation as a novel aspect of LID pathology, highlighting lipid pathways as potential therapeutic targets for mitigating dyskinesia."", ""Dyskinesia is a debilitating complication of dopaminergic therapy in advanced Parkinson's disease. To evaluate the effect of levodopa-carbidopa intestinal gel (LCIG) on dyskinesia burden. This is a post hoc analysis of the retrospective, observational COmedication Study assessing Mono- and cOmbination therapy with levodopa-carbidopa inteStinal gel (COSMOS; NCT03362879). Change in dyskinesia was assessed by LCIG treatment group (monotherapy, daytime monotherapy, polytherapy), baseline dyskinesia duration (<4 vs. ≥4 hours), and dyskinesia severity (troublesome vs. non-troublesome). Correlations between changes in dyskinesia and patient-reported outcomes (Parkinson's Disease Questionnaire-8 [PDQ-8], Parkinson's Disease Sleep Scale-2 [PDSS-2], non-motor symptoms scale [NMSS]) were assessed using Spearman correlation coefficients. The Unified Parkinson's Disease Rating Scale IV measured dyskinesia duration (Item 32), severity (Item 33), and pain (Item 34). Data were collected cross-sectionally at a single study visit. Over 50% (202/369) of LCIG-treated patients experienced improvement in dyskinesia severity. Improvements in dyskinesia duration and severity were noted in all treatment groups. The proportion of patients with troublesome dyskinesia and amount of \""Off\"" time significantly decreased from baseline to study visit, regardless of baseline dyskinesia burden (P < 0.01); dyskinesia duration improved only in the ≥4-hour subgroup (P < 0.01). In the ≥4-hour subgroup, dyskinesia duration correlated positively with PDQ-8; dyskinesia severity correlated positively with PDQ-8 and PDSS-2; and dyskinesia pain correlated positively with PDQ-8, PDSS-2, and NMSS. LCIG led to reductions in dyskinesia severity, regardless of baseline dyskinesia burden. Dyskinesia duration improved in patients with high dyskinesia burden but not in those with low dyskinesia burden."", ""How dopamine depletion in Parkinson's disease (PD) and subsequent dopamine replacement therapy (DRT) affect brain activity is poorly understood. Typically, brain activity is analysed for its spectral properties and the temporal structure of the activity is often ignored. We quantified the time irreversibility of cortical activity in terms of entropy production rate (EPR). Using the Neural Estimator for Entropy Production (NEEP) algorithm on source-reconstructed resting-state magnetoencephalogram (MEG) data, we estimated the EPR in various brain regions of persons with PD (PwPD) (n = 17) and matched healthy controls (HC, n = 20). PwPD were recorded in two conditions: OFF and ON DRT (one hour after levodopa intake). HC were also recorded in two sessions separated by one hour. Motor symptoms were assessed with Movement Disorders Society-revised Unified Parkinson's Disease Rating Scale (MDS-UPDRS-III). Despite the lack of significant group differences in EPR between the HC and PD groups, we found a positive correlation between DRT induced improvement in motor symptoms (as measured by relative change in MDS-UPDRS-III scores (OFF-ON/OFF+ON)) and change in EPR. In particular, EPR changes in sensory (visual and auditory) regions and the information hub region called temporo-parieto-occipital junction were more strongly correlated with improvement in motor symptoms. Overall DRT tended to reduce EPR in PwPD and bring it close to the EPR values of HCs. Furthermore, PwPD with a higher EPR than HC showed better response to medication correlated with an increased EPR. Higher EPR in PwPD than HCs suggests that chronic dopamine loss alters local network interactions within cortical regions so as to reduce diversity of brain state transitions. Our analysis also opens a promising avenue to extract medication effects from non-invasively acquired cortical activity."", ""Background The potential role of Alzheimer's disease (AD) pathology in contributing to cognitive impairment in α-synucleinopathies warrants in vivo detection of the pathological burden in patients with α-synucleinopathies. Objective Using a fully automated platform, we measured the levels of cerebrospinal fluid (CSF) AD biomarkers, namely Aβ 42 , P-tau181 and T-tau, as well as their derived ratios, P-tau181/Aβ 42 ratio and T-tau/Aβ 42 ratio, in a cohort of Parkinson's disease (PD), dementia with Lewy bodies (DLB), preclinical and prodromal AD (pAD) patients, as well as cognitively normal controls. We binarized the CSF biomarkers and determined the proportion of individuals with abnormal biomarker levels within each diagnostic group. We explored the associations between these biomarkers and cognitive performance. Methods This study consisted of 51 controls, 46 PD, 65 DLB, and 50 pAD patients. CSF biomarkers were measured using Roche Elecsys ® immunoassays. Cognitive performance was based on Mini-Mental State Examination (MMSE). Results CSF P-tau181/Aβ 42 and T-tau/Aβ 42 ratios were increased in DLB and pAD, but not PD, compared with controls. Both DLB and pAD had higher proportions of individuals with abnormal CSF P-tau181/Aβ 42 ratio (AD = 100%, DLB = 59%, PD = 37% and controls = 26%) and T-tau/Aβ 42 ratio (AD = 100%, DLB = 51%, PD = 26% and controls = 18%). Within each diagnostic group, there was no association between CSF AD biomarkers and cognition. Conclusions CSF AD biomarkers profile is prevalent in DLB. Our findings highlight the potential utility of Elecsys ® immunoassay platform for the evaluation of AD pathology in α-synucleinopathies."", ""Parkinson's disease (PD) is a neurodegenerative movement disorder of high global burden. Uncertainties regarding its exact etiology have been hindering the development of curative therapies. As microglia, the brain's immune cells, are suspected to contribute to neurodegeneration by instigating neuroinflammation, existing anti-inflammatory agents could potentially serve as disease-modifying treatments for PD. Here we evaluated the impact of montelukast, a leukotriene receptor antagonist and anti-inflammatory drug, on motor symptoms and neuropathology in an α-synuclein transgenic mouse model (Line 61) for early onset/genetic PD. Two -weeks -old male Line 61 mice and non-transgenic littermates received daily 10 ​mg/kg montelukast or vehicle orally for 10 weeks. Motor functions were assessed through behavioral tests. Brain tissue was analyzed via unbiased transcriptomics, biochemically, and histologically for various parameters, including microglial and inflammation mediators. Upon montelukast treatment, Line 61 mice significantly improved their beam walk performance compared to vehicle -treated mice. The striatum and cerebellum of the montelukast -treated group showed microglial changes toward a smaller but more ramified appearance. Transcriptomics analysis revealed SGK1, a serine/threonine kinase upstream of NFκB and known target in PD, as the most downregulated gene in the striatum of montelukast -treated animals. This downregulation correlated with reduced striatal protein levels of activated IκB kinase, suggesting a reduced NFκB pathway activity upon montelukast treatment. Thus, oral montelukast administration might be promising for the management of PD, with specific effects on motor coordination and balance.""]"
"Jens Nielsen","","","Computer Science","https://openalex.org/A5083238115","[""The presence of organelles is a hallmark distinguishing eukarya from bacteria and archaea, and this culminates in compartmentalization of cellular metabolism and subsequent metabolic specialization. Here we established a dataset encompassing over 300 absolute quantitative proteomes, the largest to date, across two yeast species under diverse experimental conditions. Leveraging big data analysis, formula fitting, and machine learning models, quantitative correlations among protein abundance, organelle-level resource distribution, and cellular phenotypes were elucidated at a system level. We found that protein resources always exhibit robust and precise distribution at the organelle level across distinct conditions. Specifically, at high specific growth rates, the protein mass fraction from some main organelles, i.e., peroxisome and nucleus, is consistently reduced to offset the increasing protein resource demand from the ribosome. Meanwhile, we found that the nutrition limitation could induce resource recycling by upregulating protein resources within the vacuole and lipid droplets to sustain stress adaptation. Importantly, our integrative analysis demonstrates that protein mass fraction from less than 4 organelles (e.g., nucleus and ribosomes) can accurately predict diverse yeast physiological parameters (e.g., specific growth rate, oxygen uptake rate), and a core set of 37 proteins could predict resource allocation among 24 main organelles and sub-organelles with high accuracy (average R^2 &gt; 0.9). Finally, we found organelle resource allocation reflects the divergence of yeast species. For example, anaerobic conditions and respiratory suppression have less influence on Crabtree-positive yeast, i.e., Saccharomyces cerevisiae, with respect to organelle resource allocation but have a larger effect on the Crabtree-negative yeast Issatchenkia orientalis, thus suggesting that cellular resources have facilitated adaptive evolution. In summary, the high-quality, genome-scale quantitative proteomic dataset for yeast species offers an unprecedented opportunity for understanding the basic principles underlying resource allocation at the organelle level, laying theoretical foundations for precision engineering of cell factories in synthetic biology. The resource used in this study is available at https://yeast-proteome-database.streamlit.app/."", ""<title>Abstract</title> Cardiometabolic diseases (CMD) are on the rise globally with one billion people expected to suffer from obesity and 643 million from type 2 diabetes by 2030, of which one-third will likely develop chronic kidney disease and two-thirds will die from cardiovascular disease (CVD). However, the mechanistic and molecular drivers of the transition from health to disease remain elusive. Here, in 275 metabolically healthy individuals recruited to the MetaCardis study, we identify a gut microbiome-kidney-heart axis that is predictive of future cardiovascular events. This axis, as evidenced by the associations between gut microbial metabolism of phenylalanine and tyrosine with variations in both kidney functon(as measured by estimated glomerular filtration rate) and circulating pro-atrial natriuretic peptide concentration, shows a depletion pattern in metabolically unhealthy participants of the MetaCardis study (n = 1,602) indicating a loss of health-sustaining microbiome features with CMD progression. We then validate that microbial compounds from the phenylalanine and tyrosine pathways and their host co-metabolites act as mediators of the gut microbiome-kidney associations. Moreover, Mendelian Randomization analysis adds genetic evidence to suggest that the microbial mediator metabolites regulate host kidney function and vice versa. Finally, we demonstrate that plasma metabolites derived from the microbial metabolism of phenylalanine and tyrosine associate with incident CVD in the Canadian Longitudinal Study on Aging (n = 8,669). Collectively, our results depict the presence of a gut microbiome-kidney-heart axis in metabolically healthy individuals. Major aberrations of the gut microbiome as part of this axis throughout life may increase risk of CVD."", ""The famous model organism Saccharomyces cerevisiae is widely present in a variety of natural and human-associated habitats. Despite extensive studies of this organism, the metabolic mechanisms driving its adaptation to varying niches remain elusive. We here gathered genomic resources from 1,807 S. cerevisiae strains and assembled them into a high-quality pangenome, facilitating the comprehensive characterization of genetic diversity across isolates. Utilizing the pangenome, 1,807 strain-specific genome-scale metabolic models (ssGEMs) were generated, which performed well in quantitative predictions of cellular phenotypes, thus helping to examine the metabolic disparities among all S. cerevisiae strains. Integrative analyses of fluxomics and transcriptomics with ssGEMs showcased ubiquitous transcriptional regulation of metabolic flux in specific pathways (i.e., amino acid synthesis) at a population level. Additionally, the gene/reaction inactivation analysis through the ssGEMs refined by transcriptomics showed that S. cerevisiae strains from various ecological niches had undergone reductive evolution at both the genomic and metabolic network levels when compared to wild isolates. Finally, the compiled analysis of the pangenome, transcriptome, and metabolic fluxome revealed remarkable metabolic differences among S. cerevisiae strains originating from distinct oxygen-limited niches, including human gut and cheese environments, and identified convergent metabolic evolution, such as downregulation of oxidative phosphorylation pathways. Together, these results illustrate how yeast adapts to distinct niches modulated by genomic and metabolic reprogramming, and provide computational resources for translating yeast genotype to fitness in future studies."", ""Abstract Generating longitudinal and multi-layered big biological data is crucial for effectively implementing artificial intelligence (AI) and systems biology approaches in characterising whole-body biological functions in health and complex disease states. Big biological data consists of multi-omics, clinical, wearable device, and imaging data, and information on diet, drugs, toxins, and other environmental factors. Given the significant advancements in omics technologies, human metabologenomics, and computational capabilities, several multi-omics studies are underway. Here, we first review the recent application of AI and systems biology in integrating and interpreting multi-omics data, highlighting their contributions to the creation of digital twins and the discovery of novel biomarkers and drug targets. Next, we review the multi-omics datasets generated worldwide to reveal interactions across multiple biological layers of information over time, which enhance precision health and medicine. Finally, we address the need to incorporate big biological data into clinical practice, supporting the development of a clinical decision support system essential for AI-driven hospitals and creating the foundation for an AI and systems biology-based healthcare model.""]"
"Arthur Llewellyn Photographer Basham","","","Computer Science","https://openalex.org/A5111014648","[""The centralizing bureaucratic Mauryan Empire produced a political atmosphere very different from that which was to be found in the quasi-feudal kingdoms of later times. Speculations from all the periods have become part of the common Indian heritage. Ancient India allowed considerable freedom of speculation. It is hardly necessary to point out that, as well as the six orthodox philosophical systems, schismatic and heterodox schools of thought flourished freely, and differences in metaphysics and theology were to some extent reflected in the realm of political ideas. The orthodox conception of kingship was certainly the more influential in the thought of pre-Muslim India, and we find even Buddhist and Jain kings laying claim to divinity. Rajya, the term generally translated 'state' and used in that sense in modern Indian languages, is a secondary nominal formation from the word raja, and etymologically implies 'that which pertains to the king'.""]"
"B. Meirose","","","Computer Science","https://openalex.org/A5016994750","[""Abstract We explore the decay of free neutrons into exotic long-lived particles, whose decays could be detected in the next-generation free neutron experiments. We show that such a possibility is viable as long as the exotic particle is highly mass-degenerate with the neutron, avoiding exclusion by large-volume detectors. We estimate the number of observable events and identify the most promising final states from both theoretical and experimental perspectives. Our analysis highlights the unique capability of the HIBEAM-NNBAR experiment at the European Spallation Source to probe this unexplored region of parameter space, opening a new avenue for exploring physics beyond the Standard Model. We estimate that several events per year could be observed in the NNBAR experiment."", ""The European Spallation Source, currently under construction in Lund, Sweden, is a multidisciplinary international laboratory. Once completed to full specifications, it will operate the world’s most powerful pulsed neutron source. Supported by a 3 million Euro Research and Innovation Action within the EU Horizon 2020 program, a design study (HighNESS) has been completed to develop a second neutron source located below the spallation target. Compared to the first source, designed for high cold and thermal brightness, the new source has been optimized to deliver higher intensity, and a shift to longer wavelengths in the spectral regions of cold (CN, 2–20 Å), very cold (VCN, 10–120 Å), and ultracold (UCN, >500 Å) neutrons. The second source comprises a large liquid deuterium moderator designed to produce CN and support secondary VCN and UCN sources. Various options have been explored in the proposed designs, aiming for world-leading performance in neutronics. These designs will enable the development of several new instrument concepts and facilitate the implementation of a high-sensitivity neutron-antineutron oscillation experiment (NNBAR). This document serves as the Conceptual Design Report for the HighNESS project, representing its final deliverable."", ""A key aim of the HighNESS project for the European Spallation Source is to enable cutting-edge particle physics experiments. This volume presents a conceptual design report for the NNBAR experiment. NNBAR would exploit a new cold lower moderator to make the first search in over thirty years for free neutrons converting to anti-neutrons. The observation of such a baryon-number-violating signature would be of fundamental significance and tackle open questions in modern physics, including the origin of the matter-antimatter asymmetry. This report shows the design of the beamline, supermirror focusing system, magnetic and radiation shielding, and anti-neutron detector necessary for the experiment. A range of simulation programs are employed to quantify the performance of the experiment and show how background can be suppressed. For a search with full background suppression, a sensitivity improvement of three orders of magnitude is expected, as compared with the previous search. Civil engineering studies for the NNBAR beamline are also shown, as is a costing model for the experiment."", ""The HIBEAM-NNBAR program is a proposed two-stage experiment at the European Spallation Source (ESS) designed to search for baryon number violation, which is – together with C and CP violation – one of the three fundamental Sakharov conditions to explain the observed baryon asymmetry of the Universe. Taking advantage of the ESS' unique capabilities as the future brightest neutron source, the experiment will make high sensitivity searches for neutrons converting into antineutrons and/or sterile neutrons."", ""The European Spallation Source (ESS), presently under construction in Lund, Sweden, is a multidisciplinary international laboratory that, once completed at full specifications, will operate the world’s most powerful pulsed neutron source. Supported by a 3 M Euro Research and Innovation Action within the European Union Horizon 2020 program, a design study (HighNESS) is now underway to develop a second neutron source located below the spallation target. Compared to the first source, which is located above the spallation target and designed for high cold and thermal brightness, the new source is being optimized to deliver higher intensity and a shift to longer wavelengths in the spectral regions of cold neutrons (CNs) (2 to 20 Å), very cold neutrons (VCNs) (10 to 120 Å), and ultracold neutrons (UCNs) (>500 Å). The second source consists of a large liquid deuterium moderator to deliver CNs and serve secondary VCN and UCN sources, for which different options are under study. These new sources will boost several areas of condensed matter research and will provide unique opportunities in fundamental physics. The HighNESS project is now entering its last year, and we are working toward the Conceptual Design Report of the ESS upgrade. In this paper, results obtained in the first 2 years, ongoing developments, and future perspectives are described.""]"
"Sven Ove Ögren","","","Computer Science","https://openalex.org/A5035523228","[""Background: The beat-by-beat fluctuation of heart rate (HR) in its temporal sequence (HR dynamics) provides information on HR regulation by the autonomic nervous system (ANS) and its dysregulation in pathological states. Commonly, linear analyses of HR and its variability (HRV) are used to draw conclusions about pathological states despite clear statistical and translational limitations. Objective: The main aim of this study was to compare linear and nonlinear HR measures, including detrended fluctuation analysis (DFA), based on ECG recordings by radiotelemetry in C57BL/6N mice to identify pathological HR dynamics. Methods: We investigated different behavioral and a wide range of pharmacological interventions which alter ANS regulation through various peripheral and/or central mechanisms including receptors implicated in psychiatric disorders. This spectrum of interventions served as a reference system for comparison of linear and nonlinear HR measures to identify pathological states. Results: Physiological HR dynamics constitute a self-similar, scale-invariant, fractal process with persistent intrinsic long-range correlations resulting in physiological DFA scaling coefficients of α~1. Strongly altered DFA scaling coefficients (α ≠ 1) indicate pathological states of HR dynamics as elicited by (1) parasympathetic blockade, (2) parasympathetic overactivation and (3) sympathetic overactivation but not inhibition. The DFA scaling coefficients are identical in mice and humans under physiological conditions with identical pathological states by defined pharmacological interventions. Conclusions: Here, we show the importance of tonic vagal function for physiological HR dynamics in mice, as reported in humans. Unlike linear measures, DFA provides an important translational measure that reliably identifies pathological HR dynamics based on altered ANS control by pharmacological interventions. Central ANS dysregulation represents a likely mechanism of increased cardiac mortality in psychiatric disorders."", ""Abstract Background Major depressive disorder (MDD) is defined as a complex mental disorder which is characterized by a pervasive low mood and aversion to activity. Several types of neurotransmitter systems e.g. serotonergic, glutamatergic and noradrenergic systems have been suggested to play an important role in the origination of depression, but neurotrophins such as brain derived neurotrophic factor (BDNF) have also been implicated in the disease process. Objectives The purpose of this study was to examine the effects of a newly developed class of molecules, characterized as positive allosteric modulators of neurotrophin/Trk receptor mediated signaling (Trk-PAM), on neurotransmitter release and depression-like behavior in vivo. Methods The effect of and possible interaction of neurotrophin/Trk signaling pathways with serotonergic and glutamatergic systems in the modulation of depression-related responses was studied using newly developed Trk-PAM compounds (ACD855, ACD856 and AC26845), as well as ketamine and fluoxetine in the forced swim test (FST) in rodents. Moreover, in vivo microdialysis in freely moving rats was used to assess changes in neurotransmitter levels in the rat. Results The results from the study show that several different compounds, which all potentiate Trk-receptor mediated signaling, display antidepressant-like activity in the FST. Moreover, the data also indicate that the effects of both fluoxetine and ketamine in the FST, both used in clinical practice, are mediated via BDNF/TrkB signaling, which could have implications for novel therapies in MDD. Conclusions Trk-PAMs could provide an interesting avenue for the development of novel therapeutics in this area."", ""There is evidence that interaction between the neuropeptide galanin and the 5-HT1A receptor represents an integrative mechanism in the regulation of serotonergic neurotransmission. Thus, in rats intracerebroventricular (i.c.v.) galanin did not impair retention in the passive avoidance (PA) test 24 h after training, but attenuated the retention deficit caused by subcutaneous (s.c.) administration of the 5-HT1A receptor agonist 8-OH-DPAT. This impairment has been linked to postsynaptic 5-HT1A receptor activation. To confirm these results in mice, galanin was infused i.c.v. (1 nmol/mouse) in C57BL/6/Bkl mice 30 min prior to training followed by s.c. injection (0.3 mg/kg) of 8-OH-DPAT or saline 15 min before PA training. In line with previous results, i.c.v. galanin significantly attenuated the PA impairment caused by 5-HT1A receptor activation in mice. To study if the galanin 5-HT1A receptor interaction involved the dorsal hippocampus, galanin (1 nmol/mouse) was directly infused into this brain region alone or in combination with s.c. 8-OH-DPAT. However, unlike i.c.v. galanin, galanin infusion into the dorsal hippocampus alone impaired PA retention and failed to attenuate the 8-OH-DPAT-mediated PA impairment. These results indicate that the ability of i.c.v. galanin to modify 5-HT1A receptor activation is not directly mediated via receptor interactions in the dorsal hippocampus. Instead, the galanin-mediated PA impairment suggests an important inhibitory role of galanin receptors in the dorsal hippocampus for acquisition (encoding) and/or consolidation of emotional memory. In addition, the interaction between galanin and 5-HT1A receptors probably involves a wide serotonergic network that is important for the integration of emotional and cognitive behaviors."", ""Abstract Adult neurogenesis, the production of newborn neurons from neural stem cells (NSCs) has been suggested to be decreased in patients with schizophrenia. A similar finding was observed in an animal model of schizophrenia, as indicated by decreased bromodeoxyuridine (BrdU) labelling cells in response to a non-competitive N-methyl-d-aspartate (NMDA) receptor antagonist. The antipsychotic drug clozapine was shown to counteract the observed decrease in BrdU-labelled cells in hippocampal dentate gyrus (DG). However, phenotypic determination by immunohistochemistry analysis could not reveal whether BrdU-positive cells were indeed NSCs. Using a previously established cell model for analysing NSC protection in vitro, we investigated a protective effect of clozapine on NSCs. Primary NSCs were isolated from the mouse subventricular zone (SVZ), we show that clozapine had a NSC protective activity alone, as evident by employing an ATP cell viability assay. In contrast, haloperidol did not show any NSC protective properties. Subsequently, cells were exposed to the non-competitive NMDA-receptor antagonist ketamine. Clozapine, but not haloperidol, had a NSC protective/anti-apoptotic activity against ketamine-induced cytotoxicity. The observed NSC protective activity of clozapine was associated with increased expression of the anti-apoptotic marker Bcl-2, decreased expression of the pro-apoptotic cleaved form of caspase-3 and associated with decreased expression of the autophagosome marker 1A/1B-light chain 3 (LC3-II). Collectively, our findings suggest that clozapine may have a protective/anti-apoptotic effect on NSCs, supporting previous in vivo observations, indicating a neurogenesis-promoting activity for clozapine. If the data are further confirmed in vivo, the results may encourage an expanded use of clozapine to restore impaired neurogenesis in schizophrenia.""]"
"L. Eklund","","","Computer Science","https://openalex.org/A5106498434","[""Abstract The LHCb detector has undergone a major upgrade for LHC Run 3. This Upgrade I detector facilitates operation at higher luminosity and utilises full-detector information at the LHC collision rate, critically including the use of vertex information. A new vertex locator system, the VELO Upgrade, has been constructed. The core element of the new VELO are the double-sided pixelated hybrid silicon detector modules which operate in vacuum close to the LHC beam in a high radiation environment. The construction and quality assurance tests of these modules are described in this paper. The modules incorporate 200 μm thick, n-on-p silicon sensors bump-bonded to 130 nm technology ASICs. These are attached with high precision to a silicon microchannel substrate that uses evaporative CO 2 cooling. The ASICs are controlled and read out with flexible printed circuits that are glued to the substrate and wire-bonded to the chips. The mechanical support of the module is given by a carbon fibre plate, two carbon fibre rods and an aluminium plate. The sensor attachment was achieved with an average precision of 21 μm, more than 99.5% of all pixels are fully functional, and a thermal figure of merit of 3 Kcm 2 W -1 was achieved. The production of the modules was successfully completed in 2021, with the final assembly and installation completed in time for data taking in 2022."", ""The LHCb detector has undergone a major upgrade for LHC Run 3. This Upgrade I detector facilitates operation at higher luminosity and utilises full-detector information at the LHC collision rate, critically including the use of vertex information. A new vertex locator system, the VELO Upgrade, has been constructed. The core element of the new VELO are the double-sided pixelated hybrid silicon detector modules which operate in vacuum close to the LHC beam in a high radiation environment. The construction and quality assurance tests of these modules are described in this paper. The modules incorporate 200 \\mum thick, n-on-p silicon sensors bump-bonded to 130 \\nm technology ASICs. These are attached with high precision to a silicon microchannel substrate that uses evaporative CO$_2$ cooling. The ASICs are controlled and read out with flexible printed circuits that are glued to the substrate and wire-bonded to the chips. The mechanical support of the module is given by a carbon fibre plate, two carbon fibre rods and an aluminium plate. The sensor attachment was achieved with an average precision of 21 $\\mathrm{\\mu m}$, more than 99.5\\% of all pixels are fully functional, and a thermal figure of merit of 3 \\mathrm{Kcm^{2}W^{-1}}$ was achieved. The production of the modules was successfully completed in 2021, with the final assembly and installation completed in time for data taking in 2022."", ""Abstract The thermal properties of the LHCb Vertex Locator (VELO) are studied using the real-time detector alignment procedure. The variation of the position and orientation of the detector elements as a function of the operating temperature of the VELO is presented. This study uses a dataset collected by the LHCb experiment during a VELO temperature scan performed at the end of LHC Run 2 (October 2018). Significant shrinkage of the VELO modules is observed at the operating temperature of -30°C compared to the laboratory measurements on a single module taken at a range of temperatures from +45°C to -25°C. The thermal shrinkage expected from the extrapolation of laboratory measurements to lower temperatures, and the results of this alignment study are in good agreement."", ""The NNBAR experiment for the European Spallation Source will search for free neutrons converting to antineutrons with an expected sensitivity improvement of three orders of magnitude compared to the last such search. This paper describes both the simulations of a key component for the experiment, the neutron optical reflector and the expected gains in sensitivity.""]"
"A. Kupść","","","Computer Science","https://openalex.org/A5002445204","[""We propose a novel approach to measure the spin polarization of protons produced in electron-positron collisions. Using existing tracking devices and supporting structure material, general-purpose spectrometers can be utilized as a large-acceptance polarimeter without a hardware upgrade. With the proposed approach, the spin polarization of protons can be revealed, providing a complementary and accurate description of the final-state particles. This could have far-reaching implications, such as enabling the complete determination of the nucleon timelike electromagnetic form factors."", ""We present the results of Phase I of an ongoing review of Monte Carlo tools relevant for low-energy hadronic cross sections. This includes a detailed comparison of Monte Carlo codes for electron–positron scattering into a muon pair, pion pair, and electron pair, for scan and radiative-return experiments. After discussing the various approaches that are used and effects that are included, we show differential cross sections obtained with AfkQed, BabaYaga@NLO, KKMC, MCGPJ, McMule, Phokhara, and Sherpa, for scenarios that are inspired by experiments providing input for the dispersive evaluation of the hadronic vacuum polarisation."", ""Abstract Decays of charmonium into hyperon and antihyperon pairs provide a pristine laboratory for exploring hyperon properties, such as their polarization and decay parameters, and for conducting tests of fundamental symmetries. This brief review highlights the significant progress made in precise tests of CP symmetry at BESIII using entangled hyperon-antihyperon pairs, including ΛΛ, ΣΣ, ΞΞ and ΛΣ, selected from the high statistics of J/ψ and ψ (3686) events produced in e + e - annihilations. These recent findings have sparked renewed interest in both theoretical and experimental aspects of hyperon physics, but there is still much room for improvement to reach the Standard Model expectations. To address this challenge, the prospects for future investigations on CP asymmetry at next-generation experiments are discussed."", ""Decays of charmonium into hyperon and antihyperon pairs provide a pristine laboratory for exploring hyperon properties, such as their polarization and decay parameters, and for conducting tests of fundamental symmetries. This brief review highlights the significant progress made in precise tests of CP symmetry at BESIII using entangled hyperon-antihyperon pairs, including $\\Lambda\\bar{\\Lambda}$, $\\Sigma\\bar{\\Sigma}$, $\\Xi\\bar{\\Xi}$ and $\\Lambda\\bar{\\Sigma}$, selected from the high statistics of $J/\\psi$ and $\\psi(3686)$ events produced in $e^+e^-$ annihilations. These recent findings have sparked renewed interest in both theoretical and experimental aspects of hyperon physics, but there is still much room for improvement to reach the Standard Model expectations. To address this challenge, the prospects for future investigations on CP asymmetry at next-generation experiments are discussed.""]"
"V. Vorobyev","","","Computer Science","https://openalex.org/A5061158447","[""The paper is devoted to the problem of state variables observers synthesis for linear stationary system operating under condition of noise or disturbances in the measurement channel. The paper considers a completely observable linear stationary system with known parameters. It is assumed that the state variables are not measured, and the measured output variable contains a small amplitude (in general, modulo less than one) additive noise or disturbance. It is also assumed that there is no a priori information about the disturbance or noise in the measurement channel (for example, frequency spectrum, covariance, etc.). It is well known that many observer synthesis methods have been obtained for this type of systems, including the Kalman filter, which has proven itself in practice. Under the condition of complete observability and the presence of some a priori information about a random process (which is typical for the case when a disturbance in the measurement channel can be represented as white noise), approaches based on Kalman filtering demonstrate the highest quality estimates of state variables convergence to true values. Without disputing the numerous results obtained using the application of the Kalman filter, an alternative idea of the state variables observer constructing is considered in this paper. The alternative of the new approach is primarily due to the fact that there is no need to use the usual approaches based on the Luenberger observer. The paper proposes an approach based on the estimation of unknown parameters (in this case, an unknown vector of initial conditions of the plant state variables) of a linear regression model. Within the framework of the proposed method, after a simple transformation, a transition is made from a dynamic system to a linear regression model with unknown constant parameters containing noise or disturbing effects. After that, a new nonlinear parametrization of the original regression model and an algorithm for identifying unknown constant parameters using the procedure of dynamic expansion of the regressor and mixing are proposed which ensure reduction the influence of noise. The article presents the results of computer simulations verifying the stated theoretical results."", ""Аннотация.Рассмотрено классическое линейное регрессионное уравнение, содержащее в левой и правой частях: измеряемый сигнал и сумму из n слагаемых, состоящих из произведения неизвестных параметров и известных функций (регрессоров).Отличительной особенностью рассматриваемого уравнения, по сравнению с классическим, является допущение о том, что неизвестные параметры являются нелинейными комбинациями от одного, а именно: каждый из неизвестных параметров является числом, полученным при возведении в степень одного неизвестного параметра""]"
"Olle Eriksson","","","Computer Science","https://openalex.org/A5064187188","[""LiFe6Ge4, with a theoretically predicted saturation magnetization of 1 T, a magnetocrystalline anisotropy energy of 1.78 MJ/m3 and a Curie temperature of 620 K was suggested to be a promising permanent magnet as an outcome of a data-mining search. Magnetic measurements of the synthesized sample are reported here. Unfortunately, experiments revealed a weak ferromagnetic behaviour with magnetization values much below that predicted by theory. This discrepancy is analyzed in detail, and is attributed to the trigonal crystal symmetry that was missed in the previous characterisation of the material. The correct crystal structure is R 3‾ mH (space group 166) and it is found here to have an antiferromagnetic ground state, as opposed to a theoretically predicted ferromagnetic state of the previously reported monoclinic crystal structure. Theoretical calculations show that element substitution can stabilize a ferromagnetic state of the trigonal crystal structure, with high values of saturation magnetization and magnetocrystalline anisotropy. The best results are seen for the Al or Ga substitution for Ge of the LiFe6 X 4 compound."", ""The nonstoichiometric <a:math xmlns:a=\""http://www.w3.org/1998/Math/MathML\""><a:mrow><a:msub><a:mi>Fe</a:mi><a:mn>2</a:mn></a:msub><a:mi mathvariant=\""normal\"">P</a:mi></a:mrow></a:math>-type <c:math xmlns:c=\""http://www.w3.org/1998/Math/MathML\""><c:mrow><c:msub><c:mi>FeMn</c:mi><c:mrow><c:mo>(</c:mo><c:mn>1</c:mn><c:mo>−</c:mo><c:mi>x</c:mi><c:mo>)</c:mo></c:mrow></c:msub><c:msub><c:mi mathvariant=\""normal\"">V</c:mi><c:mi>x</c:mi></c:msub><c:msub><c:mrow><c:mo>(</c:mo><c:msub><c:mi mathvariant=\""normal\"">P</c:mi><c:mrow><c:mn>0.5</c:mn></c:mrow></c:msub><c:msub><c:mi>Si</c:mi><c:mrow><c:mn>0.5</c:mn></c:mrow></c:msub><c:mo>)</c:mo></c:mrow><c:mrow><c:mn>1</c:mn><c:mo>−</c:mo><c:mi>x</c:mi></c:mrow></c:msub></c:mrow></c:math> alloys <f:math xmlns:f=\""http://www.w3.org/1998/Math/MathML\""><f:mrow><f:mo>(</f:mo><f:mi>x</f:mi><f:mo>=</f:mo><f:mn>0</f:mn><f:mo>,</f:mo><f:mn>0.01</f:mn><f:mo>,</f:mo><f:mo> </f:mo><f:mn>0.02</f:mn><f:mo>,</f:mo><f:mo> </f:mo><f:mtext>and</f:mtext><f:mo> </f:mo><f:mn>0.03</f:mn><f:mo>)</f:mo></f:mrow></f:math> have been investigated as potential candidates for magnetic refrigeration near room temperature. The magnetic ordering temperature decreases with increasing FeV concentration <g:math xmlns:g=\""http://www.w3.org/1998/Math/MathML\""><g:mi>x</g:mi></g:math>, which can be ascribed to decreased ferromagnetic coupling strength between the magnetic atoms. The strong magnetoelastic coupling in these alloys results in large values of the isothermal entropy change <h:math xmlns:h=\""http://www.w3.org/1998/Math/MathML\""><h:mrow><h:mo>(</h:mo><h:mi mathvariant=\""normal\"">Δ</h:mi><h:msub><h:mi>S</h:mi><h:mi>M</h:mi></h:msub><h:mo>)</h:mo></h:mrow></h:math>; 15.7 J/(kg K), at 2 T magnetic field for the <j:math xmlns:j=\""http://www.w3.org/1998/Math/MathML\""><j:mrow><j:mi>x</j:mi><j:mo>=</j:mo><j:mn>0</j:mn></j:mrow></j:math> alloy. <k:math xmlns:k=\""http://www.w3.org/1998/Math/MathML\""><k:mrow><k:mi mathvariant=\""normal\"">Δ</k:mi><k:msub><k:mi>S</k:mi><k:mi>M</k:mi></k:msub></k:mrow></k:math> decreases with increasing <m:math xmlns:m=\""http://www.w3.org/1998/Math/MathML\""><m:mi>x</m:mi></m:math>. Results from Mössbauer spectroscopy reveal that the average hyperfine field (in the ferromagnetic state) and average center shift (in the paramagnetic state) have the same decreasing trend as <n:math xmlns:n=\""http://www.w3.org/1998/Math/MathML\""><n:mrow><n:mi mathvariant=\""normal\"">Δ</n:mi><n:msub><n:mi>S</n:mi><n:mi>M</n:mi></n:msub></n:mrow></n:math>. The thermal hysteresis <p:math xmlns:p=\""http://www.w3.org/1998/Math/MathML\""><p:mrow><p:mo>(</p:mo><p:mi mathvariant=\""normal\"">Δ</p:mi><p:msub><p:mi>T</p:mi><p:mi>hyst</p:mi></p:msub><p:mo>)</p:mo></p:mrow></p:math> of the magnetic phase transition decreases with increasing <r:math xmlns:r=\""http://www.w3.org/1998/Math/MathML\""><r:mi>x</r:mi></r:math>, while the mechanical stability of the alloys improves due to the reduced lattice volume change across the magnetoelastic phase transition. The adiabatic temperature change <s:math xmlns:s=\""http://www.w3.org/1998/Math/MathML\""><s:mrow><s:mi mathvariant=\""normal\"">Δ</s:mi><s:msub><s:mi>T</s:mi><s:mi>ad</s:mi></s:msub></s:mrow></s:math>, which highly depends on <u:math xmlns:u=\""http://www.w3.org/1998/Math/MathML\""><u:mrow><u:mi mathvariant=\""normal\"">Δ</u:mi><u:msub><u:mi>T</u:mi><u:mi>hyst</u:mi></u:msub></u:mrow></u:math>, is 1.7 K at 1.9 T applied field for the <w:math xmlns:w=\""http://www.w3.org/1998/Math/MathML\""><w:mrow><w:mi>x</w:mi><w:mo>=</w:mo><w:mn>0.02</w:mn></w:mrow></w:math> alloy. Published by the American Physical Society 2025""]"
"Reinhilde Jacobs","","","Computer Science","https://openalex.org/A5041955556","[""Medication-related osteonecrosis of the jaw (MRONJ) is an adverse event often associated with the use of antiresorptive drugs. This systematic review aims to identify genes and their polymorphisms associated with the risk of developing MRONJ in patients with oncological or skeletal-related diseases treated with antiresorptive drugs. A systematic literature review was conducted in accordance with PRISMA guidelines. Three electronic databases (PubMed, Scopus, and Web of Science) were searched for studies published up to December 2024. The search strategy included terms such as \""MRONJ\"", \""bone density conservation agents\"", and \""genetics\"". Eligible studies were case control in design, investigating genetic polymorphisms in MRONJ patients compared to controls, which included either healthy individuals or patients receiving antiresorptive drugs without developing MRONJ. Study quality was assessed using the Q-genie tool for genetic association studies. Out of 833 retrieved articles, 27 met the inclusion criteria for qualitative analysis. Most studies were of 'fair' quality, with common limitations including small sample sizes, suboptimal control group selection, and ethnic heterogeneity within study cohorts. A total of 136 genetic variants across 58 genes were identified, many of which are involved in biological processes such as immune response and inflammation, cellular function, bone homeostasis, and angiogenesis. However, due to inconsistent findings and a lack of replication across studies, no definitive conclusions regarding specific genetic risk factors could be drawn. The findings of this review suggest that MRONJ susceptibility is likely influenced by multiple variants affecting interconnected biological pathways, particularly involved in immunity, metabolism, angiogenesis, and bone remodelling."", ""Nanoparticles are emerging as transformative agents in endodontics, addressing challenges in treating the dentin-pulp complex. This scoping review aims to explore multifunctional applications of nanoparticles in endodontics, with a focus on their roles in promoting tissue regeneration through therapeutic effects, enhancing material properties, and serving a carrier function. Following PRISMA-Scoping Review guidelines, a comprehensive literature search was conducted across Web of Science, PubMed, and Scopus. A total of 490 articles were initially identified, of which 92 met the preliminary eligibility criteria. Following full-text screening, 70 studies were included in the qualitative synthesis. Key findings from both in vitro and in vivo studies are summarized in tabular form. Results reveal a notable imbalance in the types of nanomaterials studied: inorganic nanomaterials were reported in 77% of the studies, while only 23% investigated organic nanomaterials. Despite their lower representation, organic nanomaterials demonstrated considerable relevance. Chitosan was reported in 29% of the carrier studies, while extracellular vesicles were featured in 22% of the therapeutic applications. Among inorganic materials, bioactive glass was frequently reported, appearing in 31% of enhancer-related studies, 26% of therapeutic studies, and 13% of those investigating carrier functions. The end applications of these nanoparticles were in 69% of the studies either (direct) pulp capping or root canal filling, highlighting the need for innovative materials in these applications. Regarding experimental models, 75% of the studies conducted in vitro research on relevant cell lines, while 25% employed animal models. Of those 25% in vivo studies, 18% also reported in vitro findings. Nanoparticles hold significant promise for transforming endodontics, offering enhanced antibacterial efficacy and bioactivity while addressing critical limitations of conventional materials. However, challenges remain regarding their long-term biocompatibility, scalability, and integration into clinical workflows. This review emphasizes the need for translational research to bridge the gap between laboratory innovations and clinical practice."", ""More than 1 billion individuals worldwide have experienced dental trauma, particularly children aged 7 to 12 y, predominantly affecting the anterior teeth, which has a significant impact on oral health and esthetics. Rapid emergency restorations using composite resin are followed by medium-term lab-fabricated mock-ups. Recent advancements in artificial intelligence (AI) assist dental restorations, and the objective of this study was to compare the performances of different AI approaches for the learning and reconstruction of central incisors. The study was approved by ethical committees and followed AI in dentistry recommendations. STL files of mature permanent maxillary incisors without severe wear were collected from 3 universities. Principal component analysis (PCA) and Deep Learning of Signed Distance Functions (DeepSDF) models were trained using these files. The learning of PCA and DeepSDF approaches were 3-fold cross-validated, and their performances were assessed using the following metrics to measure the reconstruction accuracy: the difference of surfaces, volumes, lengths, average Euclidian distance, Hausdorff distance, and crown–root angulations. Explainability was assessed using feature contribution analysis for PCA and Stochastic Neighbor Embedding (t-SNE) for DeepSDF. DeepSDF showed significantly better precision in surface, volume, and Hausdorff distance metrics compared with PCA. For reconstructions, the lower size of the latent code of the DeepSDF model demonstrated lower performances compared with higher sizes. In addition, DeepSDF raised concerns about explainability. This study demonstrates the potential of PCA and DeepSDF approaches, particularly DeepSDF, for the learning and reconstruction of the anatomy of upper central incisors. To foster trust and acceptance, future research should, however, focus on improving the explainability of DeepSDF models and considering a broader range of factors that influence smile design. These high performances suggest potential clinical applications, such as assisting practitioners in future smile designs and oral rehabilitation using AI approaches.""]"
"Cynthia M. Bulik","","","Computer Science","https://openalex.org/A5003258692","[""According to DSM-5-TR, avoidant/restrictive food intake disorder (ARFID) cannot be diagnosed alongside anorexia nervosa (AN), bulimia nervosa (BN), or any other body image disturbance. This does not accurately reflect real-world symptomatology and recent research findings, indicating the potential need to revise DSM-5-TR Criteria. In this study, we investigated the co-occurrence of weight- and/or shape-motivated restriction (WSR) in a large sample of adults who screened positive for ARFID, thereby providing evidence to inform such changes. The sample comprised 5,747 adults who consented to participate in the ARFID-Genes and Environment (ARFID-GEN) research study, screened positive for ARFID on the NIAS and PARDI-AR-Q, and completed the EDE-Q. Based on EDE-Q responses, participants were placed into four groups: groups one and two additionally screened positive for AN (ARFID-AN; n=147) or BN (ARFID-BN; n=193), group three also endorsed significant WSR without meeting AN or BN screening criteria (ARFID-WSR; n=2,097), and group four endorsed ARFID symptoms only (ARFID-nWSR; n=3,310). We used generalized linear models to test group differences on the NIAS, PARDI-AR-Q and EDE-Q. ARFID-nWSR showed lower scores than all other groups across most ARFID dimensions on the NIAS and PARDI-AR-Q, as well as lower odds of meeting DSM-5-TR Criteria A1 to A4 (i.e., weight loss; nutritional deficiencies; dependence on nutritional supplements; significant interference with psychosocial functioning) . These findings indicate the presence of a mixed phenotype with features of both ARFID and WSR that is associated with more severe ARFID symptomatology as measured by NIAS and PARDI-AR-Q in the general population. The current DSM-5-TR Criteria may not accurately capture complex real-world symptomatology in adults with probable ARFID, potentially precluding those with the most severe symptoms from receiving diagnoses that accurately reflect their clinical presentation and from accessing appropriate care."", ""Abstract Rare copy number variants (CNVs) are a key component of the genetic basis of psychiatric conditions, but have not been well characterized for most. We conducted a genome-wide CNV analysis across six diagnostic categories (N = 574,965): autism (ASD), ADHD, bipolar disorder (BD), major depressive disorder (MDD), PTSD, and schizophrenia (SCZ). We identified 35 genome-wide significant associations at 18 loci, including novel associations in SCZ ( SMYD3, USP7 - HAPSTR1 ) and in the combined cross-disorder analysis ( ASTN2 ). Rare CNVs accounted for 1–3% of heritability across diagnoses. In ASD, associations were uniformly positive, consistent with autism having diverse etiologies and clinical presentations. By contrast, CNVs showed a dose-dependent relationship for other diagnoses, including SCZ and PTSD, with reciprocal deletions and duplications having inversely correlated effects and distinct genotype-phenotype relationships. Our findings suggest that genes have effects that are both dose-dependent and pleiotropic, such that a positive influence on one dimension of psychopathology may be accompanied by positive or negative effects on others."", ""Psychiatric conditions share common genes, but mechanisms that differentiate diagnoses remain unclear. We present a multidimensional framework for functional analysis of rare copy number variants (CNVs) across 6 diagnostic categories, including schizophrenia (SCZ), autism (ASD), bipolar disorder (BD), depression (MDD), PTSD, and ADHD (N = 574,965). Using gene-set burden analysis (GSBA), we tested duplication (DUP) and deletion (DEL) burden across 2,645 functional gene sets defined by the intersections of pathways, cell types, and cortical regions. While diagnoses converge on shared pathways, mixed-effects modeling revealed divergence of pathway effects by cell type, brain region, and gene dosage. Factor analysis identified latent dimensions aligned with clinical axes. A primary factor (F1) captured reciprocal dose-dependent effects of DUP and DEL in SCZ reflecting positive and negative effects in excitatory versus inhibitory neurons and association versus sensory cortex. SCZ and ASD were both strongly aligned with F1 but with opposing directionalities. Orthogonal factors highlighted neuronal versus non-neuronal effects in mood disorders (F2) and differential spatial distributions of DEL effects in ADHD and MDD (F3). High-impact CNVs at 16p11.2 and 22q11.2 were enriched for combinations of cell-type-specific genes involved in pathways consistent with our broader findings. These results reveal molecular and cellular mechanisms that are broadly shared across psychiatric traits but differ between diagnostic categories in context and directionality."", ""Eating disorders arise from a complex interaction of genetic and environmental influences. Here we provide comprehensive population-level estimates of the heritability of eating disorders and their genetic relationships with various mental health and cardiometabolic disorders (CMDs), expanding beyond genome-wide association studies. We examined the heritability of three eating disorders-anorexia nervosa (AN), bulimia nervosa (BN), and other eating disorders (OED)-and investigated shared familial and genetic risk factors with mental health disorders and CMDs. Using national register data from Denmark and Sweden (1972-2016), we analysed clinical diagnoses for over 67,000 individuals with eating disorders, their first-degree relatives, and matched controls from populations totalling 17 million. Heritability estimates were moderate, h2AN = 36%, h2BN = 39%, and h2OED = 30% and genetic correlations revealed substantial overlap between AN and obsessive-compulsive disorder (rg = 0.65) and moderate correlations with other mental health disorders such as autism (rg = 0.36). Significant genetic associations were also identified between eating disorders and CMDs, showing strong replication across both countries. These findings emphasise the genetic foundations of eating disorders and their shared genetic architecture with mental health and CMDs. This research enhances our understanding of comorbidity patterns and has important implications for developing integrated treatment approaches."", ""The Psychiatric Genomics Consortium (PGC) has fueled discoveries of common and rare genetic variation contributing to liability to many psychiatric and neurodevelopmental conditions. This narrative review reflects on major findings from the past half decade of research by this international group of investigators in five priority areas: discovery of common variants using GWAS; rare variation and its interplay with polygenic risk; leveraging genetics to go beyond diagnostic boundaries; ascribing functional attributes to genomic discoveries; and developing and implementing processes for data sharing, outreach to various communities, and training. The insights gained in these domains frame the agenda for the next phase of PGC research. In addition to accelerating integrative common and rare variant-, within- and across-disorder findings for multiple psychiatric and neurodevelopmental conditions, the next phase will leverage multiple populations to further elucidate genetic etiologies, integrate results with rapidly accumulating multi-modal functional genomics data to gain mechanistic understanding, bring genetic findings to clinically actionable phenotypes such as treatment response, and address the emerging use of polygenic scores. Taken together, these next steps will illuminate the biological underpinnings of psychiatric disorders, which continue to contribute to global morbidity and mortality.""]"
"Nancy L. Pedersen","","","Computer Science","https://openalex.org/A5011350655","[""Abstract Background Polygenic scores (PGSs) may help assess genetic predisposition to multifactorial traits. We examined whether age, sex, and leisure-time physical activity (LTPA) modify the association between a PGS for handgrip strength (HGS) and measured HGS in older adults. Methods PGS HGS, based on Pan-UK Biobank GWAS data, was calculated for 5103 participants (aged 40–96; 44% women) from eight twin cohorts in Denmark, Sweden, Australia, the United States, and Finland within the IGEMS consortium. Sex-standardized HGS and self-reported LTPA were assessed cross-sectionally. Linear mixed models estimated associations between PGS and HGS, including interactions with age, country, and LTPA, as well as an association between PGS and LTPA. Fixed-effect within-pair models were conducted to assess environmental contributions. Results Higher PGS was associated with greater HGS (β = 2.14, SE = 0.15, p &lt; 0.001), explaining 4.6% of HGS variance overall, with modest variation across countries. In sex-stratified models, PGS explained 5.2% of the variance in females and 4.3% in males. A significant PGS × LTPA interaction (β = –0.034, p = 0.013) indicated that LTPA had a stronger effect among individuals with lower PGS HGS. No statistically significant interaction with age was found. The within-pair models offered limited support for the environmental impact of LTPA. Conclusions PGS for HGS was associated with measured HGS, with effect modification by LTPA. Findings provide some evidence that physical activity may buffer against genetic predisposition to lower HGS. The results highlight the potential of PGSs to capture individual differences in strength-related traits across populations."", ""Abstract Low educational attainment is recognized as a modifiable risk factor for dementia. Despite the commonly accepted notion that greater educational attainment confers lower dementia risk, few family-based studies have investigated the causal bases for the association. Using data from seven twin samples from Sweden, Denmark, Australia, and the US participating in the IGEMS (Interplay of Genes and Environment in Multiple Studies) consortium ( N = 60,027, 10.92% with dementia), we tested whether twins who achieve higher education than their co-twins have lower risk of dementia. The primary analysis applied a multilevel between-within regression framework, supported by descriptive statistics of within-pair differences. Results confirmed an overall association between educational attainment and dementia risk, such that individuals with higher educational attainment had less likelihood of developing dementia (phenotypic regression coefficient = -0.68, p &lt;.0001). Within twin pairs, however, twins who achieved greater education than their co-twins did not uniformly show lower dementia risk (within-family regression coefficient = -0.07, p =.0983, while between-family regression coefficient = -0.98, p &lt;.0001). Taken together, the pattern of results shows that the effect of educational attainment on dementia risk is largely attributable to genetic influences in common to educational attainment and dementia, although there are also contributions from environmental influences shared between members of the same family. Results were similar in men and women. These findings add to the literature by using a co-twin control design to address possible reasons that low educational attainment is associated with greater dementia risk."", ""In later older adulthood, individuals report increased depressive symptoms, whereas gender differences in depressive symptoms narrow. We evaluated whether terminal decline (i.e., accelerated worsening in proximity to death) explained these patterns. We examined the longitudinal trajectories of depressive symptoms in 2,411 participants (baseline age: 29–95 years) from the Interplay of Genes and Environments Across Multiple Studies consortium representing three countries (Sweden, Denmark, and Australia). Joint modeling revealed that individuals reporting larger annual increases in depressive symptoms after age 70 were at increased risk of death. Piecewise linear multilevel models with random changepoints revealed accelerated increases in depressive symptoms approximately 4 years before death. Co-twin control analyses with 98 twin pairs found that the deceased twin had significantly larger accelerations in depressive symptoms compared with the surviving twin. Men experienced more severe mortality-related increases compared with women. Terminal decline partially explains the increase in depressive symptoms in later older adulthood."", ""Advanced age is the most important risk factor for dementia. Measures of biological ageing such as DNA methylation age (DNAmAge) can give more information about the accumulation of age-related molecular damage in different organs than chronological age alone. Using post-mortem brain tissue from Swedish Twin Registry participants, we explored the relationship between lifestyle factors, dementia and DNAmAge measures from prefrontal cortex and cerebellum (n = 27 individuals) and paired blood samples (n = 20 individuals). We observed that smoking was associated with a higher DNAmAge deviation (PCBrainAge + 6.4 years in prefrontal cortex, CI [2.5, 10.3], p = 0.004). Conversely, a longer time spent in formal education was associated with a lower DNAmAge deviation (DNAmClockCortical - 4.8 years in prefrontal cortex, CI [-7.9, -1.8], p = 0.007). We found no significant differences between DNAmAge deviation of dementia cases versus controls, though among dementia cases there was a tendency towards higher DNAmClockCortical deviation in prefrontal cortex for those with a more advanced Braak stage on histopathological assessment (+ 3.4 years, CI [-0.68, 7.50], p = 0.13). There were no clear associations between DNAmAge from brain and blood samples collected prior to death. In summary, these data highlight the impact of smoking and education on biomarkers of brain ageing and emphasise the role for organ-specific biomarkers of ageing.""]"
"K. Jon-And","","","Computer Science","https://openalex.org/A5018094254","[""Silicon pixel detectors are at the core of the current and planned upgrade of the ATLAS experiment at the LHC. Given their close proximity to the interaction point, these detectors will be exposed ..."", ""This Technical Design Report describes the project to upgrade the ATLAS Tile Calorimeter for the operation at the High Luminosity LHC. The High Luminosity LHC is planned to begin operation in 2026 ..."", ""The ATLAS Collaboration measures the inclusive production of Z bosons via their decays into electron and muon pairs in p+Pb collisions at sNN=5.02TeV at the Large Hadron Collider. The measurements are made using data corresponding to integrated luminosities of 29.4 and 28.1 nb−1 for Z→ee and Z→μμ, respectively. The results from the two channels are consistent and combined to obtain a cross section times the Z→ℓℓ branching ratio, integrated over the rapidity region |yZ*|<3.5, of 139.8±4.8(statistical)±6.2(systematic)±3.8 (luminosity) nb. Differential cross sections are presented as functions of the Z boson rapidity and transverse momentum and compared with models based on parton distributions both with and without nuclear corrections. The centrality dependence of Z boson production in p+Pb collisions is measured and analyzed within the framework of a standard Glauber model and the model's extension for fluctuations of the underlying nucleon-nucleon scattering cross section.2 MoreReceived 23 July 2015Revised 23 September 2015DOI:https://doi.org/10.1103/PhysRevC.92.044915This article is available under the terms of the Creative Commons Attribution 3.0 License. Further distribution of this work must maintain attribution to the author(s) and the published article's title, journal citation, and DOI.©2015 CERN, for the ATLAS Collaboration"", ""This paper summarises the mechanical construction and installation of the Tile Calorimeter for the ATLAS experiment at the Large Hadron Collider in CERN, Switzerland. The Tile Calorimeter is a sampling calorimeter using scintillator as the sensitive detector and steel as the absorber and covers the central region of the ATLAS experiment up to pseudorapidities ±1.7. The mechanical construction of the Tile Calorimeter occurred over a period of about 10 years beginning in 1995 with the completion of the Technical Design Report and ending in 2006 with the installation of the final module in the ATLAS cavern. During this period approximately 2600 metric tons of steel were transformed into a laminated structure to form the absorber of the sampling calorimeter. Following instrumentation and testing, which is described elsewhere, the modules were installed in the ATLAS cavern with a remarkable accuracy for a structure of this size and weight."", ""The Tile Calorimeter, covering the central region of the ATLAS experiment up to pseudorapidities of ±1.7, is a sampling device built with scintillating tiles that alternate with iron plates. The light is collected in wave-length shifting (WLS) fibers and is read out with photomultipliers. In the characteristic geometry of this calorimeter the tiles lie in planes perpendicular to the beams, resulting in a very simple and modular mechanical and optical layout. This paper focuses on the procedures applied in the optical instrumentation of the calorimeter, which involved the assembly of about 460,000 scintillator tiles and 550,000 WLS fibers. The outcome is a hadronic calorimeter that meets the ATLAS performance requirements, as shown in this paper.""]"
"T. Krojer","","","Computer Science","https://openalex.org/A5016800053","[""Coronavirus outbreaks have occurred over the past 25 years with SARS-CoV-2 (severe acute respiratory syndrome coronavirus-2) causing a global pandemic. The SARS-CoV-2 non-structural proteins 10 (nsp10) and 14 (nsp14) are considered as potential drug targets. Nsp10 stimulates the 3'-to-5' exoribonuclease (ExoN) activity of nsp14. The ExoN domain excises mis-incorporated nucleotides from the nascent RNA chain and therefore causes resistance to nucleoside analogue drugs. We crystallized the nsp10-nsp14 ExoN complex in distinct space groups, allowing us to describe conformational changes. In particular, the general base, His268, classifying the ExoN domain as a member of the DEDDh family, is trapped in the inactive and active orientations. By X-ray fragment screening, we identified five novel fragment binding sites in the nsp10-nsp14 interface, the hinge region connecting ExoN and N7-methyltransferase domains, and on nsp10. One new site in the nsp10-nsp14 interface accommodates nine structurally and chemically related hits, providing an initial structure-activity relationship study. We could also identify enantiomers of one fragment selectively bound to two different binding sites. The binding affinities of fragment hits were estimated using microscale thermophoresis and the new sites were investigated for their potential to inhibit protein-protein interactions between nsp10 and nsp14. Our fragments represent novel starting points for hit development by structure-based design."", ""The first multi-bend achromat based synchrotron MAX IV operates two protein crystallography beamlines, BioMAX and MicroMAX. BioMAX is designed as a versatile, stable, high-throughput beamline catering for most protein crystallography experiments. MicroMAX is a more ambitious beamline dedicated to serial crystallography including time-resolved experiments. Both beamlines exploit the special characteristics of fourth-generation beamlines provided by the 3 GeV ring of MAX IV. In addition, the fragment-based drug discovery platform, FragMAX, is hosted and, at the FemtoMAX beamline, protein diffraction experiments exploring ultrafast time resolution can be performed. A technical and operational overview of the different beamlines and the platform is given as well as an outlook for protein crystallography embedded in the wider possibilities that MAX IV offers to users in the life sciences."", ""Fragment approaches are long‐established in target‐based ligand discovery yet their full transformative potential lies dormant, because progressing hits to potency remains underserved by methodological work. The only credible progression paradigm is multiple cycles of costly conventional design‐make‐test‐analyse (DMTA) medicinal chemistry, necessitating picking winners early and discarding others. It is effective to cheaply parallelize large numbers of non‐uniform multi‐step reactions, because, even without compound purification, a high‐quality readout of binding is available, viz. crystallography. This can detect low‐level binding of slightly active compounds, which the targeted binding site extracts directly from crude reaction mixtures (CRMs). In this proof‐of‐concept study, we expand a fragment hit from a crystal‐based screen of the bromodomain PHIP2, using array synthesis on low‐cost robotics to implement 6 independent multi‐step reaction routes of up to 5 steps, attempting the synthesis of 1876 diverse expansions, designs entirely driven by synthetic tractability. The expected product was present in 1108 (59%) CRMs, detected by automated mass spectrometry, 22 individual products were resolved in crystal structures of CRMs added to crystals, providing an initial SAR map, pose stability in 19 and instability in 3 products and resolved stereochemical preference. One compound showed biochemical potency (IC50=34 μM) and affinity (Kd=50 μM) after resynthesis."", ""Fragment approaches are long‐established in target‐based ligand discovery yet their full transformative potential lies dormant, because progressing hits to potency remains underserved by methodological work. The only credible progression paradigm is multiple cycles of costly conventional design‐make‐test‐analyse (DMTA) medicinal chemistry, necessitating picking winners early and discarding others. It is effective to cheaply parallelize large numbers of non‐uniform multi‐step reactions, because, even without compound purification, a high‐quality readout of binding is available, viz. crystallography. This can detect low‐level binding of slightly active compounds, which the targeted binding site extracts directly from crude reaction mixtures (CRMs). In this proof‐of‐concept study, we expand a fragment hit from a crystal‐based screen of the bromodomain PHIP2, using array synthesis on low‐cost robotics to implement 6 independent multi‐step reaction routes of up to 5 steps, attempting the synthesis of 1876 diverse expansions, designs entirely driven by synthetic tractability. The expected product was present in 1108 (59%) CRMs, detected by automated mass spectrometry, 22 individual products were resolved in crystal structures of CRMs added to crystals, providing an initial SAR map, pose stability in 19 and instability in 3 products and resolved stereochemical preference. One compound showed biochemical potency (IC50=34 μM) and affinity (Kd=50 μM) after resynthesis."", ""ABSTRACT The FragMAX facility at MAX IV Laboratory is a state‐of‐the‐art platform for crystallographic fragment screening, designed to support structure‐based drug and chemical tool compound discovery. This facility offers a comprehensive workflow, from high‐throughput crystal preparation and automated diffraction data collection at the BioMAX beamline to advanced data processing and analysis using custom software tools like FragMAXapp and FragMAXproc. Key components include an extensive relational SQLite database, various fragment libraries, laboratory automation equipment, and a range of bespoke software solutions. FragMAX has conducted numerous successful screening campaigns, serving both academic and industrial users. Users benefit from comprehensive support, and stringent data management. Here, we provide an overview of the different components of the facility and details of their practical implementation.""]"
"Henrik Larsson","","","Computer Science","https://openalex.org/A5021664662","[""Background Subclinical hypomanic symptoms are fairly common in the general population but are linked to psychiatric and neurodevelopmental conditions. However, the genetic and environmental origins of these associations are unclear. This twin study examined the phenotypic and aetiological associations between subclinical hypomania and psychiatric and neurodevelopmental diagnoses. Methods Participants were 4,932 twin pairs from the Child and Adolescent Twin Study in Sweden. Hypomanic symptoms were assessed using the parent‐rated Mood Disorders Questionnaire when the twins were aged 18. Specialist diagnoses of 14 conditions and symptoms were ascertained from Swedish population registries. Phenotypic associations between hypomania and these conditions/symptoms were investigated, and their aetiological overlap was examined using the twin method. Results Subclinical hypomania was significantly associated with all 14 diagnoses. The highest odds were for psychotic disorders (odds ratio [OR] = 1.48, 95% confidence intervals [CI] = 1.33–1.64, p &lt; .001). The genetic correlations between subclinical hypomania and these diagnoses ranged from 0.12 (95% CI: 0.04–0.33) for eating disorders (other than anorexia) to 0.58 (95% CI: 0.28–1.00) for drug misuse disorders. The nonshared environmental correlations were highest for psychotic disorders (0.52, 95% CI: −0.02 to 0.92) and lowest for body dissatisfaction (0.04, 95% CI: −0.01 to 0.08). For bipolar disorder, psychotic disorders, and attention deficit hyperactivity disorder, genetic, and nonshared environmental correlations with subclinical hypomania were of a similar magnitude. Conclusions The association between subclinical hypomania and the diagnosis of multiple psychiatric phenotypes highlights its important role in the developmental pathway to clinical disorders, its complex origins, and that it may represent a quantitative trait for various psychiatric phenotypes."", ""An abstract is not available for this content so a preview has been provided. As you have access to this content, a full PDF is available via the 'Save PDF' action button."", ""The mental health impact of increasing recreational screen use among youth has raised substantial concerns, yet questions about causality remain unresolved. Using data from ~22,000 Swedish twins followed from age 9 to 24, we examined associations between screen use and internalizing problems using multiple designs, including co-twin control comparisons, to strengthen causal inference. Associations between longer screen time and elevated internalizing symptoms during adolescence persisted in co-twin comparisons, supporting a potential causal link. Bidirectional associations were observed, with internalizing problems at younger ages also associated with later screen time increase. Adolescents who exceeded international screen time recommendations at ages 15 and 18 showed elevated internalizing symptoms at later ages, whereas those who reduced screen time to recommended levels did not. Heavy screen use at age 15, particularly during the weekdays, was associated with higher risk of clinical depression or anxiety. Findings support public health recommendations to limit recreational screen time."", ""Parental criminality is a risk factor for crime, but little is known about why some individuals exposed to this risk refrain from crime. We explored associations of resting heart rate (RHR), systolic blood pressure (SBP), cognitive ability (CA), and psychological functioning (PF) with criminal convictions among men with a convicted parent, accounting for unmeasured familial factors in sibling analyses. Data were obtained from Swedish registers, including all men born in Sweden between 1958 and 1992 with a convicted parent (N = 495,109), followed for up to 48 years. The potential protective factors were measured at mandatory conscription. Outcomes were conviction of any, violent, and non-violent crime. Survival analyses were used to test for associations, adjusting for measured covariates and unmeasured familial factors. Higher levels of RHR, SBP, CA, and PF were associated with reduced risk of criminality after adjusting for covariates. RHR associations were largely explained by familial factors. CA and PF associations were not due to sibling-shared confounders, in line with a causal interpretation. SBP results, indicating a protective effect against non-violent crime, warrant further investigation.""]"
"Paul Lichtenstein","","","Computer Science","https://openalex.org/A5046450925","[""Background Subclinical hypomanic symptoms are fairly common in the general population but are linked to psychiatric and neurodevelopmental conditions. However, the genetic and environmental origins of these associations are unclear. This twin study examined the phenotypic and aetiological associations between subclinical hypomania and psychiatric and neurodevelopmental diagnoses. Methods Participants were 4,932 twin pairs from the Child and Adolescent Twin Study in Sweden. Hypomanic symptoms were assessed using the parent‐rated Mood Disorders Questionnaire when the twins were aged 18. Specialist diagnoses of 14 conditions and symptoms were ascertained from Swedish population registries. Phenotypic associations between hypomania and these conditions/symptoms were investigated, and their aetiological overlap was examined using the twin method. Results Subclinical hypomania was significantly associated with all 14 diagnoses. The highest odds were for psychotic disorders (odds ratio [OR] = 1.48, 95% confidence intervals [CI] = 1.33–1.64, p &lt; .001). The genetic correlations between subclinical hypomania and these diagnoses ranged from 0.12 (95% CI: 0.04–0.33) for eating disorders (other than anorexia) to 0.58 (95% CI: 0.28–1.00) for drug misuse disorders. The nonshared environmental correlations were highest for psychotic disorders (0.52, 95% CI: −0.02 to 0.92) and lowest for body dissatisfaction (0.04, 95% CI: −0.01 to 0.08). For bipolar disorder, psychotic disorders, and attention deficit hyperactivity disorder, genetic, and nonshared environmental correlations with subclinical hypomania were of a similar magnitude. Conclusions The association between subclinical hypomania and the diagnosis of multiple psychiatric phenotypes highlights its important role in the developmental pathway to clinical disorders, its complex origins, and that it may represent a quantitative trait for various psychiatric phenotypes."", ""Abstract Nonsuicidal self-injury (NSSI) often temporally precedes suicide attempts (SA), and SA predicts suicide. The genetic and environmental aetiologies of the transition from NSSI to SA have not been studied. This study aims to investigate whether NSSI reported at age 18 influences the incidence of SA between ages 18 and 24, and to what extent these transitions from NSSI to SA are influenced by shared genetic and environmental factors. Twins born in Sweden were enrolled in this longitudinal population-based twin cohort study. Self-reports of NSSI and SA were collected at ages 18 and 24. The majority of individuals in the analytical sample (N = 3 934) were female (64.9%) and dizygotic twins (65.0%). We found that NSSI reported at age 18 was associated with an increased risk of SA between ages 18 and 24 (Odds Ratio 5.4, 95% CI 3.3–8.7), after adjusting for sex and childhood psychopathology. There was a strong genetic correlation between NSSI reported at age 18 and incidence of SA between ages 18 and 24 (rA=0.8, 95% CI 0.3–1.0). At age 18, the proportion of variance in NSSI explained by genetic factors was 53%, and the remaining variance was explained by non-shared environmental factors (47%). At age 24, genetic factors explained 30% of the variance in SA between ages 18 and 24, largely explained by shared genetic factors (66.6%) with NSSI reported at age 18. We found evidence that NSSI reported at age 18 had a strong genetic correlation with incidence of SA between ages 18 and 24."", ""The mental health impact of increasing recreational screen use among youth has raised substantial concerns, yet questions about causality remain unresolved. Using data from ~22,000 Swedish twins followed from age 9 to 24, we examined associations between screen use and internalizing problems using multiple designs, including co-twin control comparisons, to strengthen causal inference. Associations between longer screen time and elevated internalizing symptoms during adolescence persisted in co-twin comparisons, supporting a potential causal link. Bidirectional associations were observed, with internalizing problems at younger ages also associated with later screen time increase. Adolescents who exceeded international screen time recommendations at ages 15 and 18 showed elevated internalizing symptoms at later ages, whereas those who reduced screen time to recommended levels did not. Heavy screen use at age 15, particularly during the weekdays, was associated with higher risk of clinical depression or anxiety. Findings support public health recommendations to limit recreational screen time."", ""Parental criminality is a risk factor for crime, but little is known about why some individuals exposed to this risk refrain from crime. We explored associations of resting heart rate (RHR), systolic blood pressure (SBP), cognitive ability (CA), and psychological functioning (PF) with criminal convictions among men with a convicted parent, accounting for unmeasured familial factors in sibling analyses. Data were obtained from Swedish registers, including all men born in Sweden between 1958 and 1992 with a convicted parent (N = 495,109), followed for up to 48 years. The potential protective factors were measured at mandatory conscription. Outcomes were conviction of any, violent, and non-violent crime. Survival analyses were used to test for associations, adjusting for measured covariates and unmeasured familial factors. Higher levels of RHR, SBP, CA, and PF were associated with reduced risk of criminality after adjusting for covariates. RHR associations were largely explained by familial factors. CA and PF associations were not due to sibling-shared confounders, in line with a causal interpretation. SBP results, indicating a protective effect against non-violent crime, warrant further investigation.""]"
"G. A. Mullier","","","Computer Science","https://openalex.org/A5016424332","[""A bstract The Light Dark Matter eXperiment (LDMX) is an electron-beam fixed-target experiment designed to achieve comprehensive model independent sensitivity to dark matter particles in the sub-GeV mass region. An upgrade to the LCLS-II accelerator will increase the beam energy available to LDMX from 4 to 8 GeV. Using detailed GEANT4-based simulations, we investigate the effect of the increased beam energy on the capabilities to separate signal and background, and demonstrate that the veto methodology developed for 4 GeV successfully rejects photon-induced backgrounds for at least 2 × 10 14 electrons on target at 8 GeV."", ""Abstract The Inner Tracker silicon strip detector (ITk Strip) is a part of the ATLAS upgrade for the HL-LHC. The detector readout and control is accomplished by the interaction of three on-module custom ASICs (ABCStarv1, HCCStarv1 and AMACstar). All ASICs are designed with protections against Single Event Errors. Their resilience at the system-level can be tested using the Board for Evaluation of Triple-chip Single Event Effects (BETSEE). This special board places all three ASICs into the beam-spot of a test beam facility concurrently and allows for module-like operation. The results from irradiating BETSEE with heavy ions and protons will be presented."", ""The constituents of dark matter are still unknown, and the viable possibilities span a vast range of masses. The physics community has established searching for sub-GeV dark matter as a high priority and identified accelerator-based experiments as an essential facet of this search strategy. A key goal of the accelerator-based dark matter program is testing the broad idea of thermally produced sub-GeV dark matter through experiments designed to directly produce dark matter particles. The most sensitive way to search for the production of light dark matter is to use a primary electron beam to produce it in fixed-target collisions. The Light Dark Matter eXperiment (LDMX) is an electron-beam fixed-target missing-momentum experiment that realizes this approach and provides unique sensitivity to light dark matter in the sub-GeV range. This contribution provides an overview of the theoretical motivation, the main experimental challenges, how LDMX addresses these challenges, and projected sensitivities. We further describe the capabilities of LDMX to explore other interesting new and standard physics, such as visibly-decaying axion and vector mediators or rare meson decays, and to provide timely electronuclear scattering measurements that will inform the modeling of neutrino-nucleus scattering for DUNE."", ""Particle physics experiments rely extensively on computing and data services, making e-infrastructure an integral part of the research collaboration. Constructing and operating distributed computing can however be challenging for a smaller-scale collaboration. The Light Dark Matter eXperiment (LDMX) is a planned small-scale accelerator-based experiment to search for dark matter in the sub-GeV mass region. Finalizing the design of the detector relies on Monte-Carlo simulation of expected physics processes. A distributed computing pilot project was proposed to better utilize available resources at the collaborating institutes, and to improve scalability and reproducibility. This paper outlines the chosen lightweight distributed solution, presenting requirements, the component integration steps, and the experiences using a pilot system for tests with large-scale simulations. The system leverages existing technologies wherever possible, minimizing the need for software development, and deploys only non-intrusive components at the participating sites. The pilot proved that integrating existing components can dramatically reduce the effort needed to build and operate a distributed e-infrastructure, making it attainable even for smaller research collaborations."", ""Particle physics experiments rely extensively on computing and data services, making e-infrastructure an integral part of the research collaboration. Constructing and operating distributed computing can however be challenging for a smaller-scale collaboration. The Light Dark Matter eXperiment (LDMX) is a planned small-scale accelerator-based experiment to search for dark matter in the sub-GeV mass region. Finalizing the design of the detector relies on Monte-Carlo simulation of expected physics processes. A distributed computing pilot project was proposed to better utilize available resources at the collaborating institutes, and to improve scalability and reproducibility. This paper outlines the chosen lightweight distributed solution, presenting requirements, the component integration steps, and the experiences using a pilot system for tests with large-scale simulations. The system leverages existing technologies wherever possible, minimizing the need for software development, and deploys only non-intrusive components at the participating sites. The pilot proved that integrating existing components can dramatically reduce the effort needed to build and operate a distributed e-infrastructure, making it attainable even for smaller research collaborations.""]"
"Bengt Sundén","","","Computer Science","https://openalex.org/A5066856244","[]"
"Mathias Uhlén","","","Computer Science","https://openalex.org/A5060241561","[""Abstract Generating longitudinal and multi-layered big biological data is crucial for effectively implementing artificial intelligence (AI) and systems biology approaches in characterising whole-body biological functions in health and complex disease states. Big biological data consists of multi-omics, clinical, wearable device, and imaging data, and information on diet, drugs, toxins, and other environmental factors. Given the significant advancements in omics technologies, human metabologenomics, and computational capabilities, several multi-omics studies are underway. Here, we first review the recent application of AI and systems biology in integrating and interpreting multi-omics data, highlighting their contributions to the creation of digital twins and the discovery of novel biomarkers and drug targets. Next, we review the multi-omics datasets generated worldwide to reveal interactions across multiple biological layers of information over time, which enhance precision health and medicine. Finally, we address the need to incorporate big biological data into clinical practice, supporting the development of a clinical decision support system essential for AI-driven hospitals and creating the foundation for an AI and systems biology-based healthcare model.""]"
"Per E. Andrén","","","Computer Science","https://openalex.org/A5068745437","[""<ns3:p>We summarize research reports from 2024 relevant to Tourette syndrome, which the authors consider the most important or interesting. This working draft aims to submit this content for publication around the beginning of 2025 in the yearly Tourette Syndrome Research Highlights series on F1000Research. The authors welcome article suggestions and thoughtful feedback from readers, who can add a comment by clicking on the rectangular comment box icon to the left of the LOG IN link at the top of this page. For private comments, you can reach us by email (andreas.hartmann@aphp.fr or kevin@wustl.edu).</ns3:p>"", ""L-DOPA-induced dyskinesia (LID) is a significant and treatment-limiting complication in Parkinson's disease (PD) therapy, yet its mechanisms remain poorly understood. We used high-resolution mass spectrometry imaging to map brain-region-specific alterations of glycerophospholipids and sphingolipids in a female macaque model of PD with and without LID following chronic L-DOPA treatment. LID was associated with depletion of antioxidant plasmalogen phosphatidylcholines in the globus pallidus interna, claustrum, and precentral gyrus-regions critical for motor function-and elevations of polyunsaturated fatty acid-containing glycerophospholipids, indicative of increased membrane fluidity. This lipid profile differed from similarly treated non-dyskinetic animals, suggesting lipid composition mediates differential susceptibility to LID. Lipid alterations correlated strongly with dyskinesia severity, dopamine, and L-DOPA concentrations, supporting a mechanistic link between lipid metabolism, neurotransmitter dysregulation, and LID. This comprehensive spatial lipidomic analysis identifies region-specific lipid dysregulation as a novel aspect of LID pathology, highlighting lipid pathways as potential therapeutic targets for mitigating dyskinesia."", ""<ns3:p>We summarize research reports from 2024 relevant to Tourette syndrome, which the authors consider the most important or interesting. This working draft aims to submit this content for publication around the beginning of 2025 in the yearly Tourette Syndrome Research Highlights series on F1000Research. The authors welcome article suggestions and thoughtful feedback from readers, who can add a comment by clicking on the rectangular comment box icon to the left of the LOG IN link at the top of this page. For private comments, you can reach us by email (andreas.hartmann@aphp.fr or kevin@wustl.edu).</ns3:p>"", ""ABSTRACT One of the main challenges in analyzing chemical messengers in the brain is the optimization of tissue sampling and preparation protocols. Limiting postmortem time and terminating enzyme activity is critical to identify low‐abundance neurotransmitters and neuropeptides. Here, we used a rapid and uniform conductive heat transfer stabilization method that was compared with a conventional fresh freezing protocol. Together with a selective chemical derivatization method and an optimized quantitation approach using deuterated internal standards, we spatially mapped neurotransmitters and their related metabolites by matrix‐assisted laser desorption/ionization mass spectrometry imaging (MALDI‐MSI) in rat brain tissue sections. Although the heat stabilization did not show differences in the levels of dopamine, norepinephrine, and serotonin, their related metabolites 3,4‐dihydroxyphenylacetaldehyde, 3,4‐dihydroxyphenylacetic acid, homovanillic acid, 3‐methoxy‐4‐hydroxyphenylacetaldehyde, dihydroxyphenylethyleneglycol, and 5‐hydroxyindoleacetic acid were all significantly lower, indicating reduced neurotransmitter postmortem turnover ratios. Heat stabilization enabled detection of an increased number and higher levels of prodynorphin, proenkephalin, and tachykinin‐derived bioactive neuropeptides. The low‐abundant C‐terminal flanking peptide, neuropeptide‐γ, and nociceptin remained intact and were exclusively imaged in heat‐stabilized brains. Without heat stabilization, degradation fragments of full‐length peptides occurred in the fresh frozen tissues. The sample preparation protocols were furthermore tested on rat brains affected by acute anesthesia induced by isoflurane and medetomidine, showing comparable results to non‐anesthetized animals on the neurotransmitters level without significant changes. Our data provide evidence for the potential use of heat stabilization prior to MALDI‐MSI analyses to improve the examination of the in vivo state of neuronal chemical messengers in brain tissues not impacted by prior acute anesthesia. image"", ""Inhibitors targeting amyloids formed by the human Islet Amyloid Polypeptide (hIAPP) are promising therapeutic candidates for type 2 diabetes. Peptide formulations derived from the nonamyloidogenic rat IAPP (rIAPP) sequence are currently used as hIAPP mimetics to support insulin therapy. rIAPP itself acts as a peptide inhibitor; yet, the structural-level consequences of such inhibition, particularly its impact on amyloid polymorphism, have not been studied in detail. Here, we conduct coaggregation experiments with varying rIAPP-to-hIAPP concentration ratios and employ high-resolution cryo-electron microscopy (Cryo-EM) to elucidate the polymorphism of the resulting fibril structures. Our results demonstrate that the polymorphism of hIAPP amyloids is highly sensitive to the electrostatic environment, which can be modulated by buffer composition, the concentration of the inhibitor, and cosolvents such as hexafluoroisopropanol (HFIP). Under native conditions, rIAPP associates with hIAPP but does not cross-aggregate, resulting in fibrils primarily composed of hIAPP. Significant inhibition is observed at relatively high concentrations of rIAPP. However, trace amounts of HFIP disrupt this inhibition, leading to increased fibril concentrations due to the formation of cross-seeded products composed of both hIAPP and rIAPP, as evidenced by mass spectrometry and two-dimensional infrared (2D IR) spectroscopy. These findings highlight the critical role of experimental conditions, particularly the electrostatic environment, in modulating amyloid polymorphism, cross-seeding, and inhibition. By providing structural insights into these processes, this study advances our understanding of peptide aggregation and offers valuable guidance for the rational design of more effective therapeutic inhibitors targeting hIAPP-related amyloidosis.""]"
"E. Coniavitis","","","Computer Science","https://openalex.org/A5041774573","[""Interpatient variation of tumor radiosensitivity is rarely considered during the treatment planning process despite its known significance for the therapeutic outcome."", ""A review of the results on Higgs boson decays to leptons with the ATLAS detector at the Large Hadron Collider is presented. In the H→τ+τ− search, using the 8 TeV dataset, there is an excess of data over the background prediction, with an observed (expected) significance corresponding to 4.1σ (3.2σ). In the H→μ+μ− search, using approximately 25 fb−1 of pp collision data collected at 7 TeV and 8 TeV in 2011 and 2012, the data is consistent with the expected background and a 95% confidence level limit of 7.0 times the Standard Model prediction is placed on the signal strength, for a Higgs boson mass of 125.5 GeV."", ""A dedicated reconstruction algorithm to find decay vertices in the ATLAS muon spectrometer is presented. The algorithm searches the region just upstream of or inside the muon spectrometer volume for multi-particle vertices that originate from the decay of particles with long decay paths. The performance of the algorithm is evaluated using both a sample of simulated Higgs boson events, in which the Higgs boson decays to long-lived neutral particles that in turn decay to bb final states, and pp collision data at sqrt(s) = 7 TeV collected with the ATLAS detector at the LHC during 2011."", ""A search is presented for direct chargino production based on a disappearing-track signature using 20.3 fb−1 of proton-proton collisions at s=8 TeV collected with the ATLAS experiment at the LHC. In anomaly-mediated supersymmetry breaking (AMSB) models, the lightest chargino is nearly mass degenerate with the lightest neutralino and its lifetime is long enough to be detected in the tracking detectors by identifying decays that result in tracks with no associated hits in the outer region of the tracking system. Some models with supersymmetry also predict charginos with a significant lifetime. This analysis attains sensitivity for charginos with a lifetime between 0.1 and 10 ns, and significantly surpasses the reach of the LEP experiments. No significant excess above the background expectation is observed for candidate tracks with large transverse momentum, and constraints on chargino properties are obtained. In the AMSB scenarios, a chargino mass below 270 GeV is excluded at 95% confidence level.Received 14 October 2013DOI:https://doi.org/10.1103/PhysRevD.88.112006This article is available under the terms of the Creative Commons Attribution 3.0 License. Further distribution of this work must maintain attribution to the author(s) and the published article’s title, journal citation, and DOI.© 2013 CERN, for the ATLAS Collaboration""]"
"D. Silvermyr","","","Computer Science","https://openalex.org/A5048740401","[""The sPHENIX Time Projection Chamber Outer Tracker (TPOT) is a Micromegas based detector. It is a part of the sPHENIX experiment that aims to facilitate the calibration of the Time Projection Chamber, in particular the correction of the time-averaged and beam-induced distortions of the electron drift. This paper describes the detector mission, setup, construction, installation, commissioning and performance during the first year of sPHENIX data taking."", ""Abstract A large-volume Time Projection Chamber (TPC) is the main tracking and particle identification (PID) detector of the ALICE experiment at the CERN LHC. PID in the TPC is performed via specific energy-loss measurements (d E /d x ), which are derived from the average pulse-height distribution of ionization generated by charged-particle tracks traversing the TPC volume. During Runs 1 and 2, until 2018, the gas amplification stage was based on multiwire proportional chambers (MWPC). Signals from the MWPC show characteristic long negative tails after an initial positive peak due to the long ion drift times in the MWPC amplification region. This so-called ion tail can lead to a significant amplitude loss in subsequently measured signals, especially in the high-multiplicity environment of high-energy Pb-Pb collisions, which results in a degradation of the d E /d x resolution. A detailed study of the signal shapes measured with the ALICE TPC with the Ne-CO 2 (90-10) and Ar-CO 2 (90-10) gas mixtures is presented, and the results are compared with three-dimensional Garfield simulations. The impact of the ion tail on the PID performance is studied employing the ALICE simulation framework and the feasibility of an offline correction procedure to account for the ion tail is demonstrated.""]"
"B. Åsman","","","Computer Science","https://openalex.org/A5107824863","[""The ATLAS detector as installed in its experimental cavern at point 1 at CERN is described in this paper. A brief overview of the expected performance of the detector when the Large Hadron Collider begins operation is also presented."", ""The hadronic part of the electron structure function F2e has been measured for the first time, using e+e− data collected by the DELPHI experiment at LEP, at centre-of-mass energies of s=91.2–209.5GeV. The data analysis is simpler than that of the measurement of the photon structure function. The electron structure function F2e data are compared to predictions of phenomenological models based on the photon structure function. It is shown that the contribution of large target photon virtualities is significant. The data presented can serve as a cross-check of the photon structure function F2γ analyses and help in refining existing parameterisations."", ""The PreProcessor system of the ATLAS Level-1 Calorimeter Trigger (L1Calo) receives about 7200 analogue signals from the electromagnetic and hadronic components of the calorimetric detector system. Lateral division results in cells which are pre-summed to so-called Trigger Towers of size 0.1 × 0.1 along azimuth (ϕ) and pseudorapidity (η). The received calorimeter signals represent deposits of transverse energy.""]"
"H. Herde","","","Computer Science","https://openalex.org/A5091583777","[""Abstract At the end of Run 3 of the Large Hadron Collider (LHC), the accelerator complex will be upgraded to the High-Luminosity LHC (HL-LHC) in order to increase the total amount of data provided to its experiments. To cope with the increased rates of data, radiation, and pileup, the ATLAS detector will undergo a substantial upgrade, including a replacement of the Inner Detector with a future Inner Tracker, called the ITk. The ITk will be composed of pixel and strip sub-detectors, where the strips portion will be composed of 17,888 silicon strip detector modules. During the HL-LHC running period, the ITk will be cooled and warmed a number of times from about -35°C to room temperature as part of the operational cycle, including warm-ups during yearly shutdowns. To ensure ITk Strips modules are functional after these expected temperature changes, and to ensure modules are mechanically robust, each module must undergo ten thermal cycles and pass a set of electrical and mechanical criteria before it is placed on a local support structure. This paper describes the thermal cycling Quality Control (QC) procedure, and results from the barrel pre-production phase (about 5% of the production volume). Additionally, in order to assess the headroom of the nominal QC procedure of 10 cycles and to ensure modules don't begin failing soon after, four representative ITk Strips barrel modules were thermally cycled 100 times — this study is also described."", ""At the end of Run 3 of the Large Hadron Collider (LHC), the accelerator complex will be upgraded to the High-Luminosity LHC (HL-LHC) in order to increase the total amount of data provided to its experiments. To cope with the increased rates of data, radiation, and pileup, the ATLAS detector will undergo a substantial upgrade, including a replacement of the Inner Detector with a future Inner Tracker, called the ITk. The ITk will be composed of pixel and strip sub-detectors, where the strips portion will be composed of 17,888 silicon strip detector modules. During the HL-LHC running period, the ITk will be cooled and warmed a number of times from about ${-35}^\\circ$C to room temperature as part of the operational cycle, including warm-ups during yearly shutdowns. To ensure ITk Strips modules are functional after these expected temperature changes, and to ensure modules are mechanically robust, each module must undergo ten thermal cycles and pass a set of electrical and mechanical criteria before it is placed on a local support structure. This paper describes the thermal cycling Quality Control (QC) procedure, and results from the barrel pre-production phase (about 5% of the production volume). Additionally, in order to assess the headroom of the nominal QC procedure of 10 cycles and to ensure modules don't begin failing soon after, four representative ITk Strips barrel modules were thermally cycled 100 times - this study is also described."", ""Abstract The inner detector of the present ATLAS experiment has been designed and developed to function in the environment of the present Large Hadron Collider (LHC). At the ATLAS Phase-II Upgrade, the particle densities and radiation levels will exceed current levels by a factor of ten. The instantaneous luminosity is expected to reach unprecedented values, resulting in up to 200 proton-proton interactions in a typical bunch crossing. The new detectors must be faster and they need to be more highly segmented. The sensors used also need to be far more resistant to radiation, and they require much greater power delivery to the front-end systems. At the same time, they cannot introduce excess material which could undermine tracking performance. For those reasons, the inner tracker of the ATLAS detector was redesigned and will be rebuilt completely. The ATLAS Upgrade Inner Tracker (ITk) consists of several layers of silicon particle detectors. The innermost layers will be composed of silicon pixel sensors, and the outer layers will consist of silicon microstrip sensors. This contribution focuses on the strip region of the ITk. The central part of the strip tracker (barrel) will be composed of rectangular short (∼2.5 cm) and long (∼5 cm) strip sensors. The forward regions of the strip tracker (end-caps) consist of six disks per side, with trapezoidal shaped sensors of various lengths and strip pitches. After the completion of final design reviews in key areas, such as Sensors, Modules, Front-End electronics, and ASICs, a large scale prototyping program has been completed in all areas successfully. We present an overview of the Strip System and highlight the final design choices of sensors, module designs and ASICs. We will summarise results achieved during prototyping and the current status of pre-production and production on various detector components, with an emphasis on QA and QC procedures."", ""A bstract The Light Dark Matter eXperiment (LDMX) is an electron-beam fixed-target experiment designed to achieve comprehensive model independent sensitivity to dark matter particles in the sub-GeV mass region. An upgrade to the LCLS-II accelerator will increase the beam energy available to LDMX from 4 to 8 GeV. Using detailed GEANT4-based simulations, we investigate the effect of the increased beam energy on the capabilities to separate signal and background, and demonstrate that the veto methodology developed for 4 GeV successfully rejects photon-induced backgrounds for at least 2 × 10 14 electrons on target at 8 GeV.""]"
"Tiny Jaarsma","","","Computer Science","https://openalex.org/A5046767891","[""Aims Discussions about severe illness and the coming death do not often take place with patients with heart failure and their family. We therefore aimed to investigate how patients with end-stage heart failure and their family who discussed terminal illness and the imminence of death with a physician, experienced such communication, how they handled life emotionally and practically after said discussions, and if/how this changed over time. Methods A longitudinal interview study. Ten patients with end-stage heart failure and their closest kin were visited by a physician at home and discussed terminal illness during one visit and the imminence of death during another visit. They were interviewed three times about how they experienced the communication and how they handled life in this situation and in relation to the discussions. The interviews were analysed using qualitative thematic analysis by Braun and Clarke. Findings Two main themes and five subthemes were found. The first theme was ‘an honest and clear message hurts, but it would be worse if nothing was said’, and the subthemes included information on the experiences of communication, the desired level of communication by patients and family members and factors facilitating communication. The second theme was ‘A clear message helps in handling life’ with the subthemes of coping psychologically and practically. The findings indicate that for some patients and family members it was hard to have discussions about end-stage heart failure and the imminence of death. However, they found the discussions important and were happy that the information was not withheld from them. The discussions helped in handling life and most patients and family seemed to have found a way to accept and handle the situation. Practical planning often did not start until they heard from the physician that death could come soon. Conclusion This study confirms that patients and family members want and appreciate discussions about severe illness and the imminence of death and find them important. This can encourage physicians to change behaviour and engage in honest discussions, and to educate and train colleagues to do the same."", ""Abstract Background Exergaming is a promising intervention to decrease sedentary time in people with a chronic condition, such heart failure (HF). For implementing exergaming in healthcare, it is important to assess the strengths, weaknesses, opportunities and threats (SWOT). Purpose To describe the SWOT of implementing exergaming in healthcare experienced by patients’ representatives, healthcare professionals, game developers, people involved in healthcare regulations, and researchers. Methods The design was a qualitative interview study using deductive content analysis based on the SWOT framework. Purposeful and snowball sampling were used interchangeably to recruit study participants (patients’ representatives, healthcare professionals, game developers, people involved in healthcare regulations, and researchers). The interviews were conducted via the Zoom platform and their duration varied between 20 minutes and one hour. Results In total 24 people were interviewed (5 patients-representatives, 4 healthcare professionals, 4 game developers, 5 people involved in healthcare regulations, and 6 researchers). Participants were from Sweden, Slovakia, Israel, Iceland, Belgium, the Netherlands, the United States and Australia. Strengths mentioned in implementing exergames in healthcare, were the importance of using simple language when explaining exergames to patients and to offer additional assistance when needed. Another strength experienced was that exergames could be tailored to patients’ interests, motives, and daily activities. Weaknesses emphasised by healthcare professionals and patients were the resistance to change and use technology in clinical practice. Poor health literacy of patients and poor digital literacy of healthcare professionals were also considered weaknesses. Healthcare professionals also expressed their doubts considering the safety and the quality of exergames. An opportunity mentioned was the possibility to involve family members during the introduction of an exergame. Participants stated that providing clear information on the health benefits of exergaming to patients could help with implementation. Another opportunity was the variety of existing exergames and the possibility to be able to exergame at home. A threat highlighted was the absence of exergames tailored to patients’ needs. Participants also stated that exergame is not a suitable form of intervention for people with multiple comorbidities experiencing severe symptoms. Healthcare professionals expressed the fear of losing control over exergaming when implementing it in healthcare. Conclusion The findings indicate that implementing exergaming in healthcare needs adequate time for familiarization, family engagement and additional support for healthcare professionals and patients. Despite the presence of a large pool of exergames on the market, there is a need of developing and implementing tailored exergames, especially for persons with multiple comorbidities.Table of Results"", ""Abstract Background Digital health interventions, including mobile applications, offer new opportunities for promoting physical activity among individuals with chronic conditions. However, adherence to such applications varies widely. Previous findings from a randomized controlled trial (RCT) evaluating a tele-yoga intervention indicated high participation in live-streamed group yoga sessions but low engagement with the accompanying app. The aim was to explore user experiences with a yoga app and to determine associations between adherence levels, demographic factors, and patient-reported outcomes. Methods This mixed-methods study included participants from a RCT evaluating tele-yoga (clinicaltrials.gov: NCT03703609). The intervention included live-streamed 60-minute group sessions twice weekly and individual 10-minute sessions five days per week via an app. Quantitative data on app usage during the 3-month intervention were logged, and demographic and baseline patient-reported outcomes were analysed using linear regression. Qualitative data on user experiences were collected through interviews at three months and analysed using content analysis. Results Of 156 participants in the tele-yoga group, 13 withdrew early, leaving 143 for quantitative analysis (46 women, mean age 65, range 34-84 years). Interviews were conducted with 125 participants. Two-thirds did not meet the recommended app usage of ≥40 minutes/week, while 10% were high users, exceeding the recommendation by at least twofold. The highest recorded usage averaged 27 minutes/day. Women were significantly more adherent than men, with 48% of women using the app for ≥480 minutes, compared to 25% of men. Participants engaged in group yoga were more likely to use the app. Qualitative analysis categorized participants into adherent (480-600 min, n=40), somewhat adherent (120-480 min, n=31), and non-adherent (0-120 min, n=54). Adherent users valued the app’s technology, background music, and flexibility, having established a routine and experienced positive effects. Somewhat adherent users struggled with consistency due to the absence of fixed schedules and group support, often relying on reminders from relatives. Usage declined over time, particularly during holidays or hospital stays. Non-adherent users preferred group yoga, often practicing independently without the app. Some in this group did not engage in yoga at all. Conclusion App usage was highest when integrated into daily routines and perceived as enjoyable. Women and those actively participating in group yoga exhibited greater adherence. Future research should focus on developing a co-designed, personalized yoga app incorporating user-driven features to enhance engagement and sustainability."", ""Abstract Background People with heart failure (HF) may experience symptoms such as shortness of breath or fatigue leading to avoiding physically demanding activities and becoming sedentary. Engaging in everyday life activities significantly enhances the quality of life and promote physical and emotional well-being. Yet little is known about the performance of everyday life activities in patients with HF. Purpose The aim is to identify important everyday life activities for patients with HF and assess ability to perform the activity and satisfaction with the performance Method This is a cross-sectional study on the baseline data in the \""Heart eXg-study,\"" an ongoing randomized controlled trial. Performance of everyday life activities was assessed with the Canadian Occupational Performance Measure (COPM). In COPM activities perceived important to perform are identified and the ability to perform and satisfaction with performance are self-assessed. The ratings are made on a ten-point scale, from one \""=not able to do/not satisfied at all\"" to ten \""= able to do it extremely well/extremely satisfied\"". To describe the patient group, demographic data was extracted from the medical file (age, gender, NYHA-class), mobility was measured with the Times Up and Go (TUG) test, and frailty was assessed with the Clinical Frailty scale. Results In total, 24 patients were included, 67% male (n=16), mean age 78±9, 56% NYHA II, 44% NYHA III/IV. In total 19% of the patients were considered fit or managing well, 46% were living with very mild to mild frailty and 4% were living with moderate frailty. The mean distance on the 6-minute walk test was 338±100 meters and 32% (n=8) of the patients had impaired mobility (&amp;gt;12 seconds in TUG). Walking was a key activity that participants wished to improve. They expressed a desire to walk their dogs, walk longer without experiencing shortness of breath. Social activities for example playing with grandchildren, maintaining contact with friends, dancing and participating in volunteer activities were perceived important. Household chores, such as preparing food, cleaning kitchen cabinets, vacuuming, cleaning the car, and gardening, were also significant. Most patients felt not able to perform these activities (66% scored &amp;lt;6), and 88% were dissatisfied with their performance of everyday life activities (scored &amp;lt;6). Conclusion Patients with HF in this study have challenges in performing activities such as walking, household chores, and social interactions, leading to a high level of dissatisfaction with their performance. The results highlight the need for multidisciplinary interventions to enhance the functional abilities and satisfaction of HF patients in their daily lives. By focusing on the specific activities that patients find important and challenging, healthcare providers can develop more personalized and effective rehabilitation programs. The role of an occupational therapist in the HF team should be further explored""]"
"P. Adlarson","","","Computer Science","https://openalex.org/A5077139969","[""A bstract A study on the Bose-Einstein correlations for triplets of same-sign pions is presented. The analysis is performed using proton-proton collisions at a centre-of-mass energy of $$ \\sqrt{s} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> </mml:math> = 7 TeV, recorded by the LHCb experiment, corresponding to an integrated luminosity of 1.0 fb − 1 . For the first time, the results are interpreted in the core-halo model. The parameters of the model are determined in regions of charged-particle multiplicity. This measurement provides insight into the nature of hadronisation in terms of coherence, being consistent with the presence of coherent emission of pions."", ""Abstract The first observation of the $${{{{\\varXi } ^0_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {{\\varXi } ^-} {{\\pi } ^+} }}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> <mml:msup> <mml:mrow> <mml:mi>π</mml:mi> </mml:mrow> <mml:mo>+</mml:mo> </mml:msup> </mml:mrow> </mml:math> decay and the most precise measurement of the branching fraction of the $${{{\\varLambda } ^0_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {{\\varXi } ^-} {{K} ^+} }$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> <mml:msup> <mml:mrow> <mml:mi>K</mml:mi> </mml:mrow> <mml:mo>+</mml:mo> </mml:msup> </mml:mrow> </mml:math> decay are reported, using proton-proton collision data from the LHCb experiment collected in 2016–2018 at a centre-of-mass energy of 13 $$\\text {\\,Te\\hspace{-1.00006pt}V}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mtext>\\,Te</mml:mtext> <mml:mspace/> <mml:mtext>V</mml:mtext> </mml:mrow> </mml:math> , corresponding to an integrated luminosity of 5.4 $$\\text {\\,fb} ^{-1}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msup> <mml:mtext>\\,fb</mml:mtext> <mml:mrow> <mml:mo>-</mml:mo> <mml:mn>1</mml:mn> </mml:mrow> </mml:msup> </mml:math> . Using the $${{{{\\varLambda } ^0_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {\\varLambda } }}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> </mml:math> and $${{{{\\varXi } ^-_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {{\\varXi } ^-} }}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mo>-</mml:mo> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> </mml:mrow> </mml:math> decays as normalisation channels, the ratios of branching fractions are measured to be $$\\begin{aligned} {\\frac{{{\\mathcal {B}}}({{{\\varLambda } ^0_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {{\\varXi } ^-} {{K} ^+} })}{{{\\mathcal {B}}}({{{{\\varLambda } ^0_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {\\varLambda } }})}}&amp;= (1.17 \\pm 0.14 \\pm 0.08)\\times 10^{-2} \\, ,\\\\ {\\frac{{{\\mathcal {B}}}({{{{\\varXi } ^0_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {{\\varXi } ^-} {{\\pi } ^+} }})}{{{\\mathcal {B}}}({{{{\\varXi } ^-_{b}} \\!\\rightarrow {{J \\hspace{-1.66656pt}/\\hspace{-1.111pt}\\psi }} {{\\varXi } ^-} }})}}&amp;= (11.9 \\pm 1.4 \\pm 0.6)\\times 10^{-2}\\, , \\end{aligned}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mtable> <mml:mtr> <mml:mtd> <mml:mfrac> <mml:mrow> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> <mml:msup> <mml:mrow> <mml:mi>K</mml:mi> </mml:mrow> <mml:mo>+</mml:mo> </mml:msup> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> <mml:mrow> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> </mml:mfrac> </mml:mtd> <mml:mtd> <mml:mrow> <mml:mo>=</mml:mo> <mml:mrow> <mml:mo>(</mml:mo> <mml:mn>1.17</mml:mn> <mml:mo>±</mml:mo> <mml:mn>0.14</mml:mn> <mml:mo>±</mml:mo> <mml:mn>0.08</mml:mn> <mml:mo>)</mml:mo> </mml:mrow> <mml:mo>×</mml:mo> <mml:msup> <mml:mn>10</mml:mn> <mml:mrow> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> </mml:mrow> </mml:msup> <mml:mspace/> <mml:mo>,</mml:mo> </mml:mrow> </mml:mtd> </mml:mtr> <mml:mtr> <mml:mtd> <mml:mrow> <mml:mrow/> <mml:mfrac> <mml:mrow> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> <mml:msup> <mml:mrow> <mml:mi>π</mml:mi> </mml:mrow> <mml:mo>+</mml:mo> </mml:msup> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> <mml:mrow> <mml:mi>B</mml:mi> <mml:mo>(</mml:mo> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mo>-</mml:mo> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> </mml:mrow> <mml:mo>)</mml:mo> </mml:mrow> </mml:mfrac> </mml:mrow> </mml:mtd> <mml:mtd> <mml:mrow> <mml:mo>=</mml:mo> <mml:mrow> <mml:mo>(</mml:mo> <mml:mn>11.9</mml:mn> <mml:mo>±</mml:mo> <mml:mn>1.4</mml:mn> <mml:mo>±</mml:mo> <mml:mn>0.6</mml:mn> <mml:mo>)</mml:mo> </mml:mrow> <mml:mo>×</mml:mo> <mml:msup> <mml:mn>10</mml:mn> <mml:mrow> <mml:mo>-</mml:mo> <mml:mn>2</mml:mn> </mml:mrow> </mml:msup> <mml:mspace/> <mml:mo>,</mml:mo> </mml:mrow> </mml:mtd> </mml:mtr> </mml:mtable> </mml:mrow> </mml:math> where the first uncertainty is statistical and the second systematic."", ""A bstract The ratio of prompt production cross-sections of ψ (2 S ) and J/ψ mesons in their dimuon final state is measured as a function of centrality, using data collected by the LHCb detector in PbPb collisions at $$ \\sqrt{s_{\\textrm{NN}}} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msqrt> <mml:msub> <mml:mi>s</mml:mi> <mml:mi>NN</mml:mi> </mml:msub> </mml:msqrt> </mml:math> = 5 . 02 TeV, for the first time in the forward rapidity region. The measured ratio shows no dependence on the collision centrality, and is compared to the latest theory predictions and to the recent measurements in literature."", ""A bstract A test of lepton universality between muons and electrons is performed using B + → K + ℓ + ℓ − decays (where ℓ = e , μ ), in the dilepton invariant-mass-squared region above 14.3 GeV 2 /c 4 . The data used for the measurement consists of beauty meson decays produced in proton-proton collisions, corresponding to an integrated luminosity of 9 fb − 1 , collected by the LHCb experiment between 2011 and 2018. The ratio of branching fractions for B + → K + μ + μ − and B + → K + e + e − decays is measured to be $$ {R}_K=1.0{8}_{-0.09}^{+0.11}{\\left(\\textrm{stat}\\right)}_{-0.04}^{+0.04}\\left(\\textrm{syst}\\right) $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msub> <mml:mi>R</mml:mi> <mml:mi>K</mml:mi> </mml:msub> <mml:mo>=</mml:mo> <mml:mn>1.0</mml:mn> <mml:msubsup> <mml:mn>8</mml:mn> <mml:mrow> <mml:mo>−</mml:mo> <mml:mn>0.09</mml:mn> </mml:mrow> <mml:mrow> <mml:mo>+</mml:mo> <mml:mn>0.11</mml:mn> </mml:mrow> </mml:msubsup> <mml:msubsup> <mml:mfenced> <mml:mtext>stat</mml:mtext> </mml:mfenced> <mml:mrow> <mml:mo>−</mml:mo> <mml:mn>0.04</mml:mn> </mml:mrow> <mml:mrow> <mml:mo>+</mml:mo> <mml:mn>0.04</mml:mn> </mml:mrow> </mml:msubsup> <mml:mfenced> <mml:mtext>syst</mml:mtext> </mml:mfenced> </mml:math> , which is consistent with the Standard Model prediction of unity. This constitutes the most precise test of lepton flavour universality using B + → K + ℓ + ℓ − decays with dilepton invariant-mass-squared above the ψ (2 S ) mass, whilst being the first of its kind at a hadron collider.""]"
"M. Willander","","","Computer Science","https://openalex.org/A5074258399","[""Expression of concern for ‘Electrochemical genosensor based on gold nanostars for the detection of Escherichia coli O157:H7 DNA’ by Nasrin Razmi et al. , Anal. Methods , 2022, 14 , 1562–1570, https://doi.org/10.1039/D2AY00056C."", ""While world energy consumption is rising every year, the development of clean and renewable energy sources becomes very important for keeping the standard of living and preserving the environment. Solar driven photoelectrochemical (PEC) water splitting to produce hydrogen and oxygen is a promising method to contribute to the energy increasing demand. The development of the photoelectrode is a key factor for improving the PEC performance. A new morphology of 3D CdS-branched ZnO nanorod array nanocomposite has successfully been synthesized via solution routes as a photoanode. The present nanocomposite provides the highest photocurrent density of 2.5 mA/cm2 at a potential 1.23 V vs. RHE, which is about 83.3 times compared to a photocurrent density of 0.03 mA/cm2 of the bare ZnO nanorod array photoelectrode. The boost of the PEC performance is improved due to the improvement of light absorption capacity, the enhanced energy band alignment (type-II heterostructure) promoting the charge transfer and separation, and the improvement of the electrode-electrolyte interface kinetic reactions. The result will be useful for further research on energy conversion and energy storage devices."", ""Abstract Low temperature hydrothermal methods have been utilized to synthesize Hematite/Zinc oxide α ‐Fe 2 O 3 /ZnO composite nano‐heterojunction nanorods grown on FTO glass substrates while monitoring the effect of different concentrations of urea on the morphology of the composite nano‐heterojunction. X‐ray diffraction (XRD) and scanning electron microscopy (SEM) techniques were used for the structural characterization of the α ‐Fe 2 O 3 /ZnO different samples. UV‐visible spectroscopy was used for the characteristic absorbance versus wavelength of α ‐Fe 2 O 3 /ZnO composite nano‐heterojunction which shows an absorption edge from 400 to 560 nm. X‐ray photoelectron spectroscopy (XPS) technique was applied to study of chemical composition of the α ‐Fe 2 O 3 /ZnO and the obtained information demonstrated a pure phase α‐Fe 2 O 3 /ZnO has been achieved. The best efficiency among urea concentrations for the best composite nano‐heterojunction sample was achieved when using 0.2 M of urea. The electrochemical properties of the composite nano‐heterojunction were investigated using a three‐electrode electrochemical cell. Estimation of the electrochemical area shows that both the composite nano‐heterojunction and the bare α‐Fe 2 O 3 have similar values. This confirms that the enhanced electrochemical property of the composite nano‐heterojunction is due to a synergetic effect as expected.""]"
"Michel Foucault","","","Computer Science","https://openalex.org/A5005989677","[""This book makes available, for the first time in English, lectures and interviews that Foucault gave in Japan in 1978, reconstructing their context, and isolating the question of their singular relevance for us today. In these forgotten lectures, in a free and often informal style, Foucault explores, together with his Japanese interlocutors, what it would mean to take up, from outside Europe, the questions he was raising at the time about Revolution and Enlightenment in the traditions of European critical thought. In a series of wide-ranging discussions, on sexuality and its history, non-Christian forms of spirituality, new forms of political movements, and the role of knowledge, power, and truth in them, Foucault examines these questions in relationship to Asia. He had hoped these questions, very much debated at the time in postwar Japan, would be the start of new forms of translation, publication, and exchange. At the heart of the lectures is thus a search for the creation of a new sort of transnational collaboration, recasting the history of European colonialism and opening to a philosophy no longer simply Western, yet to come. The Japan Lectures thus contribute to the new scholarship in Asian and in translation studies which has long since moved away from earlier \""Area Studies\""; at the same time, it participates in the new scholarship about Foucault's own work and itinerary, following the publication of an extraordinary wealth of materials left unfinished or unpublished by his untimely death. In these ways, The Japan Lectures help us to better see the implications of Foucault's work for philosophy in the 21st century.""]"
"J. Sollerman","","","Computer Science","https://openalex.org/A5028904496","[""Abstract We present observations of SN 2023xgo, a transitional Type Ibn/Icn SN, from −5.6 to 63 days relative to r-band peak. Early spectra show C iii λ5696 emission like Type Icn SNe, shifting to Type Ibn features. The He i velocities (1800-10000 km s−1) and pseudo-equivalent widths are among the highest in the Ibn/Icn class. The light curve declines at 0.14mag d−1 until 30 days, matching SNe Ibn/Icn but slower than fast transients. SN 2023xgo is the faintest in our SN Ibn sample (Mr = −17.65 ± 0.04) but shows typical colour and host properties. Semi-analytical modelling of the light curve suggests a compact CSM shell (∼1012 − 1013 cm), mass-loss rate between 10−4 − 10−3 M⊙ yr−1 with CSM and ejecta masses of ∼0.22 and 0.12 M⊙, respectively. Post-maximum light-curve, spectral modelling favours a ∼3 M⊙ helium star progenitor with extended (∼1015 cm), stratified CSM (density exponent of 2.9) and mass-loss rate of 0.1 − 2.7 M⊙ yr−1. These two mass-loss regimes imply a radially varying CSM, shaped by asymmetry or changes in the progenitor's mass loss over time. This mass-loss behavior fits both binary and single-star evolution. Early Icn-like features stem from hot carbon ionization, fading to Ibn-like with cooling. SN 2023xgo thus offers rare insight into the connection between SNe Icn, Ibn, and SNe Ibn with ejecta signatures."", ""Abstract AT2022rze is a luminous, ambiguous transient located South-East of the geometric center of its host galaxy at redshift z = 0.08. The host appears to be formed by a merging galaxy system. The observed characteristics of AT2022rze are reminiscent of active galactic nuclei (AGN), tidal disruption events (TDEs), and superluminous supernovae (SLSNe). The transient reached a peak absolute magnitude of −20.2 ± 0.2 mag, showing a sharp rise (trise, 1/e = 27.5 ± 0.6 days) followed by a slow decline (tdec, 1/e = 382.9 ± 0.6). Its bumpy light curve and narrow Balmer lines indicate the presence of gas (and dust). Its light curve shows rather red colors, indicating that the transient could be affected by significant host extinction. The spectra reveal coronal lines, indicative of high-energy (X-ray/UV) emission. Archival data reveal no prior activity at this location, disfavoring a steady-state AGN, although an optical spectrum obtained prior to the transient is consistent with an AGN classification of the host. Based on this, we conclude that the transient most likely represents a Changing-look AGN at the center of the smallest component of the merging system."", ""Abstract We investigate the optical shock emission from the Large Magellanic Cloud supernova remnant 0540–69.3 (SNR 0540) using MUSE integral-field-unit data from the VLT. The observations cover the spectral range 4650–9300 Å and provide a 1 × 1 arcmin2 field of view, encompassing nearly the entire remnant. We analyse the spatial and spectral properties of shock-related emission lines, and identify clumpy optical shock emission e.g. from [S ii] λλ6716,6731 doublet and the coronal [Fe xiv] λ5303 line (typically at radial velocities ≲ |100| km s−1 and ≲ |170| km s−1, respectively). These features trace the blast-wave shell seen in previous X-ray studies. Post-shock electron density estimates, based on the [S ii]-line ratio, reveal spatial variation, with the highest densities (∼104 cm−3) in the bright knots in the west, and lower densities (∼3 × 103 cm−3) in the east. The density in the north (southwest) appears significantly lower (higher) but remains unconstrained due to limited signal. We also estimate blast-wave shock velocities using the [Fe xiv] λ5303/[Fe xi] λ7892 ratio, finding low velocities (∼400 km s−1), consistent with previous studies. All these results support the scenario that the blast wave is interacting with the surrounding interstellar medium, particularly in the western regions. Additionally, we detect four unidentified emission lines, ∼2000–3000 km s−1 south from the pulsar in transverse velocity, but their origin remains unclear. Possible explanations, including Fe lines from a high-velocity ejecta clump, all present challenges. Our findings highlight the complex nature of the circum- and interstellar medium surrounding SNR 0540."", ""Abstract We present the optical discovery and multiwavelength follow-up observations of AT 2024kmq, a likely tidal disruption event (TDE) associated with a supermassive ( M BH ∼ 10 8 M ⊙ ) black hole in a massive galaxy at z = 0.192. The optical light curve of AT 2024kmq exhibits two distinct peaks: an early fast (timescale 1 day) and luminous ( M ≈ −20 mag) red peak, then a slower (timescale 1 month) blue peak with a higher optical luminosity ( M ≈ −22 mag) and featureless optical spectra. The second component is similar to the spectroscopic class of “featureless TDEs” in the literature, and during this second component we detect highly variable, luminous ( L X ≈ 10 44 erg s −1 ), and hard ( f ν ∝ ν −1.5 ) X-ray emission. Luminous (10 29 erg s −1 Hz −1 at 10 GHz) but unchanging radio emission likely arises from an underlying active galactic nucleus. The luminosity, timescale, and color of the early red optical peak can be explained by synchrotron emission, or alternatively by thermal emission from material at a large radius ( R ≈ a few × 10 15 cm). Possible physical origins for this early red component include an off-axis relativistic jet, and shocks from self-intersecting debris leading to the formation of the accretion disk. Late-time radio observations will help distinguish between the two possibilities.""]"
"G. Ripellino","","","Computer Science","https://openalex.org/A5055373831","[""A bstract This paper investigates the search for long-lived dark scalars from exotic Higgs boson decays at the Future Circular Collider in its e + e − stage, FCC-ee, considering an integrated luminosity of 10 . 8 ab −1 collected during the ZH run at a center-of-mass energy $$ \\sqrt{s} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> </mml:math> = 240 GeV. The work considers Zh events where the Z boson decays leptonically and the Higgs boson h decays into two long-lived dark scalars s which further decay into bottom anti-bottom quark pairs. The analysis is performed using a parametrized simulation of the IDEA detector concept and targets dark scalar decays in the tracking volume, resulting in multiple displaced vertices in the final state. The sensitivity towards long-lived dark scalars at FCC-ee is estimated using an event selection requiring two opposite-charge, same-flavor leptons compatible with the Z boson, and at least two displaced vertices in the final state. The selection is seen to efficiently remove the Standard Model background, while retaining sensitivity for dark scalar masses between m s = 20 GeV and m s = 60 GeV and mean proper lifetimes cτ between approximately 10 mm and 10 m. The results show that the search strategy has potential to probe Higgs to dark scalar branching ratios as low as 10 −4 for a mean proper lifetime cτ ≈ 1 m. The results provide the first sensitivity estimate for exotic Higgs decays at FCC-ee with the IDEA detector concept, using the common FCC framework."", ""This study presents a novel method for the definition of signal regions in searches for new physics at collider experiments. By leveraging multi-dimensional histograms with precise arithmetic and utilizing the SparkDensityTree library, it is possible to identify high-density regions within the available phase space, potentially improving sensitivity to very small signals. Inspired by a search for dark mesons at the ATLAS experiment, CMS open data is used for this proof-of-concept intentionally targeting an already excluded signal. Signal regions are defined based on density estimates of signal and background. These preliminary regions align well with the physical properties of the signal while effectively rejecting background events."", ""This paper investigates the search for long-lived dark scalars from exotic Higgs boson decays at the Future Circular Collider in its $e^+e^-$ stage, FCC-ee, considering an integrated luminosity of 10.8 $\\text{ab}^{-1}$ collected during the ZH run at a center-of-mass energy $\\sqrt{s}=240$ GeV. The work considers $Zh$ events where the $Z$ boson decays leptonically and the Higgs boson $h$ decays into two long-lived dark scalars $s$ which further decay into bottom anti-bottom quark pairs. The analysis is performed using a parametrized simulation of the IDEA detector concept and targets dark scalar decays in the tracking volume, resulting in multiple displaced vertices in the final state. The sensitivity towards long-lived dark scalars at FCC-ee is estimated using an event selection requiring two opposite-charge, same-flavor leptons compatible with the $Z$ boson, and at least two displaced vertices in the final state. The selection is seen to efficiently remove the Standard Model background, while retaining sensitivity for dark scalar masses between $m_s=20$ GeV and $m_s=60$ GeV and mean proper lifetimes $c\\tau$ between approximately 10 mm and 10 m The results show that the search strategy has potential to probe Higgs to dark scalar branching ratios as low as $10^{-4}$ for a mean proper lifetime $c\\tau\\approx 1$ m. The results provide the first sensitivity estimate for exotic Higgs decays at FCC-ee with the IDEA detector concept, using the common FCC framework."", ""This study presents a novel method for the definition of signal regions in searches for new physics at collider experiments, specifically those conducted at CERN's Large Hadron Collider. By leveraging multi-dimensional histograms with precise arithmetic and utilizing the SparkDensityTree library, it is possible to identify high-density regions within the available phase space, potentially improving sensitivity to very small signals. Inspired by an ongoing search for dark mesons at the ATLAS experiment, CMS open data is used for this proof-of-concept intentionally targeting an already excluded signal. Several signal regions are defined based on density estimates of signal and background. These preliminary regions align well with the physical properties of the signal while effectively rejecting background events. While not explored in this work, this method is also scalable, which makes it ideal for large datasets such as those expected at the high-luminosity upgrade of the LHC. Finally, this method is flexible and can be easily extended, promising a boost to the signal region definition process for new physics searches at colliders.""]"
"Gerd Meyer","","","Computer Science","https://openalex.org/A5068165268","[""The understanding of structure and bonding in intermetallic phases still lags behind that of molecular compounds. For that reason, exploring intermetallic phases and identifying structural patterns and relationships are particularly important for closing this knowledge gap. In particular, here we report on the addition of increasing amounts of platinum to ∼2:1 mixtures of tin and neodymium, which yields eight ternary Pt/Sn/Nd compounds, four of which have not been reported before. Interestingly, except for PtSnNd (1), all observed ternary phases of the system can be derived from the binary compounds Sn2Nd and Sn5Nd2 by adding Pt to the composition(s), as they lie on or close to two lines: Sn2Nd-Pt (Pt0.21(1)Sn2Nd (2), PtSn2Nd (3), Pt1.33Sn2Nd (4), Pt2-xSn2+xNd (x = 0.27(3), 5), and Pt3Sn2Nd (6)) or Sn5Nd2-Pt (Pt1.5Sn5-xNd2 (x = 0.16(2), 7) and Pt3Sn5Nd2-x (x = 0.161(8), 8)). While the introduction of increasing amounts of Pt to the binaries Sn2Nd and Sn5Nd2 leads to stepwise changes in the coordination environment of Nd, Pt preserves its coordination over the entire system in the form of interpenetrating bipyramidal {PtSn5Nd5} clusters."", ""An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures."", ""The rare earth (oxides), R2O3, may be converted into trihalides, RX3, by a number of different synthetic routes of which the ammonium halide route is inexpensive and easy to perform. It runs through ternary ammonium rare-earth halides, e.g. (NH4)3YCl6, and, with sufficient care, avoids the formation of oxide-halides, e.g. YOCl. Nevertheless the formation usually thought as a pitfall, can be a blessing, as the first synthesis of {OYb4}Cl6 attests. Again, there is a number of methods to reduce trihalides to lower oxidation states, most prominently to the divalent state. Binary dihalides, RX2, as well as ternaries such as ARX3, are either prepared by comproportionation or metallothermic reduction reactions, to name the two most prolific routes. Further reduction results in metal-rich halides, of which the most abundant are (complex) octahedral cluster halides, in most cases sequestering a main-group (E) or transition metal (T) atom to overcome the electron paucity of group 3 rare-earth metal atoms R. Clusters in cluster halides with endohedral atoms, e.g. [{PtPr6}I12]Pr, may be isolated or connected through common edges to oligomers, chains, layers or even to three-dimensional structures. Crystal growth is conveniently achieved from rare-earth halide melts. These halide fluxes can be reactive, hence produce complex cluster halides with endohedral atoms, or non-reactive and act as a medium for crystal growth of (polar) intermetallics, e.g. {Pt3Pr4} from a PrCl3 flux. Alternative innocent fluxes can be alkali-metal halides; reactive fluxes such as tin produce ternary intermetallics, e.g. {PtSn2}Nd.Download : Download high-res image (215KB)Download : Download full-size image"", ""An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures."", ""An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures.""]"
"Lars Hultman","","","Computer Science","https://openalex.org/A5086073947","[""Achieving large two-dimensional (2D) sheets of any metal is challenging due to their tendency to coalescence or cluster into 3D shapes. Recently, single-atom-thick gold sheets, termed goldene, was reported. Here, we ask if goldene can be extended to include multiple layers. The answer is yes, and trilayer goldene is the magic number, for reasons of electronegativity. Experiments are made to synthesize the atomically laminated phase Ti 4 Au 3 C 3 through substitutional intercalation of Si layers in Ti 4 SiC 3 for Au. Density functional theory calculations suggest that it is energetically favorable to insert three layers of Au into Ti 4 SiC 3 , compared to inserting a monolayer, a bilayer, or more than three layers. Isolated trilayer goldene sheets, ~100 nanometers wide and 6.7 angstroms thick, were obtained by chemically etching the Ti 4 C 3 layers from Ti 4 Au 3 C 3 templates. Furthermore, trilayer goldene is found in both hcp and fcc forms, where the hcp is ~50 milli–electron volts per atom more stable at room temperature from ab initio molecular dynamic simulations."", ""Abstract Metallenes are presented for a new class of single-atom-thick two-dimensional (2D) metal sheets. It is motivated by a recent (2024) discovery of 2D gold, dubbed goldene, by selectively etching off Ti3C2 slabs from a Ti3AuC2 nanolaminate. This synthesis-derivative method bypasses the natural tendency for metals to form three-dimensional forms. Thus, 2D-materials’ research goes beyond ceramics with graphene as the most-known example. A range of noble and non-noble metals are now proposed for metallene preparation. Their exploration is motivated by the unique properties offered by 2D and nanostructured materials. Metallene’s ultimate high surface-to-volume ratio with abundant uncoordinated metal atoms makes them attractive for high-end applications, like in catalysis, sensing, electronics, and biomedicine. Challenges for scientific research and practical use, however, lie in scalable synthetic processes, sheet integrity, and transfer methods. Here, we review state-of-the-art for processes to prepare atomic and few-atomic layer-thick noble metals, as well as their characterization and properties.""]"
"Marie-Louise G. Wadenberg","","","Computer Science","https://openalex.org/A5112766495","[""The acetylcholine esterase inhibitor/cholinergic nicotinic receptor (nAChR) allosteric modulator galantamine (Gal) is used against cognitive impairment in Alzheimer's disease. Negative/cognitive and psychotic symptom improvement in schizophrenia by adjunct Gal to antipsychotic drugs (APDs) has been reported. Cognitive symptoms in schizophrenia may involve brain prefrontal hypo-dopaminergia. Experimental data by others indicate nAChR involvement in animal pro-cognitive effects of Gal. The role of nAChRs in antipsychotic effects by Gal has, however, not been elucidated. Using the conditioned avoidance response (CAR) and the catalepsy tests for antipsychotic activity and extrapyramidal side-effect (EPS) liability, respectively, we here investigated the effects of adjunct Gal (1.25 mg/kg) to the typical APD haloperidol (Hal) (0.05 mg/kg), or the atypical APD risperidone (Ris) (0.2 mg/kg), in rats. Adjunct Gal significantly enhanced APD-like effects by low doses of Hal or Ris, but showed a safe EPS liability profile only in combination with Ris. Pretreatment with the muscarinic receptor (mAChR) antagonist scopolamine, but not the nAChR antagonist mecamylamine, completely reversed the enhancing effects of adjunct Gal to Hal treatment, in the CAR test. While the nAChR-modulating properties of Gal probably contribute to pro-cognitive activity, as shown by others, the present data suggest that any contribution to antipsychotic activity by Gal is mediated primarily via mAChRs. This property combination of Gal may offer a unique, favourable therapeutic profile for schizophrenia treatment.""]"
"Andrei Khrennikov","","","Computer Science","https://openalex.org/A5082978886","[""A physical model for phenomenological proto-consciousness as an intrinsic, primary property of matter is introduced. A fundamental particle is characterized as a classic-like system in physical space supplemented with an information space that endows the particle with the capacity of storing and processing incoming information which has causal power on the behaviour of the particle. These features transform the postulated initial random particles into information-theoretic Darwinian physical systems controlled by algorithms susceptible of evolution under natural selection. Consciousness is then defined as the elementary microscopic sentient process of subjectively experiencing a representation of the locations of the surrounding systems that is induced on matter by the irreversible erasure of dynamically superfluous stored information when the particle is measured. A physical description of the possible emergence of both quantum behaviour of matter in accordance with the conventional quantum formalism and consciousness in complex biological systems as a quantum coherence phenomenon is explored."", ""In this work, we conduct a sentiment analysis of English-language reviews using a quantum-like (wave-based) model of text representation. This model is explored as an alternative to machine learning (ML) techniques for text classification and analysis tasks. Special attention is given to the problem of segmenting text into semantic units, and we illustrate how the choice of segmentation algorithm is influenced by the structure of the language. We investigate the impact of quantum-like semantic interference on classification accuracy and compare the results with those obtained using classical probabilistic methods. Our findings show that accounting for interference effects improves accuracy by approximately 15%. We also explore methods for reducing the computational cost of algorithms based on the wave model of text representation. The results demonstrate that the quantum-like model can serve as a viable alternative or complement to traditional ML approaches. The model achieves classification precision and recall scores of around 0.8. Furthermore, the classification algorithm is readily amenable to optimization: the proposed procedure reduces the estimated computational complexity from O(n2) to O(n)."", ""This paper investigates the properties of the Gorini–Kossakowski–Sudarshan–Lindblad (GKSL) equation within the camel-like framework, with a focus on quantum correlations in the context of open quantum systems. Here, we compute quantum correlations such as quantum discord and quantum steering, analyzing their behavior under decoherence and environmental interaction for three sets of quantum states. Our results indicate that the sign of the entanglement entropy's derivative serves as an indicator of the system's drift toward classical or quantum information exchange—an insight with important implications for quantum error correction and dissipation processes in quantum thermal machines. \\\\ Moreover, we parametrize quantum states using both single-parameter and Bloch-sphere representations. The Bloch-sphere analysis is particularly developed by examining the topology of the quantum states as elements on the \\(\\mathbb{S}^2\\) sphere, yielding a gradient map and a topological basin map which illustrate how quantum states evolve under the camel-like framework and how this can be partitioned into basins on the Bloch sphere, by stability/instability against decoherence. The decoherence-unstable regions form a Braiding ring around the Bloch sphere at the circle defined by $\\theta=\\frac{3\\pi}{4}$. Notably, these unstable states on the Braiding ring form attraction points for the evolution of states by a constructed Lyapunov function, which gives an illustration of the complex interplay between geometry and dynamics and which shapes the topological geometry on the Bloch sphere. This deeper analysis underscores that quantum dynamics are governed not only by local energetics but also by the global structure of the state space. Moreover, we also derive a testable rudimentary experimental setup from the camel-like entropy, which describes the unitary evolution of pure states on the Bloch surface and add the analytical solutions of selected quantum states."", ""We proposed a unified principle of the universe and existence based on adaptive dynamics (Ando et al., AJIS 14, No.3, 2025). This principle posits the fundamental interconnection of all entities and events, akin to the concept of causation in Buddhism. Through scientific advancements, humanity has reached a stage where enlightenment—akin to Buddhahood—is potentially accessible to all. Thus, we advocate for a broad understanding of these principles and encourage efforts to navigate the challenges of our increasingly restricted and overpopulated world. Received: 27 April 2025 / Accepted: 26 June 2025 / Published: 08 July 2025""]"
"Erik G. Larsson","","","Computer Science","https://openalex.org/A5043552696","[""Machine learning methods have been shown to be effective for weather forecasting, based on the speed and accuracy compared to traditional numerical models. While early efforts primarily concentrated on deterministic predictions, the field has increasingly shifted toward probabilistic forecasting to better capture the forecast uncertainty. Most machine learning-based models have been designed for global-scale predictions, with only limited work targeting regional or limited area forecasting, which allows more specialized and flexible modeling for specific locations. This work introduces Diffusion-LAM, a probabilistic limited area weather model leveraging conditional diffusion. By conditioning on boundary data from surrounding regions, our approach generates forecasts within a defined area. Experimental results on the MEPS limited area dataset demonstrate the potential of Diffusion-LAM to deliver accurate probabilistic forecasts, highlighting its promise for limited-area weather prediction."", ""Decentralized learning enables distributed agents to train a shared machine learning model through local computation and peer-to-peer communication. Although each agent retains its dataset locally, the communication of local models can still expose private information to adversaries. To mitigate these threats, local differential privacy (LDP) injects independent noise per agent, but it suffers a larger utility gap than central differential privacy (CDP). We introduce Whisper D-SGD, a novel covariance-based approach that generates correlated privacy noise across agents, unifying several state-of-the-art methods as special cases. By leveraging network topology and mixing weights, Whisper D-SGD optimizes the noise covariance to achieve network-wide noise cancellation. Experimental results show that Whisper D-SGD cancels more noise than existing pairwise-correlation schemes, substantially narrowing the CDP-LDP gap and improving model performance under the same privacy guarantees.""]"
"Mauro Conti","","","Computer Science","https://openalex.org/A5063847107","[""Convolutional neural networks (CNNs) are vulnerable to adversarial attacks in computer vision tasks. Current adversarial detections are ineffective against white-box attacks and inefficient when deep CNNs generate high-dimensional hidden features. This study proposes MeetSafe, an effective and scalable adversarial example (AE) detection against white-box attacks. MeetSafe identifies AEs using critical hidden features rather than the entire feature space. We observe a non-uniform distribution of Z-scores between clean samples and adversarial examples (AEs) among hidden features and propose two utility functions to select those most relevant to AEs. We process critical hidden features using feature engineering methods: local outlier factor (LOF), feature squeezing, and whitening, which estimate feature density relative to its k-neighbors, reduce redundancy, and normalize features. To deal with the curse of dimensionality and smooth statistical fluctuations in high-dimensional features, we propose local reachability density (LRD). Our LRD iteratively selects a bag of engineered features with random cardinality and quantifies their average density by its k-nearest neighbors. Finally, MeetSafe constructs a Gaussian Mixture Model (GMM) with the processed features and detects AEs if it is seen as a local outlier, shown by a low density from GMM. Experimental results show that MeetSafe achieves 74%, 96%, and 79% of detection accuracy against adaptive, classic, and white-box attacks, respectively, and at least 2.3× faster than comparison methods.""]"
"Richard Bellman","","","Computer Science","https://openalex.org/A5076973014","[""The Bellman""]"
"Weimin Chen","","","Computer Science","https://openalex.org/A5020453892","[""Abstract The effects of Sb incorporation on the molecular beam epitaxial growth of GaInNAs triple quantum well (QW) core–multishell nanowires on Si(111) substrates were investigated. Sb was not directly incorporated into the QWs but was predominantly localized at the interface between the QWs and the adjacent GaAs(Sb) barrier, indicating significant Sb segregation with the concentration reaching approximately 1%, which was independently confirmed by optical measurements. Room-temperature photoluminescence measurements reveal a significant red shift of the QW emission peak from 1100 nm in Sb-free nanowires to 1250 nm in the Sb-containing structures. These findings suggest that Sb acts as an efficient surfactant."", ""Abstract Spin polarized excitons induced by spin injection from magnetic ion to a single quantum dot, has been considered as a basic unit of quantum information transfer between spin and photon for spin-photonic applications. However, this state-of-the-art technology has only been found with limited coupling strength and weak excitonic emission. Here, we demonstrate a spin-polarized self-trapped exciton naturally formed in the zero-dimensional lattice of cesium copper iodide. Upon excitation, the conversion from Cu + ion to spin-1/2 Cu 2+ ion results in an in-situ self-trapped exciton, which facilitates a local Jahn-Teller distortion and guarantees the strong spin-exciton coupling and near-unity excitonic emission efficiency. Consequently, a giant Zeeman splitting of −53 meV and an effective excitonic g-factor of −93.5 are observed from magneto-photoluminescence. More importantly, this nano-scale coupling can also be driven by an external electric field, which generates electroluminescence with a circular polarization of 44.5% at 4.2 K and 8% at 300 K. The spin-optic properties of this copper compound will stimulate the fabrication of next-generation spin-photonic devices based on self-trapped excitons."", ""Due to its attractive electronic properties, the GaNAs alloy is considered a promising material for optoelectronic applications in the near-infrared spectral region. Unfortunately, nitrogen incorporation is also known to lead to material degradation due to the formation of non-radiative defects and strong band tailing effects caused by alloy disorder. In this study, we show that post-growth hydrogenation of GaNAs-based nanowires (NWs) can largely suppress these unwanted effects. First, we find that this treatment results in a more homogeneous electronic structure due to the passivation of nitrogen-related band tail states, without affecting the bandgap energy of the material. Additionally, hydrogenation reduces the density of quantum emitters that are spontaneously formed in dilute nitride NWs upon N incorporation. This leads to spectrally isolated emission lines from these emitters, which is important for creating high-purity single-photon sources. Finally, the treatment improves the overall optical quality of the material, giving an up to threefold increase in the intensity of band-to-band emission after hydrogenation. Our findings, therefore, highlight the potential of hydrogenation as a viable approach for improving material quality and tailoring the optoelectronic properties of GaNAs NWs without compromising their emission wavelength, paving the way for their integration into telecom-compatible photonic devices.""]"
"P. Christiansen","","","Computer Science","https://openalex.org/A5021081970","[""The Large Hadron Collider (LHC) at CERN became operational in 2009 and has since then produced a plethora of physics results from proton–proton ( pp ) collisions. This short review covers results that relate to soft quantum chromodynamics (QCD) with a focus on nondiffractive physics at midrapidity. Most of the presented results are based on transverse momentum spectra and related derived observables, including multiplicity, the average transverse momentum, and various particle ratios. Additionally, the phenomenon of the observed ridge and its potential connection to the formation of a quark–gluon plasma in pp collisions are discussed. The goals of the review are to introduce the topics and provide references for scientists joining the LHC program and to highlight what we consider to be the most interesting results and open questions, to inspire novel measurements."", ""This corrects the article DOI: 10.1103/PhysRevLett.116.122301."", ""The PHENIX experiment measured the centrality dependence of two-pion Bose-Einstein correlation functions in √𝑠𝑁⁢𝑁=200GeV Au+Au collisions at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory. The data are well represented by Lévy-stable source distributions. The extracted source parameters are the correlation-strength parameter 𝜆, the Lévy index of stability 𝛼, and the Lévy-scale parameter 𝑅 as a function of transverse mass 𝑚𝑇 and centrality. The 𝜆⁡(𝑚𝑇) parameter is constant at larger values of 𝑚𝑇, but decreases as 𝑚𝑇 decreases. The Lévy-scale parameter 𝑅⁡(𝑚𝑇) decreases with 𝑚𝑇 and exhibits proportionality to the length scale of the nuclear overlap region. The Lévy exponent 𝛼⁡(𝑚𝑇) is independent of 𝑚𝑇 within uncertainties in each investigated centrality bin, but shows a clear centrality dependence. At all centralities, the Lévy exponent 𝛼 is significantly different from that of Gaussian (𝛼=2) or Cauchy (𝛼=1) source distributions. Comparisons to the predictions of Monte-Carlo simulations of resonance-decay chains show that, in all but the most peripheral centrality class (50%–60%), the obtained results are inconsistent with the measurements, unless a significant reduction of the in-medium mass of the 𝜂′ meson is included. In each centrality class, the best value of the in-medium 𝜂′ mass is compared to the mass of the 𝜂 meson, as well as to several theoretical predictions that consider restoration of U𝐴⁢(1) symmetry in hot hadronic matter. locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon locked icon Physics Subject Headings (PhySH)Particle correlations & fluctuationsQuark-gluon plasmaRelativistic heavy-ion collisionsBose-Einstein condensatesPionsHadron colliders"", ""A bstract This study provides an analysis of atmospheric neutrino oscillations at the ESSnuSB far detector facility. The prospects of the two cylindrical Water Cherenkov detectors with a total fiducial mass of 540 kt are investigated over 10 years of data taking in the standard three-flavor oscillation scenario. We present the confidence intervals for the determination of mass ordering, θ 23 octant as well as for the precisions on sin 2 θ 23 and $$ \\left|\\Delta {m}_{31}^2\\right| $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mfenced> <mml:mrow> <mml:mi>Δ</mml:mi> <mml:msubsup> <mml:mi>m</mml:mi> <mml:mn>31</mml:mn> <mml:mn>2</mml:mn> </mml:msubsup> </mml:mrow> </mml:mfenced> </mml:math> . It is shown that mass ordering can be resolved by 3 σ CL (5 σ CL) after 4 years (10 years) regardless of the true neutrino mass ordering. Correspondingly, the wrong θ 23 octant could be excluded by 3 σ CL after 4 years (8 years) in the case where the true neutrino mass ordering is normal ordering (inverted ordering). The results presented in this work are complementary to the accelerator neutrino program in the ESSnuSB project."", ""In collisions between heavy nuclei, such as those at the Large Hadron Collider (LHC) at CERN, hydrodynamic models have successfully related measured azimuthal momentum anisotropies to the transverse shape of the collision region. For an elliptically shaped interaction area, the hydrodynamic pressure gradient is greater along the minor axis, resulting in increased particle momentum in that direction - a phenomenon known as positive elliptic flow. In this paper, we demonstrate that in smaller systems, such as proton-proton and peripheral ion-ion collisions, microscopic models for final state interactions, can produce anisotropies where the elliptic flow is negative - that is, the momentum is largest along the major axis, contrary to hydrodynamic predictions. We present results from two distinct microscopic models: one based on repulsion between string-like fields and another based on effective kinetic theory. Negative elliptic flow is a solid prediction of the string interaction model while in the model based on kinetic theory it is linked to a finite interaction range. Consequently, an experimental determination of the sign of elliptic flow, will provide novel insights into the degrees of freedom governing strong nuclear interactions in high-energy collisions and the way in which they interact.""]"
"Stefano Moretti","","","Computer Science","https://openalex.org/A5034676393","[""We study resonant production of pairs of Standard Model (SM-)like Higgs bosons, in the presence of new neutral Higgs states together with new colored scalars (stops or sbottoms) in loops within the Next-to-Minimal Supersymmetric Standard Model. This is used as a test case to prove that the Large Hadron Collider has sensitivity to a variety of effects stemming from interferences between resonant (heavy) Higgs diagrams and/or among these and nonresonant topologies involving loops of both tops and stops. These effects can alter significantly the naive description of individual <a:math xmlns:a=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><a:mrow><a:mi>s</a:mi></a:mrow></a:math>-channel Breit-Wigner resonances, leading to distortions of the latter, which, on the one hand, may mask their presence but, on the other hand, could enable one to extract features of the underlying new physics scenario. This last aspect is made possible through a decomposition of the <c:math xmlns:c=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><c:mi>g</c:mi><c:mi>g</c:mi><c:mo stretchy=\""false\"">→</c:mo><c:mi>h</c:mi><c:mi>h</c:mi></c:math> signal process into all its amplitude components, each of which has a well-defined coupling structure. Ultimately, such effects can be traced back to the relevant Feynman diagrams and can enable a detailed interpretation of this process. To illustrate this, we introduce various benchmark points that exhibit potentially observable features during the current and/or upcoming runs of the LHC in one or more of the three customary di-Higgs decay channels: <f:math xmlns:f=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><f:mi>b</f:mi><f:mover accent=\""true\""><f:mi>b</f:mi><f:mo stretchy=\""false\"">¯</f:mo></f:mover><f:mi>b</f:mi><f:mover accent=\""true\""><f:mi>b</f:mi><f:mo stretchy=\""false\"">¯</f:mo></f:mover></f:math>, <l:math xmlns:l=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><l:mi>b</l:mi><l:mover accent=\""true\""><l:mi>b</l:mi><l:mo stretchy=\""false\"">¯</l:mo></l:mover><l:msup><l:mi>τ</l:mi><l:mo>+</l:mo></l:msup><l:msup><l:mi>τ</l:mi><l:mo>−</l:mo></l:msup></l:math>, and <p:math xmlns:p=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><p:mi>b</p:mi><p:mover accent=\""true\""><p:mi>b</p:mi><p:mo stretchy=\""false\"">¯</p:mo></p:mover><p:mi>γ</p:mi><p:mi>γ</p:mi></p:math>."", ""Expectations for an imminent new outburst of the recurrent symbiotic nova T CrB are mounting, initiated by the discovery in 2015 of a new enhanced mass-transfer phase (SAP), which is reminiscent of the one preceding the last recorded outburst in 1946. We aim to derive a robust estimate of the most important parameters describing the physical nature of T CrB, trace the accretion history onto its white dwarf, and account for the unexpected delay in the occurrence of the new outburst. In particular, the SAP prior to 1946 was brighter and followed by a nova eruption within six months from its conclusion. This time the 2015-2023 SAP has been fainter and although two years have passed since the end of this phase, no new eruption has taken place. Between 2005-2025, the period covering the SAP and the preceding quiescence, we collected a massive amount of photometric and spectroscopic observations at optical wavelengths. We analyzed these data together with the abundant ultraviolet (UV) observations available in the archive of the Swift satellite. Guided by the results of the orbital solution and, in particular, by the radiative modeling process we employed on the whole set of available data, we derived for T CrB a binary period of 227.5528 days, along with an inclination of 61^̧irc and masses of 1.35 M_⊙ and 0.93 M_⊙ for the white dwarf and the M3III companions, respectively, making the mass transfer dynamically stable. The red giant completely fills its Roche lobe and at $V_ rot i$=4.75±0.26 km, s^-1, it is rotating much more slowly than the 16 km, s^-1 co-rotation value. The ∼20^̧irc azimuth of the hot spot, implied by the hump shaping the optical light curve in quiescence, fixes the outer radius of the disk to ∼58 R_⊙. This is the same as the canonical value expected from disk theory. In quiescence, the disk is cold and mostly neutral. The SAP was caused by an inside-out collapse of the disk, during which the mean accretion rate onto the WD was ∼28times larger than in quiescence. The SAP ended in late April 2023, but from May 2024, the mass flow has intensively resumed at disk inner radii, while the collapse wave reached the outer portions of the disk. The consequent revamp in the mass accretion could fill the gap inherited by the fainter 2015-2023 SAP and eventually lead the WD accreted shell to ignition."", ""This work investigates the discovery potential for singly produced vectorlike quarks (VLQs) <a:math xmlns:a=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><a:mi>T</a:mi></a:math> (<c:math xmlns:c=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><c:mi>Q</c:mi><c:mo>=</c:mo><c:mo>+</c:mo><c:mn>2</c:mn><c:mo>/</c:mo><c:mn>3</c:mn><c:mi>e</c:mi></c:math>) and <e:math xmlns:e=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><e:mi>Y</e:mi></e:math> (<g:math xmlns:g=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><g:mi>Q</g:mi><g:mo>=</g:mo><g:mo>−</g:mo><g:mn>4</g:mn><g:mo>/</g:mo><g:mn>3</g:mn><g:mi>e</g:mi></g:math>) decaying to <i:math xmlns:i=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><i:mi>W</i:mi><i:mi>b</i:mi></i:math> at future <k:math xmlns:k=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><k:mi>μ</k:mi><k:mi>p</k:mi></k:math> colliders with <m:math xmlns:m=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><m:msqrt><m:mi>s</m:mi></m:msqrt><m:mo>=</m:mo><m:mn>5.29</m:mn></m:math>, 6.48, and 9.16 TeV, analyzing both leptonic and hadronic <o:math xmlns:o=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><o:mi>W</o:mi></o:math> decay channels through detailed detector simulations. The hadronic channel demonstrates superior sensitivity, enabling <q:math xmlns:q=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><q:mn>5</q:mn><q:mi>σ</q:mi></q:math> discovery up to <s:math xmlns:s=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><s:msub><s:mi>m</s:mi><s:mi>T</s:mi></s:msub><s:mo>=</s:mo><s:mn>3750</s:mn><s:mtext> </s:mtext><s:mtext> </s:mtext><s:mi>GeV</s:mi></s:math> (<u:math xmlns:u=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><u:msub><u:mi>m</u:mi><u:mi>Y</u:mi></u:msub><u:mo>=</u:mo><u:mn>4100</u:mn><u:mtext> </u:mtext><u:mtext> </u:mtext><u:mi>GeV</u:mi></u:math>) at 9.16 TeV with <w:math xmlns:w=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><w:mn>100</w:mn><w:mtext> </w:mtext><w:mtext> </w:mtext><w:msup><w:mi>fb</w:mi><w:mrow><w:mo>−</w:mo><w:mn>1</w:mn></w:mrow></w:msup></w:math>, while exclusion limits reach <y:math xmlns:y=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><y:msub><y:mi>m</y:mi><y:mi>T</y:mi></y:msub><y:mo>=</y:mo><y:mn>4500</y:mn><y:mtext> </y:mtext><y:mtext> </y:mtext><y:mi>GeV</y:mi></y:math> (<ab:math xmlns:ab=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><ab:msup><ab:mi>g</ab:mi><ab:mo>*</ab:mo></ab:msup><ab:mo>≥</ab:mo><ab:mn>0.06</ab:mn></ab:math>) and <cb:math xmlns:cb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><cb:msub><cb:mi>m</cb:mi><cb:mi>Y</cb:mi></cb:msub><cb:mo>=</cb:mo><cb:mn>4800</cb:mn><cb:mtext> </cb:mtext><cb:mtext> </cb:mtext><cb:mi>GeV</cb:mi></cb:math> (<eb:math xmlns:eb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><eb:msub><eb:mi>κ</eb:mi><eb:mi>Y</eb:mi></eb:msub><eb:mo>≥</eb:mo><eb:mn>0.04</eb:mn></eb:math>)—significantly beyond LHC capabilities. At 5.29 TeV, discovery regions cover <gb:math xmlns:gb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><gb:msup><gb:mi>g</gb:mi><gb:mo>*</gb:mo></gb:msup><gb:mo>∈</gb:mo><gb:mo stretchy=\""false\"">[</gb:mo><gb:mn>0.15</gb:mn><gb:mo>,</gb:mo><gb:mn>0.5</gb:mn><gb:mo stretchy=\""false\"">]</gb:mo></gb:math> for <kb:math xmlns:kb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><kb:msub><kb:mi>m</kb:mi><kb:mi>T</kb:mi></kb:msub><kb:mo>∈</kb:mo><kb:mo stretchy=\""false\"">[</kb:mo><kb:mn>1500</kb:mn><kb:mo>,</kb:mo><kb:mn>2520</kb:mn><kb:mo stretchy=\""false\"">]</kb:mo><kb:mtext> </kb:mtext><kb:mtext> </kb:mtext><kb:mi>GeV</kb:mi></kb:math> and <ob:math xmlns:ob=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><ob:msub><ob:mi>κ</ob:mi><ob:mi>Y</ob:mi></ob:msub><ob:mo>∈</ob:mo><ob:mo stretchy=\""false\"">[</ob:mo><ob:mn>0.12</ob:mn><ob:mo>,</ob:mo><ob:mn>0.5</ob:mn><ob:mo stretchy=\""false\"">]</ob:mo></ob:math> for <sb:math xmlns:sb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><sb:msub><sb:mi>m</sb:mi><sb:mi>Y</sb:mi></sb:msub><sb:mo>∈</sb:mo><sb:mo stretchy=\""false\"">[</sb:mo><sb:mn>1700</sb:mn><sb:mo>,</sb:mo><sb:mn>2700</sb:mn><sb:mo stretchy=\""false\"">]</sb:mo><sb:mtext> </sb:mtext><sb:mtext> </sb:mtext><sb:mi>GeV</sb:mi></sb:math>, with exclusions extending to <wb:math xmlns:wb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><wb:msub><wb:mi>m</wb:mi><wb:mi>T</wb:mi></wb:msub><wb:mo>=</wb:mo><wb:mn>2750</wb:mn><wb:mtext> </wb:mtext><wb:mtext> </wb:mtext><wb:mi>GeV</wb:mi></wb:math> and <yb:math xmlns:yb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><yb:msub><yb:mi>m</yb:mi><yb:mi>Y</yb:mi></yb:msub><yb:mo>=</yb:mo><yb:mn>3020</yb:mn><yb:mtext> </yb:mtext><yb:mtext> </yb:mtext><yb:mi>GeV</yb:mi></yb:math>. These results, obtained within a simplified two-parameter framework (<ac:math xmlns:ac=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><ac:msub><ac:mi>m</ac:mi><ac:mrow><ac:mi>T</ac:mi><ac:mo>/</ac:mo><ac:mi>Y</ac:mi></ac:mrow></ac:msub></ac:math> and electroweak couplings <cc:math xmlns:cc=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><cc:msup><cc:mi>g</cc:mi><cc:mo>*</cc:mo></cc:msup></cc:math>), establish <ec:math xmlns:ec=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><ec:mi>μ</ec:mi><ec:mi>p</ec:mi></ec:math> colliders as uniquely powerful tools for probing high-mass VLQ states, particularly in the boosted jet regime."", ""Abstract We propose a method for probing CP-violation in the heavy (pseudo)scalar sector of an extended Higgs model, in which we make simultaneous use of the HVV ( $$V=W^\\pm , Z$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mi>V</mml:mi> <mml:mo>=</mml:mo> <mml:msup> <mml:mi>W</mml:mi> <mml:mo>±</mml:mo> </mml:msup> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> </mml:mrow> </mml:math> ) and $$Ht\\bar{t}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mi>H</mml:mi> <mml:mi>t</mml:mi> <mml:mover> <mml:mrow> <mml:mi>t</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> </mml:mrow> </mml:math> interactions of a heavy Higgs state H . The CP-even component of H can be probed through the tree-level HVV interaction, while the CP-odd component of H can be probed if the final $$t\\bar{t}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mi>t</mml:mi> <mml:mover> <mml:mrow> <mml:mi>t</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> </mml:mrow> </mml:math> pair can be tested to form a $$^1S_0$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mmultiscripts> <mml:mrow/> <mml:mrow/> <mml:mn>1</mml:mn> </mml:mmultiscripts> <mml:msub> <mml:mi>S</mml:mi> <mml:mn>0</mml:mn> </mml:msub> </mml:mrow> </mml:math> state. We can then confirm CP-violation if both CP-even and CP-odd components of H are discovered. This is possible at the Compact Linear Collider (CLIC) by exploiting H production from vector-boson fusion (VBF) and decay to $$t\\bar{t}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mrow> <mml:mi>t</mml:mi> <mml:mover> <mml:mrow> <mml:mi>t</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> </mml:mrow> </mml:math> pairs. We analyze the distribution of the azimuthal angle between the leptons coming from top and antitop quarks, which allows one to disentangle the CP nature of such a heavy Higgs state. We also show its implications for the two-Higgs-doublet model (2HDM) with CP-violation."", ""A bstract Aiming to uncover the CP properties of spin-0 particle Dark Matter (DM), we explore a two-component DM scenario within the framework of 3-Higgs Doublet Models (3HDMs), a well-motivated set-up previously studied due to the complementarity of its collider and astrophysical probes. We devise benchmark points in which the two components of DM have same CP in one case and opposite CP in another. We then show several cross section distributions of observables at collider experiments where the two cases are clearly distinguishable.""]"
"Mikael Skoglund","","","Computer Science","https://openalex.org/A5041348422","[]"
"Per Hall","","","Computer Science","https://openalex.org/A5053177403","[""Abstract Background BOADICEA is a widely used algorithm for predicting breast and ovarian cancer risks, using a combination of genetic and lifestyle, hormonal and reproductive risk factors. However, it has largely been developed using data from White/European individuals, limiting its applicability to other ethnicities. Here, we updated BOADICEA to provide ethnicity-specific risk estimates. Methods We utilised data from multiple sources to derive estimates for the distributions and effect sizes of risk factors in major UK ethnic groups (White, Black, South Asian, East Asian, and Mixed), along with ethnicity-specific population cancer incidences. We also developed a method for deriving adjusted polygenic scores for individuals of mixed genetic ancestry. Results The predicted average absolute risks were smaller in all non-White ethnic groups than in Whites, and the risk distributions were narrower. The proportion of women classified as at moderate or high risk of breast or ovarian cancer, according to national guidelines, was considerably smaller in non-Whites. Discussion The updated BOADICEA, available in the CanRisk tool ( www.canrisk.org ), is based on more appropriate estimates for non-White women in the UK. Further validation of the model in prospective studies is required. Considering these findings, risk classification guidelines for non-White women may need to be revised."", ""The role of germline genetics in adjuvant aromatase inhibitor (AI) treatment efficacy in ER-positive breast cancer is poorly understood. We employed a two-stage candidate gene approach to examine associations between survival endpoints and common germline variants in 753 endocrine resistance-related genes. For a discovery cohort, we screened the Breast Cancer Association Consortium database (n ≥ 90,000 cases) and retrieved 2789 AI-treated patients. Cox model-based analysis revealed 125 variants associated with overall, distant relapse-free, and relapse-free survival (p-value ≤ 1E-04). In validation analysis using five independent cohorts (n = 8857), none of the six selected candidates representing major linkage blocks at CELA2B/CASP9, NR1I2/GSK3B, LRP1B, and MIR143HG (CARMN) were validated. We discuss potential reasons for the failed validation and replication of published findings, including study/treatment heterogeneity and other limitations inherent to genomic treatment outcome studies. For the future, we envision prospective longitudinal studies with sufficiently long follow-up and endpoints that reflect the dynamic nature of endocrine resistance."", ""Background BOADICEA is a widely used algorithm for predicting breast and ovarian cancer risks, using a combination of genetic and lifestyle/environmental risk factors. However, it has largely been developed using data from individuals of White ethnicity. Methods We utilised data from multiple sources to derive estimates for the distributions of risk factors and their effect sizes in major UK ethnic groups (White, Black, South Asian, East Asian, and Mixed). We combined these with ethnicity-specific population cancer incidences to update BOADICEA so that it provides ethnicity-specific risk estimates. We also developed and included a method for deriving adjusted polygenic scores for individuals of mixed genetic ancestry. Results The predicted average absolute risks were smaller in all non-White ethnic groups than in Whites, and the risk distributions were narrower. The proportion of women classified as at moderate or high risk of breast or ovarian cancer, according to national guidelines, was considerably smaller in non-White women. Discussion The updated BOADICEA (v7), available in the CanRisk tool (www.canrisk.org), is based on estimates more appropriate for non-White women in the UK. Further validation of the model in prospective studies is required. Considering these findings, risk classification guidelines for non-White women may need to be revised.""]"
"Olli Kallioniemi","","","Computer Science","https://openalex.org/A5050670008","[""As the non-coding genome remains poorly characterized in acute myeloid leukemia (AML), we aimed to identify and functionally characterize novel long non-coding RNAs (lncRNAs) relevant to AML biology and treatment. We first identified lncRNAs overexpressed in AML blasts and, among them, discovered a novel transcript, which we named myeloid and AML-associated intergenic long non-coding RNA (MALNC). MALNC is overexpressed in AML, particularly in cases with the PML-RARA fusion or IDH2R140/NPM1 co-mutations, and is associated with a distinct gene expression profile. Functional studies showed that MALNC knockout impairs AML cell proliferation and colony formation, enhances ATRA-induced differentiation, and sensitizes cells to arsenic trioxide. Transcriptomic analysis revealed that MALNC loss alters the expression of retinoic acid pathway genes, and chromatin binding studies showed that MALNC binds to genes related to the retinoic acid and Rho GTPase pathways. In conclusion, we have identified MALNC as a novel lncRNA that promotes leukemic cell proliferation, counteracts ATRA-induced differentiation, and modulates drug sensitivity in AML."", ""Targeted monotherapies for cancer often fail due to inherent or acquired drug resistance. By aiming at multiple targets simultaneously, drug combinations can produce synergistic interactions that increase drug effectiveness and reduce resistance. Computational models based on the integration of omics data have been used to identify synergistic combinations, but predicting drug synergy remains a challenge. Here, we introduce Drug synergy Interaction Prediction (DIPx), an algorithm for personalized prediction of drug synergy based on biologically motivated tumor- and drug-specific pathway activation scores (PASs). We trained and validated DIPx in the AstraZeneca-Sanger (AZS) DREAM Challenge human cell-line dataset using two separate test sets: Test Set 1 comprised the combinations already present in the training set, while Test Set 2 contained combinations absent from the training set, thus indicating the model’s ability to handle novel combinations. The Spearman’s correlation coefficients between predicted and observed drug synergy were 0.50 (95% CI: 0.47–0.53) in Test Set 1 and 0.26 (95% CI: 0.22–0.30) in Test Set 2, compared to 0.38 (95% CI: 0.34–0.42) and 0.18 (95% CI: 0.16–0.20), respectively, for the best performing method in the Challenge. We show evidence that higher synergy is associated with higher functional interaction between the drug targets, and this functional interaction information is captured by PAS. We illustrate the use of PAS to provide a potential biological explanation in terms of activated pathways that mediate the synergistic effects of combined drugs. In summary, DIPx can be a useful tool for personalized prediction of drug synergy and exploration of activated pathways related to the effects of combined drugs."", ""Ovarian cancer (OC) is a leading cause of death of gynecological cancers in women. Poor patient response to treatment highlights the need to better understand how the tumor microenvironment affects OC progression. Growing evidence indicates the crucial role of non‐cancerous components, such as cancer‐associated fibroblasts, in establishing a complex network of cellular and molecular interactions, influencing cancer progression and response to treatment. Therefore, in this study, we sought to characterize the impact of fibroblasts on OC cell behavior and drug response. Using both direct and indirect cell co‐culture systems, we observed distinct changes in cancer cell proliferation, morphology, and secretome in the presence of fibroblasts. Furthermore, an imaging‐based high‐throughput drug screen of 528 oncology compounds revealed multiple drugs that showed altered efficacy in the co‐culture conditions, demonstrating the role of fibroblasts in driving cancer cell resistance to treatment. Most importantly, our data identified the two drug combinations of Birinapant or Vorinostat with Carboplatin as promising treatments, exploiting the altered cancer cell phenotype in co‐cultures. These findings were supported by the increased sensitivity of ex vivo cultures to these combinations."", ""Abstract Targeted monotherapies for cancer often fail due to inherent or acquired drug resistance. By aiming at multiple targets simultaneously, drug combinations can produce synergistic interactions that increase drug effectiveness and reduce resistance. Computational models based on the integration of omics data have been used to identify synergistic combinations, but predicting drug synergy remains a challenge. Here, we introduce DIPx, an algorithm for personalized prediction of drug synergy based on biologically motivated tumor- and drug-specific pathway activation scores (PASs). We trained and validated DIPx in the AstraZeneca-Sanger (AZS) DREAM Challenge dataset using two separate test sets: Test Set 1 comprised the combinations already present in the training set, while Test Set 2 contained combinations absent from the training set, thus indicating the model’s ability to handle novel combinations. The Spearman correlation coefficients between predicted and observed drug synergy were 0.50 (95% CI: 0.47–0.53) in Test Set 1 and 0.26 (95% CI: 0.22–0.30) in Test Set 2, compared to 0.38 (95% CI: 0.34–0.42) and 0.18 (95% CI: 0.16–0.20), respectively, for the best performing method in the Challenge. We show evidence that higher synergy is associated with higher functional interaction between the drug targets, and this functional interaction information is captured by PAS. We illustrate the use of PAS to provide a potential biological explanation in terms of activated pathways that mediate the synergistic effects of combined drugs. In summary, DIPx can be a useful tool for personalized prediction of drug synergy and exploration of activated pathways related to the effects of combined drugs.""]"
"Bruno Latour","","","Computer Science","https://openalex.org/A5054780327","[""Recorded 18 February 2013. Transcribed by Helen Bradstock."", ""Through our three-way collaboration we sought to understand Gaia and its political implications from the bottom-up and from within. Here we introduce that view of Gaia and how the dialogue between a philosopher (Bruno), a scientist (Tim), and a historian and philosopher of science (Séb) turned into a research programme. This sets in context a previously unpublished piece by Latour: ‘There is nothing simple in a feedback loop – or why goal function is not the problem of Gaia’.""]"
"L. Barranco Navarro","","","Computer Science","https://openalex.org/A5048186003","[""Among all the elementary particles that constitute the Standard Model of particle physics, the top quark can provide key information on fundamental interactions at the electroweak symmetry breaking scale and beyond. Effects of new physics can be parametrised in terms of an Effective Field Theory. This talk presents a selection of the most recent measurements provided by the ATLAS experiment from LHC accelerator in the top-quark sector that have a direct Effective Field Theory interpretation."", ""This thesis presents two analyses devoted to the search of new physics in single-top-quark events with data collected by the ATLAS detector at the LHC. The aim of the first analysis is to probe the Wtb vertex structure through the measurement of W-boson polarization observables using single-top-quark events produced in the t-channel. The dataset corresponds to proton-proton collision events at a center-of-mass energy of 8 TeV with a total integrated luminosity of 20.2/fb. Selected events contain one isolated lepton (electron or muon), large missing transverse momentum and exactly two jets, with one of them identified as likely to contain a b-hadron. Further selection requirements are applied to further separate t-channel single-top-quark events from background. The W boson polarization observables are extracted from asymmetries in angular distributions measured with respect to defined spin quantization axes. The asymmetry measurements are performed at parton level by correcting the observed angular distributions for detector effects and hadronization after subtracting the background contributions. The measured W-boson polarization values are in agreement with the Standard Model predictions. The results are used to set limits on the imaginary part of the anomalous coupling gR in a model-independent way. The second analysis presents a search for events with one top quark and large missing transverse energy at the final state. The dataset corresponds to proton-proton collision data at a center-of-mass energy of 13 TeV collected by the ATLAS detector during 2015 and 2016, corresponding to an integrated luminosity of 36.1/fb. The channel with leptonic decay of the W boson from the top quark is covered in this thesis. Selected events contain one isolated lepton (electron or muon), large missing transverse momentum and exactly one jet, identified as likely to contain a b-hadron. In addition, further selection requirements are applied to discriminate signal events from background. The obtained results are combined with the analysis of the hadronic W boson decay and interpreted in the context of generic models for Dark Matter production. In the absence of evidence for any of these signals, 95% confidence level upper limits on the corresponding production cross-sections are obtained and these limits are translated into constraints on the parameter space of the considered models. In addition to the two mentioned analyses, some contributions to the alignment of the ATLAS inner detector are presented in this manuscript. ############################## Aquesta tesi presenta dues analisis dedicades a la recerca de nova fisica en esdeveniments de quarks top unics amb dades enregistrades pel detector ATLAS al LHC. L'objectiu de la primera analisi es investigar l'estructura del vertex Wtb a traves de la mesura d'observables de polaritzacio del boso W utilitzant esdeveniments de quarks top unics produits al canal t. El conjunt de dades correspon a esdeveniments de col·lisio de proto-proto amb una energia de centre de masses de 8 TeV amb una lluminositat total integrada de 20.2 / fb. Els esdeveniments seleccionats contenen un lepto aillat (electro o muo), gran moment transvers mancant i exactament dos dolls, amb un d'ells identificat com probable que contingui un b-hadro. Requisits de seleccio addicionals s'apliquen per a separar la senyal del fons. Els observables de polaritzacio del boso W s'extreuen de les asimetries en distribucions angulars mesurades respecte als eixos de quantificacio de l'espin. Les mesures d'asimetria es realitzen a nivell de parto corregint les distribucions angulars observades pels efectes del detector i l'hadronitzacio despres de restar les contribucions de fons. Els valors de polaritzacio del boso W mesurats coincideixen amb les prediccions del Model Estandard. Els resultats s'utilitzen per establir limits a la part imaginaria de l'acoblament anomal gR d'una manera independent del model. La segona analisi presenta una cerca d'esdeveniments amb un quark top i una gran energia transversa mancant a l'estat final. El conjunt de dades correspon a dades de col·lisio de proto-proto amb una energia de centre de masses de 13 TeV recollida pel detector ATLAS durant 2015 i 2016, corresponent a una lluminositat integrada de 36.1 / fb. El canal amb desintegracio leptonica del boso W provinent del quark top esta cobert en aquesta tesi. Els esdeveniments seleccionats contenen un lepto aillat (electro o muo), gran moment transvers mancant i exactament un doll, identificat com a probable que contingui un b-hadro. A mes, s'apliquen nous requisits de seleccio per discriminar esdeveniments de senyal front al fons. Els resultats obtinguts es combinen amb l'analisi de la desintegracio hadronica del boso W i s'interpreten en el context de models generics per a la produccio de materia fosca. A falta d'evidencia d'aquestes senyals, s'obtenen limits superiorsal nivell de confianca del 95% en les seccions eficaces de produccio corresponents i aquests limits es tradueixen en restriccions sobre l'espai de parametres dels models considerats. A mes de les dues analisis esmentades, aquest manuscrit presenta algunes contribucions a l'alineament del detector intern ATLAS. ############################## Esta tesis presenta dos analisis dedicados a la busqueda de nueva fisica en eventos de quark top unicos con datos recopilados por el detector ATLAS del LHC. El objetivo del primer analisis es investigar la estructura del vertice Wtb a traves de la medida de observables de polarizacion del boson W utilizando eventos de quarks top unicos producidos en el canal t. El conjunto de datos corresponde a eventos de colision de proton-proton con una energia de centro de masas de 8 TeV con una luminosidad total integrada de 20.2 / fb. Los eventos seleccionados contienen un lepton aislado (electron o muon), gran momento transverso faltante y exactamente dos jets, con uno de ellos identificado como probable que contenga un b-hadron. Requisitos de seleccion adicionales se aplican para separar la senal del fondo. Los observables de polarizacion del boson W se extraen de las asimetrias en distribuciones angulares medidas respecto a los ejes de cuantificacion del spin. Las medidas de asimetria se realizan a nivel de partones corrigiendo las distribuciones angulares observadas por efectos del detector y de hadronizacion tras restar las contribuciones de fondo. Los valores de polarizacion del boson W medidos coinciden con las predicciones del Modelo Estandar. Los resultados se utilizan para establecer limites a la parte imaginaria del acoplamiento anomalo gR de una manera independiente del modelo. El segundo analisis presenta una busqueda de eventos con un quark top y una gran energia transversa faltante en el estado final. El conjunto de datos corresponde a datos de colision de proton-proton a una energia de centro de masas de 13 TeV recogida por el detector ATLAS durante 2015 y 2016, correspondiente a una luminosidad integrada de 36.1 / fb. El canal con desintegracion leptonica del boson W del quark top esta cubierto en esta tesis. Los acontecimientos seleccionados contienen un lepton aislado (electron o muon), gran momento transverso faltante y exactamente un jet, identificado como probable que contenga un b-hadron. Ademas, se aplican requisitos adicionales de seleccion para discriminar eventos de senal frente al fondo. Los resultados obtenidos se combinan con el analisis de la desintegracion hadronica del boson W y se interpretan en el contexto de modelos genericos para la produccion de materia oscura. A falta de evidencia de alguna de estas senales, se obtienen limites superiores al nivel de confianza del 95% en las secciones eficaces de produccion correspondientes y estos limites se traducen en restricciones sobre el espacio de parametros de los modelos considerados. Ademas de los dos analisis mencionados, este manuscrito presenta algunas contribuciones al alineamiento del detector interno ATLAS."", ""This is the third out of five chapters of the final report [1] of the Workshop on Physics at HL-LHC, and perspectives on HE-LHC [2]. It is devoted to the study of the potential, in the search for Beyond the Standard Model (BSM) physics, of the High Luminosity (HL) phase of the LHC, defined as $3~\\mathrm{ab}^{-1}$ of data taken at a centre-of-mass energy of $14~\\mathrm{TeV}$, and of a possible future upgrade, the High Energy (HE) LHC, defined as $15~\\mathrm{ab}^{-1}$ of data at a centre-of-mass energy of $27~\\mathrm{TeV}$. We consider a large variety of new physics models, both in a simplified model fashion and in a more model-dependent one. A long list of contributions from the theory and experimental (ATLAS, CMS, LHCb) communities have been collected and merged together to give a complete, wide, and consistent view of future prospects for BSM physics at the considered colliders. On top of the usual standard candles, such as supersymmetric simplified models and resonances, considered for the evaluation of future collider potentials, this report contains results on dark matter and dark sectors, long lived particles, leptoquarks, sterile neutrinos, axion-like particles, heavy scalars, vector-like quarks, and more. Particular attention is placed, especially in the study of the HL-LHC prospects, to the detector upgrades, the assessment of the future systematic uncertainties, and new experimental techniques. The general conclusion is that the HL-LHC, on top of allowing to extend the present LHC mass and coupling reach by $20-50\\%$ on most new physics scenarios, will also be able to constrain, and potentially discover, new physics that is presently unconstrained. Moreover, compared to the HL-LHC, the reach in most observables will generally more than double at the HE-LHC, which may represent a good candidate future facility for a final test of TeV-scale new physics."", ""We study unconventional decays of the top-quark and the top-squark in the framework of SUSY models with broken R-parity. The model under study is the MSSM with an additional bilinear term that breaks R-parity. In this model the top-squark behaves similar to a third generation leptoquark. We demonstrate that existing Tevatron data on the top give rise to restrictions on the SUSY parameter space. In particular, we focus on scenarios where the tau-neutrino mass is smaller than 1 eV. We give an exclusion plot derived from the leptoquark searches at Tevatron.""]"
"Axel Brandenburg","","","Computer Science","https://openalex.org/A5023835521","[""Abstract The question of whether a dynamo can be triggered by gravitational collapse is of great interest, especially for the early Universe. Here, we employ supercomoving coordinates to study the magnetic field amplification from decaying turbulence during gravitational collapse. We perform 3D simulations and show that for large magnetic Reynolds numbers, there can be exponential growth of the comoving magnetic field with conformal time before the decay of turbulence impedes further amplification. The collapse dynamics only affect the nonlinear feedback from the Lorentz force, which diminishes more rapidly for shorter collapse times, allowing nearly kinematic continued growth. We confirm that helical turbulence is more efficient in driving dynamo action than nonhelical turbulence, but this difference decreases for larger collapse times. We also show that for nearly irrotational flows, dynamo amplification is still possible, but it is always associated with a growth of vorticity—even if it still remains very small. In nonmagnetic runs, the growth of vorticity is associated with viscosity and grows with the Mach number. In the presence of magnetic fields, vorticity emerges from the curl of the Lorentz force. During a limited time interval, an exponential growth of the comoving magnetic field with conformal time is interpreted as clear evidence of dynamo action."", ""Powerful lasers may be used in the future to produce magnetic fields that would allow us to study turbulent magnetohydrodynamic inverse cascade behaviour. This has so far only been seen in numerical simulations. In the laboratory, however, the produced fields may be highly anisotropic. Here, we present corresponding simulations to show that, during the turbulent decay, such a magnetic field undergoes spontaneous isotropisation. As a consequence, we find the decay dynamics to be similar to that in isotropic turbulence. We also find that an initially pointwise non-helical magnetic field is unstable and develops magnetic helicity fluctuations that can be quantified by the Hosking integral. It is a conserved quantity that characterises magnetic helicity fluctuations and governs the turbulent decay when the mean magnetic helicity vanishes. As in earlier work, the ratio of the magnetic decay time to the Alfvén time is found to be approximately $50$ in the helical and non-helical cases. At intermediate times, the ratio can even reach a hundred. This ratio determines the endpoints of cosmological magnetic field evolution."", ""Abstract Kinetic helicity is a fundamental characteristic of astrophysical turbulent flows. It is not only responsible for the generation of large-scale magnetic fields in the Sun, stars, and spiral galaxies, but it also affects turbulent diffusion, resulting in the dissipation of large-scale magnetic fields. Using the path integral approach for random helical velocity fields with a finite correlation time and large Reynolds numbers, we show that turbulent magnetic diffusion is reduced by the kinetic helicity, while the turbulent diffusivity of a passive scalar is enhanced by the helicity. The latter can explain the results of recent numerical simulations for forced helical turbulence. One of the crucial reasons for the difference between the kinetic helicity effect on magnetic and scalar fields is related to the helicity dependence of the correlation time of a turbulent velocity field."", ""Abstract We numerically study axion-U(1) inflation, focusing on the regime where the coupling between axions and gauge fields results in significant backreaction from the amplified gauge fields during inflation. These amplified gauge fields not only generate high-frequency gravitational waves (GWs), but also enhance spatial inhomogeneities in the axion field. GWs serve as key probe for constraining the coupling strength between the axion and gauge fields. We find that, when backreaction is important during inflation, the constraints on the coupling strength due to GW overproduction are relaxed compared to previous studies, in which backreaction matters only after inflation. Moreover, our results suggest that the probability density function (PDF) of axion fluctuations tends toward a Gaussian distribution even in cases where gauge field backreaction is important only after inflation. This aligns with previous studies where the same effect was observed for cases with strong backreaction during inflation. This finding can be crucial for future studies of primordial black hole (PBH) formation, which can further constrain the coupling strength. We also calculate the spectrum of the produced magnetic fields in this model and find that their strength is compatible with the observed lower limits.""]"
"Y. V. Khotyaintsev","","","Computer Science","https://openalex.org/A5053872042","[""Abstract Collisionless shocks can exhibit non‐stationary behavior even under steady upstream conditions, forming a complex transition region. Ion phase‐space holes, linked to shock self‐reformation and surface ripples, are a signature of this non‐stationarity. We statistically analyze their occurrence using 521 crossings of Earth's quasi‐perpendicular bow shock. Phase‐space holes appear in 65% of cases, though the actual rate may be higher as the holes may not be resolved during fast shock crossings. The occurrence rate peaks at 70% for shocks with Alfvén Mach numbers . These findings suggest that Earth's quasi‐perpendicular bow shock is predominantly non‐stationary."", ""Magnetospheric-ionospheric coupling studies often rely on multi-spacecraft conjunctions, which require accurate magnetic field mapping tools. For example, linking measurements from the magnetotail with those in the ionosphere involves determining when the orbital magnetic footpoint of THEMIS or MMS intersects with the footpoint of Swarm. The Tsyganenko models are commonly used for tracing magnetic field lines. In this study, we aim to analyze how the footpoint locations are impacted by the inputs parameters of these models, including solar wind conditions, geomagnetic activity, and the location in the magnetotail. A dataset of 2394 bursty bulk flows (BBFs) detected by MMS was mapped to Earth's ionosphere with six different Tsyganenko models. Approximately 90% of the ionospheric footpoints are concentrated within 70° +/- 5° magnetic latitude (MLAT) and +/- 3 hours of magnetic local time (MLT) around midnight, with a pronounced peak in the pre-midnight sector. The MLT position showed a difference of approximately +/- 1 hour MLT across the models. Footpoint locations were linked to the dawn-dusk position of the BBFs, with differences between models associated with variations in the interplanetary magnetic field clock angle. The MLAT values exhibited similar differences of approximately +/- 4° around the mean value, with a systematic shift toward lower latitudes in the T89 model. This position is also influenced by the input parameters of the model representing the dynamics of Earth's magnetosphere, where stronger magnetospheric activity typically corresponds to lower latitudes. The uncertainty on the BBF footpoint location impacts the number of conjunctions with Swarm. Generally, Swarm B exhibited more conjunctions than Swarm A or C in the northern hemisphere. However, when considering only Swarm-BBF conjunctions where the distance between footpoints computed with T89 and TA15n is smaller than the BBF footprint size, the number of conjunctions is reduced to less than half of the total."", ""Abstract The evolution of the properties of short‐scale electrostatic waves across collisionless shocks remains an open question. We use a method based on the interferometry of the electric field measured aboard the magnetospheric multiscale spacecraft to analyze the evolution of the properties of electrostatic waves across four quasi‐perpendicular shocks, with and . Most of the analyzed wave bursts across all four shocks have a frequency in the plasma frame lower than the ion plasma frequency and a wavelength on the order of 20 Debye lengths . Their direction of propagation is predominantly field‐aligned upstream and downstream of the bow shock, while it is highly oblique within the shock transition region, which might indicate a shift in their generation mechanism. The similarity in wave properties between the analyzed shocks, despite their different shock parameters, indicates the fundamental nature of electrostatic waves for the dynamics of collisionless shocks."", ""Abstract Dipolarization events with inductive, radial electric fields are investigated with multi‐spacecraft analysis techniques. Observations by Magnetospheric Multiscale with separations around ion scales are used to study spatial and temporal variations of these events in the inner magnetosphere. force, magnetic pressure force, and tension force are compared based on the Taylor expansion method, which includes the curlometer technique. The magnetic pressure force, possibly related to energetic particles and magnetic flux transported from the magnetotail, tends to contribute to the force for the background components near the equator, while the tension force, related to Alfvén waves, contributes to the fluctuating components or outside the equator. The scale length is thousands of km for the background components, probably related to meso‐scale structures, while that length is tens to a thousand km for fluctuating components. The small‐scale fluctuations would be related to particle acceleration. The fluctuations are inferred to be Alfvén waves with quasi‐perpendicular propagation directions and with finite temperature or kinetic effects. These fluctuations could be intermittent and not self‐similar between different inter‐spacecraft distances. Some energy transfer from fluctuations to particles would occur.""]"
"Jeff Hearn","","","Computer Science","https://openalex.org/A5049950427","[""In the world we live in today, the presence and claims of crisis abound – from climate change, financial and political crisis to depression, livelihoods and personal security crisis. There is a challenge to studying crisis due to the ways in which crisis as a notion, condition and experience refers to and operates at various societal levels. Further, different kinds of crisis can overlap and intersect with each other, and act as precursors or consequences of other crises, in what can be thought of as inter-crisis relations or chains of crises. This article makes an enquiry into how to develop more adequate analytical tools for understanding crisis as a multidimensional phenomenon. We ask how crisis can be conceptualised and what the analytical potentials of a distinct crisis perspective might be? In this article we suggest a multi- and interdisciplinary approach to bridge between traditionally separated realms. Our ambition is to present a case for the development of Interdisciplinary Crisis Studies as a field of scholarly enquiry, which allows for new perspectives on data collection and analysis. Using the cases of, first, crisis and security and, second, crisis and climate, conflict and migration, we illustrate how studying and intervening in crises requires non-linear approaches which connect across disciplines to develop more comprehensive, interdisciplinary understandings of societal problems and better solutions. In concluding the paper, we assert that key features of Interdisciplinary Crisis Studies must include (1) temporality, spatiality and scale; (2) multi-layeredness, processuality and contradictions; and (3) gender, intersectionality and social inequalities."", ""The critique and conceptualisation of current policy and research on gender-based violence in higher education institutions (HEIs) and research performing organisations (RPOs) are matters of central importance. Building critically on recent European research and policy experience, and conceptual reflections arising from a large European multi-country research and innovation project, three key steps in critique and reconceptualising of gender-based violence in HEIs and RPOs are explicated. These are: first, clarification of differential definitions of and inclusions in gender-based violence in HEIs and RPOs; second, drawing on the recent European UniSAFE project survey and analysis of 42,000 university staff and student respondents in 46 institutions within 15 countries, differential contextualisations of prevalence and consequences, especially the need for multi-level and intersectional analysis of prevalence and consequences; and, third, engagement with ongoing theoretical and practical contestations in conceptualisation. The article concludes with discussion of further key issues for research and policy. These include how gender and gender-based violence are understood across national, organisational contexts, and the need for more focus on perpetrators, and the organisational relations between perpetrators and victims. In working towards violence-free and safe HEIs and RPOs, the connections between violence and organisational structures, processes and dynamics must be confronted proactively."", ""The digital age requires people of all ages to communicate and organise their lives through digital technologies. The project EQualCare (“Alone but connected? Digital (in)equalities in care work and generational relationships among older people living alone”) investigated how the growing population of older people living alone is man-aging this transition, how it shapes their (non-)digital social networks and what changes on local, regional, national and international levels need to be brought about to ensure (digital) equality. This white paper gives insight into the multi-method work that was done, summarises key findings, and provides recommendations for policy and practice. EQualCare was a cross-cultural comparison and collaboration across Finland, Ger-many, Latvia and Sweden, with Finland and Sweden as two countries advanced in the digitalisation of civic and private life and thus providing a helpful contrast to Germany and Latvia that are at different levels of digitalisation. Their joint work comprised of four parts: • To begin with all four national researcher teams conducted respective critical document analyses of social policy documents and legislation, examining how ageing, living alone, digitalisation, and care-responsibilities are portrayed in the national policy documents. • Following, an analyses of existing national and EU data sets on ageing took place to draw comparative information on living conditions, income, health, use of dig-ital devices, and care work across the four countries. • The central part of EQualCare entailed a participatory action research (PAR) project that was conducted across the four countries and involved older people as co-researchers in nine local project teams. • The model of EQualCare was a participatory policy making one, whereby the work of one of the PAR projects was connected with the others, with the findings from the policy analyses and statistical analyses providing the backdrop and scaf-folding to develop the recommendations. The project team was strongly multidisciplinary; bring together experienced re-searchers in anthropology, business organisation and management studies, cultural studies, education, gender studies, psychology, social psychology, social work and sociology.""]"
"Magnus Karlsson","","","Computer Science","https://openalex.org/A5052801826","[""The study utilizes data of naturally occurring peer interactions in a Swedish preschool, to explore how dyadic affiliations (and exclusions) are created, played, out and transformed in small interacting groups of three children (aged 5) during free time play. The overall results point to the complex and shifting nature of children's (dyadic) relationships as children move in and out of dyads, which means that a child is not always excluded. It is shown how dyads can be built around relationship buildings as well as the evaluating skills of others in a specific activity, a formation that can be strengthened when reacting against a third-party seeking access. However, the results also show how dyadic affiliations can be renegotiated during a rather short period of time; third parties can benefit from “relational cracks” and tensions in the dyad as they restructure and challenge (dyadic) relationships and relative positions toward each other. Finally, the analysis reveals how part of these relational negotiations is done nonverbally, where, for example, bodily positionings, adjustments, and movements in space contribute to the intricate interaction and changing social formations that takes place between the parties."", ""Socialt arbete har diskuterats i Socialmedicinsk tidskrift sedan dess den först publicerades. I föreliggande text visas hur ämnet diskuterats med olika frekvens under olika perioder, och särskilt tre perioder under tiden 1924-2006 utmärker sig i detta fall. Här identifieras och karaktäriseras dessa tre perioder och exempel ges på hur det sociala arbetet diskuterats i tidskriften under dem. De historiska exempel som lyfts fram illustrerar en period där socialt arbete diskuterades framför allt i relation till vårdinsatser, en där ämnet socialt arbete etablerades och en där välfärdens sociala arbete på olika sätt kom att granskas.""]"
"Nathalie Feiner","","","Computer Science","https://openalex.org/A5004935819","[""<ns3:p>We present a genome assembly from an individual female <ns3:italic>Podarcis liolepis</ns3:italic> (Catalonian Wall Lizard; Chordata; Lepidosauria; Squamata; Lacertidae). The assembly contains two haplotypes with total lengths of 1 574.00 megabases and 1 478.08 megabases. Most of haplotype 1 (97.05%) is scaffolded into 20 chromosomal pseudomolecules, including the W and Z sex chromosomes. Haplotype 2 was assembled to scaffold level. The mitochondrial genome has also been assembled, with a length of 17.32 kilobases.</ns3:p>"", ""<ns3:p>We present a genome assembly from an individual female <ns3:italic>Podarcis tiliguerta</ns3:italic> (Tyrrhenian Wall Lizard; Chordata; Lepidosauria; Squamata; Lacertidae). The assembly contains two haplotypes with total lengths of 1 462.31 megabases and 1 394.94 megabases. Most of haplotype 1 (99.26%) is scaffolded into 20 chromosomal pseudomolecules, including the W and Z sex chromosomes. Most of haplotype 2 (99.2%) is scaffolded into 18 chromosomal pseudomolecules. The mitochondrial genome has also been assembled, with a length of 17.19 kilobases.</ns3:p>"", ""The vertebrate skull originates from two embryonic lineages, the mesoderm and the neural crest, offering a unique framework to study how developmental mechanisms connect phenotypic variation and evolutionary diversification. Using 3D geometric morphometrics, we analysed skull shape variation in lacertid lizards. Mesoderm- and neural crest-derived bones formed distinct, conserved modules at both micro- and macroevolutionary scales. In the common wall lizard ( Podarcis muralis ), rapid evolution of skull shape under sexual selection was primarily driven by neural crest-derived bones. While the primary axis of shape divergence in P. muralis aligned with a major axis of variation across lacertids, neural crest-derived bones exhibited slower evolutionary rates and lower morphological disparity than mesodermal-derived bones. We propose that this discrepancy between the role of the neural crest for skull evolution on micro- and macroevolution reflects constraints imposed by neural crest cell biology: although developmental plasticity enables rapid, correlated responses under sexual selection, pleiotropy may limit long-term evolvability of neural crest-derived skull regions."", ""Neural crest cells (NCCs) are a key component of the vertebrate body plan and contribute to a variety of different traits. However, their dynamic migratory behavior and spatiotemporal heterogeneity in the developing embryo pose significant challenges for their identification and isolation. Consequently, most studies of NCCs have been confined to model organisms with established transgenic tools. To overcome this limitation, we present a novel approach that combines antibody labelling with fluorescence activated cell sorting to enrich for NCCs and we demonstrate the approach in the common wall lizard ( Podarcis muralis ). Through microscopy, reverse transcription quantitative polymerase chain reaction and single-cell RNA sequencing, we show that the method enriches for NCCs as efficiently as methods relying on transgenic animals. Using this technique, we successfully characterise transcriptional profiles of NCCs in wall lizard embryos. We anticipate that this method can be applied to a wide range of vertebrates that lack transgenic tools, enabling deeper insights into the roles that neural crest cells are playing in developmental and evolution."", ""<ns5:p>We present a genome assembly from a female specimen of <ns5:italic>Podarcis vaucheri</ns5:italic> (Andalusian wall lizard; Chordata; Lepidosauria; Squamata; Lacertidae). The assembly contains two haplotypes with total lengths of 1,614.91 megabases and 1,510.74 megabases. Most of haplotype 1 (98.26%) is scaffolded into 20 chromosomal pseudomolecules, including the W and Z sex chromosomes. Most of haplotype 2 (98.63%) is scaffolded into 18 chromosomal pseudomolecules. The mitochondrial genome has also been assembled, with a length of 17.24 kilobases.</ns5:p>""]"
"Susanna C. Larsson","","","Computer Science","https://openalex.org/A5090235061","[""Abstract Sleep problems and inadequate physical activity (PA) are associated with numerous adverse health outcomes. Most studies explored the influence of PA on sleep, but how sleep affects engagement in PA across adulthood remains less well investigated. This study examined the association between sleep traits and PA reported the following year in 51,247 Swedish women and men (55–93 years), who completed questionnaires regarding their sleep, PA, and other characteristics. Health status was assessed with data from the Swedish National Patient Registry. Odds ratios and 95% confidence intervals were estimated by binary logistic regression. In multivariable analysis, long sleep duration (≥ 9 h/night), sleep disturbance, and symptoms of sleep-disordered breathing (SDB) were associated with lower odds of engaging in walking or cycling, and exercising the following year. In addition, short sleep duration (&lt; 7 h/night), sleep disturbance, and symptoms of SDB were linked to sedentary behavior (time spent on reading or watching TV). Several sex- and age-specific associations were observed. In addition to previous evidence of a positive effect of PA on sleep, our findings indicate that poor sleep may contribute to lower engagement in PA and a higher level of sedentary behavior, highlighting the bidirectional nature of the relationship between PA and sleep."", ""Peripheral arterial disease (PAD) is a major vascular complication associated with significant morbidity and mortality. While traditional cardiovascular risk factors such as smoking, hypertension, and diabetes are well established, emerging evidence suggests that alcohol consumption, alcoholic liver disease, and metabolic-associated steatotic liver disease may also contribute to PAD risk. This review synthesizes current epidemiological evidence linking alcohol intake, alcoholic liver disease, and metabolic-associated steatotic liver disease to PAD and explores potential mechanisms, including atherosclerosis, endothelial dysfunction, chronic inflammation, dyslipidemia, and coagulation abnormalities. Observational studies suggest a possible protective effect of light-to-moderate alcohol consumption though genetic studies challenge this notion. In addition, alcoholic liver disease and metabolic-associated steatotic liver disease are increasingly recognized as contributors to systemic vascular dysfunction and PAD progression. In conclusion, given the rising burden of liver disease, it is crucial to determine whether PAD screening is warranted in patients with high-risk alcoholic liver disease and metabolic-associated steatotic liver disease. Addressing modifiable risk factors and optimizing pharmacological interventions may help mitigate PAD risk. Future research should focus on longitudinal studies, sex- and ethnicity-specific differences, and omics-based approaches to refine risk prediction, early detection, and targeted interventions."", ""Alcohol is one of the most commonly consumed substances in the world, exhibiting complex relationships with multiple aspects of cardiovascular health and disease. The majority of the research on the topic is observational and therefore prone to bias and confounding. The available evidence suggests no risk to possible risk reduction when alcohol is consumed in low amounts (such as no more than 1 to 2 drinks a day) in regard to coronary artery disease, stroke, sudden death, and possibly heart failure. The risk associated with consuming 1 to 2 drinks a day on atrial fibrillation remains unknown. More randomized trials of low to moderate alcohol consumption are needed for more definitive conclusions. In stark contrast, heavier alcohol consumption such as binge drinking or consuming on average ≥3 drinks/d is consistently associated with worse outcomes in every cardiovascular disease entity studied. Considering the level of evidence, it remains unknown whether drinking is part of a healthy lifestyle and therefore clinicians should reinforce healthy lifestyle behaviors such as regularly engaging in physical activity, avoiding tobacco use, and maintaining healthy body weight."", ""Background Alcohol is a known carcinogen, yet the evidence for an association with pancreatic cancer risk is considered as limited or inconclusive by international expert panels. We examined the association between alcohol intake and pancreatic cancer risk in a large consortium of prospective studies. Methods and findings Population-based individual-level data was pooled from 30 cohorts across four continents, including Asia, Australia, Europe, and North America. A total of 2,494,432 participants without cancer at baseline (62% women, 84% European ancestries, 70% alcohol drinkers [alcohol intake ≥ 0.1 g/day], 47% never smokers) were recruited between 1980 and 2013 at the median age of 57 years and 10,067 incident pancreatic cancer cases were recorded. In age- and sex-stratified Cox proportional hazards models adjusted for smoking history, diabetes status, body mass index, height, education, race and ethnicity, and physical activity, pancreatic cancer hazard ratios (HR) and 95% confidence intervals (CI) were estimated for categories of alcohol intake and in continuous for a 10 g/day increase. Potential heterogeneity by sex, smoking status, geographic regions, and type of alcoholic beverage was investigated. Alcohol intake was positively associated with pancreatic cancer risk, with HR 30-to-&lt;60 g/day and HR ≥60 g/day equal to 1.12 (95% CI [1.03,1.21]) and 1.32 (95% CI [1.18,1.47]), respectively, compared to intake of 0.1 to &lt;5 g/day. A 10 g/day increment of alcohol intake was associated with a 3% increased pancreatic cancer risk overall (HR: 1.03; 95% CI [1.02,1.04]; p value &lt; 0.001) and among never smokers (HR: 1.03; 95% CI [1.01,1.06]; p value = 0.006), with no evidence of heterogeneity by sex ( p heterogeneity = 0.274) or smoking status ( p heterogeneity = 0.624). Associations were consistent in Europe–Australia (HR 10 g/day = 1.03, 95% CI [1.00,1.05]; p value = 0.042) and North America (HR 10 g/day = 1.03, 95% CI [1.02,1.05]; p value &lt; 0.001), while no association was observed in cohorts from Asia (HR 10 g/day = 1.00, 95% CI [0.96,1.03]; p value = 0.800; p heterogeneity = 0.003). Positive associations with pancreatic cancer risk were found for alcohol intake from beer (HR 10 g/day = 1.02, 95% CI [1.00,1.04]; p value = 0.015) and spirits/liquor (HR 10 g/day = 1.04, 95% CI [1.03,1.06]; p value &lt; 0.001), but not wine (HR 10 g/day = 1.00, 95% CI [0.98,1.03]; p value = 0.827). The differential associations across geographic regions and types of alcoholic beverages might reflect differences in drinking habits and deserve more investigations. Conclusions Findings from this large-scale pooled analysis support a modest positive association between alcohol intake and pancreatic cancer risk, irrespective of sex and smoking status. Associations were particularly evident for baseline alcohol intake of at least 15 g/day in women and 30 g/day in men.""]"
"Feng Gao","","","Computer Science","https://openalex.org/A5100729278","[""A mechanistic understanding of how molecular structure governs photoluminescence quantum yield (PLQY) in non-fullerene acceptors (NFAs) remains elusive, hindering further progress in organic solar cells. Here, we report a comprehensive study of nearly 100 organic semiconductors—primarily NFAs—with emission peaks spanning 550-1000 nm and PLQY values ranging from &lt; 0.01 to &gt; 0.70. We find that the commonly used structural, photophysical, and quantum descriptors fail to account for the observed variations. Instead, we identify the twisted intramolecular charge transfer (TICT) state as the primary quenching pathway. Through a combination of experimental spectroscopy, quantum calculations, and exciton decay dynamics, we show that TICT acts as a dark state on the first excited-state potential energy surface, leading to suppressed PLQY and multi-exponential decay. Crucially, suppressing TICT formation enables substantial enhancements in emission intensity. These results reveal a unifying mechanism underlying luminescence in organic semiconductors, provide a predictive framework for exciton dynamics, and open new avenues for NFA-based technologies beyond photovoltaics. Crucially, this is the first structurally resolved new species yet identified in NFA samples."", ""Correction: Characterization of two proline-rich proteins involved in silicon deposition in Cucummis sativus"", ""Perovskite materials have revolutionized optoelectronics by virtue of their tunable bandgaps, exceptional optoelectronic properties, and structural flexibility. Notably, the state-of-the-art performance of perovskite solar cells has reached 27%, making perovskite materials a promising candidate for next-generation photovoltaic technology. Although numerous reviews regarding perovskite materials have been published, the existing reviews generally focus on individual material systems (e.g., organic-inorganic hybrid perovskites) and specific optimizations in one particular optoelectronic application (e.g., stability engineering for solar cells), lacking a systematic overview of the progress and challenges across diverse perovskite types. This review breaks this limitation by providing a systematic overview of all perovskite categories used in solar cells classified by different criteria, including composition (organic-inorganic hybrid perovskites, all-inorganic perovskites, lead-free perovskites, and metal-free perovskites), dimensionality (3D and low-dimensional perovskitoids), and crystallinity (poly-crystal thin film and single-crystal perovskites). The recent progress and future perspectives for each category of perovskite solar cells are focused on, aiming to establish a holistic roadmap for perovskite solar cells toward technological innovations and industrial viability.""]"
"Pieter Jelle Visser","","","Computer Science","https://openalex.org/A5029875703","[""Background The Models of Patient Engagement for Alzheimer's Disease (MOPEAD) project aimed to identify the most effective and cost-efficient recruitment model for detecting prodromal and mild Alzheimer's disease (AD) across five European countries. Objective To examine differences in cardiovascular risk factors and cognitive performance among countries and recruitment models using MOPEAD data. Methods Individuals aged 65–85 with a high risk for prodromal or mild AD were included. Four recruitment models were used: a web-based screening tool, an open house initiative (OHI), a primary care-based protocol for early detection of cognitive decline, and a tertiary care-based screening at a diabetologist clinic. Participants from Germany, Spain, the Netherlands, Sweden, and Slovenia were recruited. Cardiovascular risk factors were self-reported, and cognition was assessed using The Repeatable Battery for the Assessment of Neuropsychological Status (RBANS). Results A total of 414 individuals (mean age 71.9, SD = 5.0) were included. Significant differences were observed in physical activity (p &lt; 0.001), with individuals from Sweden and Slovenia being the most active. Dutch participants scored highest on most cognitive measures. Individuals recruited via web-based survey were youngest, most active (61.7%), and had the lowest rates of diabetes (12.0%) and heart disease (6.4%), as well as the best cognitive scores. Those recruited via diabetologist clinics displayed the highest cardiovascular risk and the lowest cognitive performance. Conclusions This study unveils significant disparities in cardiovascular health and cognition across recruitment strategies and European countries. The OHI shows promise for future recruitment in the context of disease-modifying AD treatments."", ""The trajectories of core Alzheimer disease (AD) cerebrospinal fluid (CSF) biomarkers and the concurrent cognitive changes across the clinical spectrum remain unclear yet are important for clinical trial design. To map longitudinal CSF amyloid, tau, and cognitive trajectories along the clinical spectrum of AD and amyloid-negative controls. This longitudinal cohort study included participants with a minimum of 2 CSF samples from Alzheimer Centrum Amsterdam cohorts across the AD clinical spectrum (ie, abnormal amyloid levels at first visit, in different clinical stages) and cognitively normal controls with initially normal CSF markers from November 2003 to July 2019. The maximum follow-up period was 19.5 years (median [IQR], 2 [0-3] years). Data were analyzed from March 2024 to May 2025. AD biomarkers (ß-amyloid [Aß]1-42 to Aß1-40 ratio, total tau [t-tau], and phosphorylated tau [p-tau]) detected in serially collected CSF. CSF AD biomarkers were measured with Lumipulse G600II. Cognition was measured using the Mini-Mental State Examination (MMSE) and delayed memory recall component of the of the Rey Auditory Verbal Learning Test. Analysis was conducted using linear mixed models, including random intercepts and slopes, adjusting for age, education level, and sex. Each model included an interaction term of time and clinical stage to study stage-specific slopes. Biomarker conversion rates per clinical stage were studied by comparing biomarker status between visits. The sample included 197 individuals (103 male [52.3%]), including 83 controls (mean [SD] age, 63 [8] years), 31 individuals with amyloid positivity who were cognitively unimpaired (mean [SD] age, 67 [9] years), 30 individuals with amyloid-positive mild cognitive impairment (MCI; mean [SD] age, 67 [7] years), and 53 individuals with amyloid-positive dementia (mean [SD] age, 65 [8] years). Aβ1-42/1-40 ratios decreased in controls (β [SE] = -8.55 × 10-4 [1.87 × 10-4]; P < .001) and the amyloid-positive cognitively unimpaired group (β [SE] = -1.05 × 10-3 [3.14 × 10-4]; P < .001), and remained low in amyloid-positive MCI and dementia groups. There were 10 controls (12.0%) who reached abnormal amyloid over a mean (SD) of 4.8 (3.4) years. In controls, CSF t-tau (β [SE] = 8.49 [2.55] pg/mL per year; P = .002) and p-tau (β [SE] = 1.36 [0.41] pg/mL per year; P = .001) levels increased over time, and levels also increased for those in the amyloid-positive cognitively unimpaired (t-tau: β [SE] = 17.24 [4.58] pg/mL per year; P < .001; p-tau: β [SE] = 3.10 [0.72] pg/mL per year; P < .001) and amyloid-positive MCI (t-tau: β [SE] = 30.80 [5.99] pg/mL per year; P < .001; p-tau: β [SE] = 4.40 [0.93] pg/mL per year; P < .001) groups, with t-tau increasing further in the dementia group (β [SE] = 24.97 [7.80] pg/mL per year; P = .002). Longitudinal increases in p-tau and t-tau were steeper in the amyloid-positive cognitively unimpaired and MCI groups than in controls, with 10 controls (12.0%) reaching abnormal p-tau and 12 controls (14.5%) reaching abnormal t-tau levels. Delayed recall declined most in the amyloid-positive cognitively unimpaired group (β [SE] = -0.31 [0.07]; P < .001) and was associated with CSF amyloid levels (β [SE] = 102.29 [47.30]; P = .03). MMSE scores declined most in individuals with amyloid-positive MCI (β [SE] = -1.25 [0.12]; P < .001) and dementia (β [SE] = -1.89 [0.13]; P < .001). In this cohort study, CSF amyloid decreased toward abnormal levels in controls, declined further in the amyloid-positive cognitively unimpaired group , and was concurrent with decline of delayed recall; CSF amyloid stabilized in those with amyloid-positive MCI and dementia, while tau markers became increased (ie, more abnormal) in the amyloid-positive cognitively unimpaired and amyloid-positive MCI groups, suggesting that increase in CSF tau requires abnormal amyloid.""]"
"Nadhir Al‐Ansari","","","Computer Science","https://openalex.org/A5070829177","[""The satellite-derived climatic variables offer extensive spatial and temporal coverage for research; however, their inherent biases can subsequently reduce their accuracy for water balance estimate. This study evaluates the effectiveness of bias correction in improving the Tropical Rainfall Measuring Mission (TRMM) rainfall and the Global Land Data Assimilation System (GLDAS) land surface temperature (LST) data and illustrates their long-term (2000–2019) hydrological assessment. The novelty lies in coupling the bias-corrected climate variables with the Thornthwaite–Mather water balance model as well as land use land cover (LULC) for improved predictive hydrological modeling. Bias correction significantly improved the agreement with ground observations, enhancing the R2 value from 0.89 to 0.96 for temperature and from 0.73 to 0.80 for rainfall, making targeted inputs ready to predict hydrological dynamics. LULC mapping showed a predominance of agricultural land (64.5%) in the area followed by settlements (20.0%), forest (7.3%), barren land (6.5%), and water bodies (1.7%), with soils being silt loam, clay loam, and clay. With these improved datasets, the model found seasonal rise in potential evapotranspiration (PET), peaking at 120.7 mm in June, with actual evapotranspiration (AET) following a similar trend. The annual water balance showed a surplus of 523.8 mm and deficit of 121.2 mm, which proves that bias correction not only enhances the reliability of satellite data but also reinforces the credibility of hydrological indicators, with a direct, positive impact on evidence-based irrigation planning and flood mitigation and drought management, especially in data-scarce regions."", ""Abstract Conservation agriculture (CA) presents a promising substitute to the tillage-intensive rice–wheat cropping system (RWS) prevalent in the Indo-Gangetic plains (IGPs). In the northwestern IGPs, on-farm studies examining the impact of CA durations on soil properties and quality are limited. This study assessed the effects of CA practised for 2 (CA2), 4 (CA4), 8 (CA8), and 12 (CA12) years and conventional tillage (CT) on soil quality in the Nilokheri block of Haryana, India. The collected soil samples from 0–5 to 5–15 cm were analyzed for 22 different soil parameters, and a soil quality index (SQI) was developed using principal component analysis (PCA) for each scenario. The results showed that scenarios CA8 and CA12 had 9.8–10.7 and 11.1–11.3% lower bulk density, respectively, compared to CT. Mean weight diameter, saturated hydraulic conductivity, and water holding capacity were significantly higher in CA8 and CA12 over CT at both soil layers. Microbial biomass carbon and dehydrogenase activity increased by 32 and 42.7%, 14.9 and 32.3% in CA8 and CA12, respectively, over CT in the surface soil. Most of the chemical parameters were significantly influenced by CA, except for pH, electrical conductivity, and available Cu. Key soil quality indicators identified through PCA included Ks, WHC, β-glucosidase activity, dehydrogenase activity, available S, available Fe, and available Cu. The highest SQI was observed in CA12, followed by CA8 and CA4, and the lowest in CT at both depths. The derived regression coefficients revealed a strong positive relationship between SQI and both rice equivalent yield and wheat yield. This finding highlights the potential of enhancing soil quality to boost agricultural productivity under CA, thereby fostering sustainable farming. Such improvements are vital for building climate-resilient cropping and supporting the widespread adoption of CA practices. Therefore, it may be concluded that adopting CA for more than 8 years could help restore soil health and sustain productivity in the rice–wheat cropping system of northwest IGPs."", ""<title>Abstract</title> Climate change has assured the critical role of small dams in water resource management by enhancing local water storage, flood control, and irrigation, thereby improving resilience to climate variability. This study evaluates the effectiveness of one-arc-second digital elevation models (DEMs) in determining reservoir volume-elevation data, comparing these satellite-derived data with field survey data, which are known to be costly and time-consuming. Focused on small dams, the research compares field survey data with data derived from the one-arc-second DEM to address the lack of validation for these studies in small reservoirs. The methodology involves analyzing ten reservoirs in two different locations, namely Erbil and Sulaymaniyah governorates in Northern Iraq, using ArcGIS and Remote Sensing for digital elevation model processing. The volume-elevation data for the reservoirs are determined using ArcGIS. Consequently, evaluations of terrain metrics and sensitivity analyses support the adoption of one-arc-second DEMs for determining volume-elevation data in early-stage dam planning and reservoir assessments. Key innovations include addressing the limitations of low-resolution DEMs by using a wider 5 km radius for terrain analysis, which enables a thorough evaluation of landform features. The Terrain Ruggedness Index (TRI) is introduced as a key metric for evaluating terrain complexity, regional variations, and sensitivity to absolute error percentages. Morris’s sensitivity analysis highlights TRI's significance as the decisive parameter by examining how different terrain parameters affect error rates. These advancements enhance the accuracy of reservoir and terrain evaluations, offering valuable insights for improved dam design and water resource management."", ""Abstract The monitoring of lotic ecosystems is an important issue. This study investigated the total petroleum hydrocarbons (THP) in the Tigris River within Baghdad City, Iraq, which is considered the ultimate water supply source of the city. The study included measurement of THP concentrations, distribution, and origins of total petroleum hydrocarbons (TPHs) in various matrices (water, sediment, and macrophyte) in Tigris River within Baghdad City, in addition to some environmental factors during two seasons (dry and wet) for October 2020 to April 2021. The sampling was collected from three sites along the river. Thirteen compounds were identified in the current investigation from total petroleum hydrocarbons (TPHs), including hexatriacontane, tetracontane, tetratetracontane, undecane, dodecane, hexane, nonane, tetradecane, hexadecane, eicosane, dortiacontane, decane, and octadecane. TPHs concentrations were arranged in the following order macrophyte &gt; sediment &gt; water, and the oil refinery (site 2) was the most hydrocarbon contamination due to its anthropogenic activity. The highest results varied: 54.8 mg/L for hexatriacontane in water, 258.8 mg/g for hexadecane and nonane in sediment, while macrophyte was 393 mg/g for hexadecane. The origin of TPHs in different matrices in the current investigation was pyrogenic (Anthropogenic) according to the portion of low molecular weight/high molecular weight (LMW/HMW). The oil refinery site is considered a risk site by the increasing concentration of TPHs. The THPs pollution is beside other environmental problems in Iraq need to find a quick solution to save the river."", ""Real-time monitoring of canopy chlorophyll content is crucial for understanding crop growth and guiding precision agricultural management. The SPAD chlorophyll meter is a valuable tool for assessing nitrogen status in maize (Zea mays L.), a key cereal crop used for food, feed, and biofuels. Efficient nitrogen management is essential to maximize maize yield, particularly under varying water regimes. A study conducted over two years (2020-2021) utilized a strip plot design to investigate the spatiotemporal dynamics of SPAD readings and their correlation with maize yield under rainfed (M1) and irrigated (M2) conditions. Eight precision nitrogen management practices were implemented, including SPAD at sufficiency index and Green Seeker at response index, achieving ranges of 86-100% and 1.11-1.41, respectively. The findings revealed that irrigated maize produced significantly higher grain yields (6347 kg ha-1) compared to rainfed maize (5262 kg ha-1). The highest yield (9508.2 kg ha-1) was achieved when nitrogen was applied at a sufficiency index of 96-100%. The correlation between SPAD values and grain yield was strongest at reproductive stages (VT and R4), with R² values of 0.99 and 0.98 under rainfed conditions. In irrigated conditions, R² values ranged from 0.95 to 0.96 for earlier growth stages (V10, V12, VT, and R4). Multivariate analysis indicated critical management stages for optimizing yields in both conditions. Overall, SPAD-based nitrogen management strategies have the potential to enhance maize yields and resource efficiency while informing the development of sophisticated monitoring tools for real-time crop management.""]"
"Jari Tiihonen","","","Computer Science","https://openalex.org/A5072366449","[""ABSTRACT Introduction Despite well‐known diagnostic and neurobiological overlaps between psychopathic traits and schizophrenia, it has remained unclear whether psychopathic traits increase the risk for later schizophrenia. Former studies have proven only a weak correlation between psychopathy and DSM axis I diagnoses. Methods We combined data from individuals who underwent forensic psychiatric evaluations (FPEs) at Niuvanniemi Hospital between 1984 and 1993 with the records from the Care Register for Health Care to examine the relationship between psychopathic traits, measured by the Psychopathy Checklist‐Revised (PCL‐R), and the development of schizophrenia following the evaluation. We conducted survival analyses using Kaplan–Meier estimates and Cox proportional hazards models, with a follow‐up period of up to 40 years. Mortality data were obtained from the National Death Registry. Statistical analyses were adjusted for age, sex, criminal responsibility, and substance abuse disorder at the time of the FPE. Results The study included 341 individuals (278 males [81.51%] and 63 females [18.49%], mean [SD] age 33.52 [11.49]) who were adults, criminally responsible, and did not have a psychotic illness, severe mental disability, or brain damage at FPE. Compared to individuals with total PCL‐R scores less than or equal to 10, those with scores of 11–244 (adjusted hazard ratio [aHR] = 5.30, 95% CI = 1.21–23.25) and 25 or higher (aHR = 9.33, 95% CI = 2.04–42.76) had a significantly higher risk of later hospitalization due to schizophrenia. Also, individuals classified as psychopathic (PCL‐ R ≥ 25) had a significantly higher risk of developing schizophrenia compared with those classified as non‐psychopathic (PCL‐ R &lt; 25): aHR = 2.37, 95% CI =1.17–4.80. A total of 20% of psychopaths developed schizophrenia over the follow‐up. Conclusions The novel results suggest that there is a link between higher PCL‐R scores and a higher risk of later‐life schizophrenia outbreak among non‐psychotic individuals undergoing FPE. Multiple factors can explain the finding, including substance use and mutual risk factors."", ""Brexpiprazole is a second-generation antipsychotic with multiple indications, including the treatment of schizophrenia. As a partial dopamine agonist, brexpiprazole differs from most other antipsychotics, yet uncertainties about its full mechanism of action have led to some ambiguity among prescribers. To address this gap, an international panel of psychiatric experts was organized and convened with funding from Otsuka Pharmaceutical Europe Ltd and H. Lundbeck A/S to discuss the safe and effective use of brexpiprazole across different stages of schizophrenia treatment. Brexpiprazole's pharmacological profile-characterized by balanced binding affinities across norepinephrine, dopamine, and serotonin receptors-contributes to its efficacy in multiple symptom domains, including agitation and negative symptoms. Its tolerable safety profile, marked by minimal activation and minimal sedation and a relatively low risk of long-term cardiometabolic concerns, further supports its clinical utility. Brexpiprazole is a viable first-line therapy in both inpatient and outpatient settings when properly titrated and monitored. Brexpiprazole also serves as an option for different symptom domains in schizophrenia and for patients needing to switch antipsychotics due to inadequate symptom control or intolerable adverse events. Long-term maintenance therapy is essential for relapse prevention in schizophrenia, and when prescribed at appropriate doses for psychotic symptom control, brexpiprazole can provide sustained benefits with minimal long-term safety concerns. This report outlines the consensus panel's recommendations on the optimal initiation and administration of brexpiprazole in management of schizophrenia treatment, including a treatment algorithm, which is supported by a literature review."", ""ABSTRACT Background Individuals with bipolar disorder face an elevated risk of premature death, often due to external causes such as accidental injuries, self‐harm, and substance‐related deaths. This study aimed to investigate the incidence of severe poisonings among individuals with bipolar disorder and to examine associated demographic and clinical factors. Methods We conducted a cohort study using data from national registers in Finland, measuring hospitalizations and deaths due to poisoning by medicines or illegal substances in 1996–2018. Cox proportional hazards regression models were used to assess associations between predictor variables and poisoning outcomes. Results The study population comprised 60,045 individuals aged 15–65 diagnosed with bipolar disorder in 1987–2018. During the study period, 13.1% ( N = 7872) of the population experienced at least one poisoning resulting in hospitalization or death. The age‐standardized rate of hospitalizations was 50.6 (95% CI, 49.5–51.7) per 1000 person‐years and of deaths 1.8 (95% CI, 1.6–2.0) per 1000 person‐years. The majority of poisonings leading to hospitalization (59.1%) or death (56.6%) were intentional and caused by pharmaceuticals (hospitalizations, 76.9%; deaths, 63.6%). Additionally, psychoactive narcotics and stimulants were the cause of 26.8% of the poisoning deaths. The strongest risk factors for hospitalization were substance use disorder (adjusted hazard ratio, aHR, 2.75, 95% CI, 2.61–2.90) and a history of suicide attempt (2.70, 2.52–2.88). The risk of poisoning death was most strongly associated with substance use disorder (3.02, 2.60–3.52) and a history of suicide attempt (2.38, 1.94–2.91). Female sex was associated with a higher risk of hospitalization (1.19, 1.14–1.25), but a lower risk of death (0.72, 0.62–0.82). Conclusion Individuals with bipolar disorder face a substantial risk of poisoning by medicines or illegal substances, with notable sex differences in hospitalization and death rates. Key risk factors include substance use disorder and a history of suicide attempt.""]"
"Bo Mattìasson","","","Computer Science","https://openalex.org/A5041402257","[""In recent years, injectable cryogels have been frequently used in various biomedical applications such as tissue engineering, drug delivery, therapeutics, therapy, cell transplantation and immunotherapy. Cryogels, one of the biomaterials with injectable 3D scaffold structures, have important properties such as biocompatibility, physical resistance, macropore and sensitivity. In addition to being biocompatible, cryogels, consisting of three-dimensional (3D) scaffolds with physicochemical properties such as elasticity, hardness and biological degradation, provide a structural support environment for tissue development along with cell attachment. In addition, injectable cryogels can be loaded with therapeutic agents or cells in tissue engineering and clinical studies. The purpose of this review is to explain the physical and chemical properties of injectable cryogels consisting of three-dimensional scaffolding, after explaining the properties and synthesis stages of cryogels, and it is thought to be a guide to make their use in different areas more widespread for future studies."", ""Microbial contaminants are responsible for several infectious diseases, and they have been introduced as important potential food- and water-borne risk factors. They become a global burden due to their health and safety threats. In addition, their tendency to undergo mutations that result in antimicrobial resistance makes them difficult to treat. In this respect, rapid and reliable detection of microbial contaminants carries great significance, and this research area is explored as a rich subject within a dynamic state. Optical sensing serving as analytical devices enables simple usage, low-cost, rapid, and sensitive detection with the advantage of their miniaturization. From the point of view of microbial contaminants, on-site detection plays a crucial role, and portable, easy-applicable, and effective point-of-care (POC) devices offer high specificity and sensitivity. They serve as advanced on-site detection tools and are pioneers in next-generation sensing platforms. In this review, recent trends and advances in optical sensing to detect microbial contaminants were mainly discussed. The most innovative and popular optical sensing approaches were highlighted, and different optical sensing methodologies were explained by emphasizing their advantages and limitations. Consequently, the challenges and future perspectives were considered.""]"
"T. Ekelöf","","","Computer Science","https://openalex.org/A5115589102","[""In this paper, we study scalar mediator induced nonstandard interactions (SNSIs) in the context of the ESSnuSB experiment. In particular, we study the capability of ESSnuSB to put bounds on the SNSI parameters and also study the impact of SNSIs in the measurement of the leptonic <a:math xmlns:a=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><a:mi>C</a:mi><a:mi>P</a:mi></a:math> phase <c:math xmlns:c=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><c:msub><c:mi>δ</c:mi><c:mrow><c:mi>C</c:mi><c:mi>P</c:mi></c:mrow></c:msub></c:math>. Existence of SNSIs modifies the neutrino mass matrix and this modification can be expressed in terms of three diagonal real parameters (<e:math xmlns:e=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><e:msub><e:mi>η</e:mi><e:mrow><e:mi>e</e:mi><e:mi>e</e:mi></e:mrow></e:msub></e:math>, <g:math xmlns:g=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><g:msub><g:mi>η</g:mi><g:mrow><g:mi>μ</g:mi><g:mi>μ</g:mi></g:mrow></g:msub></g:math>, and <i:math xmlns:i=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><i:msub><i:mi>η</i:mi><i:mrow><i:mi>τ</i:mi><i:mi>τ</i:mi></i:mrow></i:msub></i:math>) and three off-diagonal complex parameters (<k:math xmlns:k=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><k:msub><k:mi>η</k:mi><k:mrow><k:mi>e</k:mi><k:mi>μ</k:mi></k:mrow></k:msub></k:math>, <m:math xmlns:m=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><m:msub><m:mi>η</m:mi><m:mrow><m:mi>e</m:mi><m:mi>τ</m:mi></m:mrow></m:msub></m:math>, and <o:math xmlns:o=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><o:msub><o:mi>η</o:mi><o:mrow><o:mi>μ</o:mi><o:mi>τ</o:mi></o:mrow></o:msub></o:math>). Our study shows that the upper bounds on the parameters <q:math xmlns:q=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><q:msub><q:mi>η</q:mi><q:mrow><q:mi>μ</q:mi><q:mi>μ</q:mi></q:mrow></q:msub></q:math> and <s:math xmlns:s=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><s:msub><s:mi>η</s:mi><s:mrow><s:mi>τ</s:mi><s:mi>τ</s:mi></s:mrow></s:msub></s:math> depend upon how <u:math xmlns:u=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><u:mi mathvariant=\""normal\"">Δ</u:mi><u:msubsup><u:mi>m</u:mi><u:mn>31</u:mn><u:mn>2</u:mn></u:msubsup></u:math> is minimized in the theory. However, this is not the case when one tries to measure the impact of SNSIs on <x:math xmlns:x=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><x:msub><x:mi>δ</x:mi><x:mrow><x:mi>C</x:mi><x:mi>P</x:mi></x:mrow></x:msub></x:math>. Further, we show that the <z:math xmlns:z=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><z:mi>C</z:mi><z:mi>P</z:mi></z:math> sensitivity of ESSnuSB can be completely lost for certain values of <bb:math xmlns:bb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><bb:msub><bb:mi>η</bb:mi><bb:mrow><bb:mi>e</bb:mi><bb:mi>e</bb:mi></bb:mrow></bb:msub></bb:math> and <db:math xmlns:db=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><db:msub><db:mi>η</db:mi><db:mrow><db:mi>μ</db:mi><db:mi>τ</db:mi></db:mrow></db:msub></db:math> for which the appearance channel probability becomes independent of <fb:math xmlns:fb=\""http://www.w3.org/1998/Math/MathML\"" display=\""inline\""><fb:msub><fb:mi>δ</fb:mi><fb:mrow><fb:mi>C</fb:mi><fb:mi>P</fb:mi></fb:mrow></fb:msub></fb:math>. Published by the American Physical Society 2024"", ""The armed invasion of Ukraine by the Russian Federation has adversely affected the relations between Russia and Western countries. Among other aspects, it has put scientific cooperation and collaboration into question and changed the scientific landscape significantly. Cooperation between some Western institutions and their Russian and Belarusian partners were put on hold after February 24, 2022. The CERN Council decided at its meeting in December 2023 to terminate cooperation agreements with Russia and Belarus that date back a decade. CERN is an international institution with UN observer status, and has so far played a role in international cooperation which was independent of national political strategies. We argue that the Science4Peace idea still has a great value and scientific collaboration between scientists must continue, since fundamental science is by its nature an international discipline. A ban of scientists participating in international cooperation and collaboration is against the traditions, requirements and understanding of science. We call for measures to reactivate the peaceful cooperation of individual scientists on fundamental research in order to stimulate international cooperation for a more peaceful world in the future. Specifically, we plead for finding ways to continue this cooperation through international organizations, such as CERN and JINR."", ""The ISOLDE Scientific Infrastructure at CERN offers a unique range of post-accelerated radioactive beams. The scientific program can be improved with the “Isolde Superconducting Recoil Separator” (ISRS), an innovative spectrometer able to deliver unprecedented (A, Z) resolution. In this paper we present an overview of the physics and ongoing technical developments."", ""The results of a search with the DELPHI Barrel RICH for Cherenkov rings having radii greater than those produced by ultrarelativistic particles are presented. The search for such anomalous rings is based on the data collected by the DELPHI Collaboration at CERN during the LEP1 and LEP2 periods. The DELPHI RICH detector was conceived for the identification of the stable and quasi-stable hadrons ($\\pi/K/p$). The present analysis was made investigating electron-like particles. A subsample of events containing anomalous rings has been identified for which the probability that the reconstructed rings in a given event are due to fortuitous combinations of background hits is low ($10^{-3}$ or less). A detailed study of background sources capable of producing apparently anomalous rings has been done; it indicates that the background hypothesis has a low probability. Additional arguments against this hypothesis are provided by by a comparison of rates of events with single and double anomalous rings in the gaseous radiator, and by the observation of a high degree of correlation between anomalous ring radii in the liquid and gaseous radiators. The results of the present analysis provide an interesting indication of the existence of anomalous Cherenkov rings. To corroborate this indication further searches for anomalous rings need to be made in future dedicated experiments. This work has been performed by the authors following the rules for external access to the DELPHI archived data, as established in http://delphiwww.cern.ch/delsec/finalrules/FINALrules011203.pdf The opinions, findings and conclusions expressed in this material are those of the authors alone and do not reflect in any way the views of the DELPHI Collabora"", ""The nature of b-quark jet hadronisation has been investigated using data taken at the Z peak by the DELPHI detector at LEP. Two complementary methods are used to reconstruct the energy of weakly decaying b-hadrons, $E_{\\mathrm{B}}^{\\mathrm{weak}}$ . The average value of $x^{\\mathrm{weak}}_{\\mathrm{B}} = E_{\\mathrm{B}}^{\\mathrm{weak}}/E_{\\mathrm{beam}}$ is measured to be 0.699±0.011. The resulting $x^{\\mathrm{weak}}_{\\mathrm{B}}$ distribution is then analysed in the framework of two choices for the perturbative contribution (parton shower and Next to Leading Log QCD calculation) in order to extract measurements of the non-perturbative contribution to be used in studies of b-hadron production in other experimental environments than LEP. In the parton shower framework, data favour the Lund model ansatz and corresponding values of its parameters have been determined within PYTHIA 6.156 from DELPHI data: $$a= 1.84^{+0.23}_{-0.21}\\quad\\mbox{and}\\quad b=0.642^{+0.073}_{-0.063}~\\mathrm{GeV}^{-2},$$ with a correlation factor ρ=92.2%. Combining the data on the b-quark fragmentation distributions with those obtained at the Z peak by ALEPH, OPAL and SLD, the average value of $x^{\\mathrm{weak}}_{\\mathrm{B}}$ is found to be 0.7092±0.0025 and the non-perturbative fragmentation component is extracted. Using the combined distribution, a better determination of the Lund parameters is also obtained: $$a= 1.48^{+0.11}_{-0.10}\\quad\\mbox{and}\\quad b=0.509^{+0.024}_{-0.023}~\\mathrm{GeV}^{-2},$$ with a correlation factor ρ=92.6%.""]"
"Anders Nilsson","","","Computer Science","https://openalex.org/A5075231234","[""In this work, we introduce a modified dip-and-pull electrochemical X-ray photoelectron spectroscopy (ECXPS) approach that offers new mechanistic insight into the alkaline carbon monoxide reduction reaction (CORR) over a Cu(111) single crystal surface. We tackle two major unresolved questions in the CORR mechanism that persist in the literature. Firstly, we address the mechanism for methane formation on Cu(111) and show that the mechanism likely proceeds via atomic carbon, which subsequently couples, leading to the accumulation of amorphous carbon on the surface. Secondly, we provide insight into whether the mechanism for acetate formation occurs entirely on the surface or partially within the solution phase, showing that acetate is present on the surface, indicating a surface-based reaction. These insights into surface-based mechanisms provide a handle for designing future catalysts that can efficiently target the binding of specific intermediates. Furthermore, we expect that our modified approach to dip-and-pull ECXPS - in which we have changed the electrode geometry, the method of introducing the reactant gas and used hard x-rays - will significantly expand the technique's applicability, enabling studies of the CO(2)RR and beyond."", ""Abstrakt In dieser Arbeit stellen wir einen modifizierten Ansatz der elektrochemischen Röntgen‐Photoelektronenspektroskopie (ECXPS) vor, der neue mechanistische Einblicke in die alkalische Kohlenmonoxid‐Reduktionsreaktion (CORR) an einer Cu(111)‐Einkristalloberfläche bietet. Wir befassen uns mit zwei wichtigen ungelösten Fragen zum CORR‐Mechanismus, die in der Literatur immer noch bestehen. Erstens befassen wir uns mit dem Mechanismus der Methanbildung auf Cu(111) und zeigen, dass der Mechanismus wahrscheinlich über atomaren Kohlenstoff abläuft, der anschließend koppelt, was zur Ansammlung von amorphem Kohlenstoff auf der Oberfläche führt. Zweitens geben wir Aufschluss darüber, ob der Mechanismus für die Acetatbildung vollständig an der Oberfläche oder teilweise in der Lösungsphase abläuft, indem wir zeigen, dass Acetat an der Oberfläche vorhanden ist, was auf eine oberflächenbasierte Reaktion hindeutet. Diese Einblicke in die auf der Oberfläche stattfindenden Mechanismen bieten einen Ansatzpunkt für die Entwicklung zukünftiger Katalysatoren, die so effizient für die Bindung spezifischer Zwischenprodukte optimiert werden können. Darüber hinaus erwarten wir, dass unser modifizierter Ansatz für Dip‐and‐Pull ECXPS mit geänderter Elektrodengeometrie und Methode zur Einführung des Reaktionsgases sowie der Verwendung harter Röntgenstrahlung, die Anwendbarkeit der Technik erheblich erweitern und Untersuchungen der CO (2) RR und weiterer Reaktionen ermöglichen wird."", ""In this work, we introduce a modified dip-and-pull ECXPS approach that offers new mechanistic insight into the alkaline CORR over a Cu(111) single crystal surface. We tackle two major unresolved questions in the CORR mechanism that persist in the literature. Firstly, we address the mechanism for methane formation on Cu(111) and show that the mechanism likely proceeds via atomic carbon, which subsequently couples, leading to the accumulation of amorphous carbon on the surface. Secondly, we provide insight into whether the mechanism for acetate formation occurs entirely on the surface or partially within the solution phase, showing that acetate is present on the surface, indicating a surface-based reaction. These insights into surface-based mechanisms provide a handle for designing future catalysts that can efficiently target the binding of specific intermediates. Furthermore, we expect that our modified approach to dip-and-pull ECXPS - in which we have changed the electrode geometry, the method of introducing the reactant gas, and used hard x-rays - will significantly expand the technique’s applicability, enabling studies of the CO(2)RR and beyond.""]"
"Robin Room","","","Computer Science","https://openalex.org/A5082884914","[""Cross sectional research has demonstrated that screening tool questions on frequency of alcohol consumption are a better predictor of dependence and harmful drinking in younger adults; questions about quantity per occasion are a better predictor in older adults. The aim of this study is to see if this relationship also holds longitudinally. A total of 9076 respondents aged 15 and over completed at least two waves of the longitudinal annual Household Income and Labour Dynamics in Australia survey 10 years apart between 2001-2010 and 2012-2020. Standardised scores from responses to questions on drinking quantity and frequency in the first survey were used to predict consumption 10 years later in groups stratified by age. Frequency of consumption was a significantly better predictor of future consumption than quantity in younger drinkers (aged < 36; β = 9.3, 95% confidence interval [CI] 8.6-10.0), than older drinkers (aged > 49; β = 5.1, 95% CI 4.8-5.5) while quantity was a better predictor in older drinkers (β = 8.2, 95% CI 7.2-9.3) than younger drinkers (β = 3.4, 95% CI 3.1-3.7). Some commonly used screening items, such as drinking quantity and frequency, are differentially effective at identifying future heavy drinkers between age groups. Development of age-specific screening tools could potentially lead to more accurate identification of people who could benefit from intervention to reduce their alcohol consumption."", ""Abstract Introduction Despite successful public health campaigns, tobacco use persists as a major cause of preventable illness and death. While tobacco taxation is recognized as an effective control strategy, concerns remain about potential financial strain on lower socioeconomic groups. This study investigates the relationship between household tobacco expenditure and financial stress in Australia, a country with high tobacco taxes and declining smoking rates. Methods Household data from the 2015-16 Australian Household Expenditure Survey were analysed (N=10,036). Financial stress was measured using a scale based on nine self-reported indicators. Respondents were asked to report if their household had experienced any of these difficulties, e.g. inability to pay utility bills or going without meals. Negative binomial regression models assessed the association between tobacco expenditure share and financial stress, adjusting for sociodemographic factors, household wealth, and other expenditures. Results Financial stress was more prevalent among households that did (45.0%; (95% CI: 42.5, 47.5)) versus did not (25.4%) purchase tobacco. All levels of tobacco expenditure were significantly associated with higher financial stress bivariably, after controlling for covariates. For instance, households in the second-lowest tobacco expenditure share quintile had a higher mean financial stress score than non-purchasing households (RR=1.59, CI, 1.36, 1.85, p&amp;lt;0.001). Discussion In Australia, financial stress is prevalent among tobacco-purchasing households and household tobacco expenditure is significantly associated with increased financial stress even at modest levels of spending, i.e. the lower quintiles of tobacco expenditure. These findings underscore the need for targeted policies to mitigate financial strain and support smoking cessation among vulnerable populations. Implications This study found that the prevalence of financial stress is higher in Australian households that purchase tobacco, regardless of their spending on tobacco. Although tobacco price increases reduce overall tobacco use, our study shows that increased prices exacerbate strain among financially disadvantaged smokers. Further research into associations between financial well-being and tobacco use is needed, both nationally and internationally. Longitudinal research should also examine the longer-term health and economic impacts mediated by financial stress.""]"
"Bertil Persson","","","Computer Science","https://openalex.org/A5004175173","[""Idealet om at politikk skal være kunnskapsbasert står sterkt. Politikken henter kunnskap og forskning fra forskningssystemet, særlig fra instituttsektoren. Denne boka handler om forskningsinstitusjoner og forskningspolitikk, og mest om instituttsektoren. Det er institusjoner utenfor universiteter og høyskoler med forskning og utviklingsarbeid som hovedoppgave. Slike institutter finnes i mange land, men deres roller og antall har variert over tid og mellom land. Her handler det mest om norske forhold, men også om forskningsinstitutter i Danmark og Sverige. Boka gir ny innsikt om instituttsektorens roller og virke i Skandinavia. Den handler også om universiteter og høyskoler og forholdet mellom institusjonene. Ulike forvaltnings- og politikkområders forhold til forskningsinstitutter og bruk av forskning diskuteres. Boka har 15 forfattere og er flerfaglig. De fleste er statsvitere og historikere og kommer fra universiteter og høyskoler, og fra instituttsektoren. Mange har også egne erfaringer fra forskjellige deler av forskningssystemet. Historiske og statsvitenskapelige perspektiver kombineres, med vekt på politisk kontekst, intensjoner ved etablering av instituttene og analyser av utvikling over tid."", ""Background: Teledermoscopy (TDS) emerges as an efficient tool for diagnosing skin lesions. In Sweden, double reading is the standard of care, but risk factors for misdiagnosis or mismanagement using single reader evaluations (SRE) are not well-studied. This study aimed to assess the accuracy of SRE compared with the gold standard in TDS. Methods: This retrospective cohort study involved 1,997 TDS referrals sent from general practitioners to dermatologists in Stockholm, Sweden, selected based on dermoscopic diagnoses. All referrals underwent double reader evaluations (DRE). Each case was reassessed by a single external assessor, blinded to the DRE result. Based on predefined rules, a gold standard for the most correct diagnosis was established. Diagnostic accuracy and risk factors for misdiagnosis were evaluated. The trial was registered on ClinicalTrials.gov (ID NCT05033678). Results: Primary diagnosis by SRE agreed with the gold standard on benign-malignant classification in 84% of cases. Discordance was linked to lower diagnostic confidence and more frequent recommendations for further intervention. SRE achieved a benign-malignant sensitivity and specificity of 84% (95% confidence interval: 81-87% and 82-86%, respectively). The risk of overdiagnosis increased 96 times when assessors reported being \""very unconfident.\"" Out of a total of 311 melanomas, melanoma in situ, lentigo maligna, and severely dysplastic nevi, 62 were not recognized in the SRE primary diagnosis. However, 50 of these misdiagnosed lesions were still recommended for accurate management. Conclusions: The confidence level of TDS assessors heavily influences diagnostic accuracy. Therefore, when diagnostic confidence is perceived as moderate or low, additional interventions should be considered."", ""In the dawning era of artificial intelligence (AI), health care stands to undergo a significant transformation with the increasing digitalization of patient data. Digital imaging, in particular, will serve as an important platform for AI to aid decision making and diagnostics. A growing number of studies demonstrate the potential of automatic pre-surgical skin tumor delineation, which could have tremendous impact on clinical practice. However, current methods rely on having ground truth images in which tumor borders are already identified, which is not clinically possible. We report a novel approach where hyperspectral images provide spectra from small regions representing healthy tissue and tumor, which are used to generate prediction maps using artificial neural networks (ANNs), after which a segmentation algorithm automatically identifies the tumor borders. This circumvents the need for ground truth images, since an ANN model is trained with data from each individual patient, representing a more clinically relevant approach."", ""Diagnosing invasive cutaneous melanoma (CM) can be challenging due to subjectivity in distinguishing equivocal nevi, melanoma in situ and thin CMs. The underlying molecular mechanisms of progression from nevus to melanoma must be better understood. Identifying biomarkers for treatment response, diagnostics and prognostics is crucial. Using biomedical data from biobanks and population-based healthcare data, translational research can improve patient care by implementing evidence-based findings. The BioMEL biobank is a prospective, multicentre, large-scale biomedical database on equivocal nevi and all stages of primary melanoma to metastases. Its purpose is to serve as a translational resource, enabling researchers to uncover objective molecular, genotypic, phenotypic and structural differences in nevi and all stages of melanoma. The main objective is to leverage BioMEL to significantly improve diagnostics, prognostics and therapy outcomes of patients with melanoma. The BioMEL biobank contains biological samples, epidemiological information and medical data from adult patients who receive routine care for melanoma. BioMEL is focused on primary and metastatic melanoma, but equivocal pigmented lesions such as clinically atypical nevi and melanoma in situ are also included. BioMEL data are gathered by questionnaires, blood sampling, tumour imaging, tissue sampling, medical records and histopathological reports. The BioMEL biobank project is approved by the national Swedish Ethical Review Authority (Dnr. 2013/101, 2013/339, 2020/00469, 2021/01432 and 2022/02421-02). The datasets generated are not publicly available due to regulations related to the ethical review authority. NCT05446155.""]"
"Jonas Bergquist","","","Computer Science","https://openalex.org/A5067538945","[""Previous scientific explorations of kohl and other make-up substances from ancient Egypt have revealed a considerable diversity of materials and recipes used in different regions and time periods. However, samples from Sudanese Nubia have never been included in scientific investigations of make-up substances used along the Nile valley. For the first time, 24 samples of kohl and other cosmetics from Bronze Age Sudanese Lower Nubia (c. 2055–1070 BCE) were analysed using optical microscopy, GC-MS, SEM-EDS, ATR-FTIR and XRD. Beyond expanding our knowledge of make-up usage in the ancient Nile valley by including samples from Sudan, this study adds further depth to our understanding of make-up substances in ancient Northeast Africa by exploring samples from well-defined archaeological contexts. The multi-analytical approach presented here sheds light on the diversity of recipes used by various communities in the Middle Nile valley during the Bronze Age. Most samples are dominated by lead sulphides, but these occur in various mixtures with quartz, clay, calcite, gypsum and zinc compounds, in addition to plant gums and animal fats. We also report for the first time the use of synthetic calcium antimonate in ancient cosmetic mixtures. Besides expanding our knowledge of make-up mixtures in ancient Northeast Africa, our study suggests that the considerable variation detected across the cultural borders of Bronze Age Egypt and Nubia reflects distinctive bodily ideals."", ""Myalgic encephalomyelitis (ME) is a chronic, multisystem illness characterized by post-exertional malaise (PEM) and cognitive dysfunction, yet the molecular mechanisms driving these hallmark symptoms remain unclear. This study investigated haptoglobin (Hp) as a potential biomarker of PEM severity and cognitive impairment in ME, with a focus on Hp phenotypes and structural proteoforms. A longitudinal case–control study was conducted in 140 ME patients and 44 matched sedentary healthy controls. In the discovery phase, global plasma proteomic profiling was performed in 61 ME patients and 20 controls before and after a standardized, non-invasive stress protocol in order to induce PEM. Associations between Hp levels, phenotype, and cognitive performance were assessed. In the validation phase, plasma Hp concentrations and proteoform composition were analyzed in an independent cohort of 89 ME patients and 24 controls using high-performance liquid chromatography (HPLC). ME patients demonstrated a significant reduction in Hp levels following post-exertional stress. Lower baseline Hp concentrations were associated with impaired cognitive performance. Hp phenotypes were differentially associated with symptom burden, with the Hp2-1 phenotype enriched in ME and linked to greater PEM severity and cognitive deficits compared to Hp1-1 and Hp2-2. HPLC analysis revealed altered Hp proteoform profiles in the Hp2-1 subgroup, including increased high-mass tetrameric and pentameric forms and shorter retention times indicative of structural changes. In contrast, the Hp1-1 phenotype was associated with milder symptoms and greater cognitive resilience. These findings suggest that Hp phenotype and proteoform structure modulate the physiological response to post-exertion in ME, offering insight into the molecular basis of PEM and its clinical heterogeneity. Hp may serve as a translational biomarker for patient stratification and a potential therapeutic target to mitigate oxidative stress and cognitive dysfunction in ME."", ""Abstract Background Critical Illness Myopathy (CIM) is a devastating consequence of modern critical care, causing dramatic loss of muscle mass and function in intensive care unit (ICU) patients. However, the loss in function by far exceeds the loss in muscle mass and myosin content, but the molecular mechanisms underlying the loss of force and myosin-expressing non-force (NF) generating fibers remain elusive. Objectives To explore the mechanisms underlying the compromised myosin function in ICU patients exposed to 12-day mechanical ventilation and immobilization. Methods Mass spectrometry-based proteomics and molecular dynamics simulations were used to explore the pathophysiology underlying the compromised muscle fiber function previously reported at the single muscle fiber level in six ICU patients on 12 th day compared with the 1 st day of mechanical ventilation and immobilization. Results Previous measurements revealed single muscle fiber size and specific force to be decreased by ∼25% and 35% (p &lt; 0.05), respectively, from the 1 st to the 12 th days. On the 12 th -day, all fibers showed a decreased specific force and a subset of myosin-expressing fibers was identified exhibiting a complete loss of contractile function despite showing comparable fiber atrophy levels (∼30%, p &lt; 0.05). Compromised force-generating capacity was linked to 27 post-translational myosin modifications, including oxidation, ubiquitination, acetylation, and methylation. Molecular dynamics simulations revealed an oxidative-induced rigidity of the myosin head, compromising the actin-binding and converter domains’ flexibility. Notably, the non-force generating fibers exhibited a unique proteomic signature linked with increased structural exposure and rigidity of the myosin motor domain. Conclusions In addition to muscle wasting and preferential myosin loss, abnormal myosin post-translational modifications contribute to the dramatic loss in muscle function in ICU patients with CIM, including the development of muscle fibers unable to generate contractile force."", ""Sodium-glucose co-transporter 2 inhibitors like canagliflozin (CFZ) have shown promise in preventing hyperinsulinemia-associated laminitis in horses, but data on pharmacokinetics, tolerability, and controlled studies are limited. This randomized, open-label, placebo-controlled, crossover study evaluated these aspects of CFZ treatment in eight healthy Standardbred mares. Each horse received single supratherapeutic oral doses of CFZ (1.8mg/kg or 3.6mg/kg) and placebo, with a two-week washout between treatments. A graded glucose infusion (GGI) was administered post-treatment to evaluate glucose and insulin responses. Plasma CFZ, glucose, insulin, urinary glucose, serum biochemistry, and urinalysis samples were collected over 72hours post-treatment. For CFZ 1.8mg/kg, median Cmax was 2623ng/mL, Tmax 2.2hours, and T1/2Z 21.8hours; for 3.6mg/kg, Cmax was 4975ng/mL, Tmax 2.8hours, and T1/2Z 23.0hours. The pharmacokinetics of CFZ displayed dose-proportionality across the two tested doses. Insulin and glucose responses to a GGI, measured by the area under the concentration-time curve (AUC), were similar between CFZ doses but significantly reduced compared to placebo (p < 0.001). Specifically, mean glucose AUC for CFZ treatments was approximately 14-15% lower, and mean insulin AUC 22-29% lower, than for placebo. For CFZ-treated horses, mean urinary glucose concentrations ranged from 277 to 347mmol/L at 24, 48, and 72hours post-administration, with no significant differences between dose levels. No clinical signs of adverse effects were observed, although a significant increase in GLDH levels compared to placebo (p< 0.05) was observed with the CFZ 3.6mg/kg dose."", ""Abstract Background Myalgic encephalomyelitis / chronic fatigue syndrome (ME/CFS) is a multisystem disorder characterised by unrelenting fatigue, post-exertional malaise, and dysfunction across immune, nervous, metabolism, and endocrine systems. Given the broad role of steroid hormones in regulating these systems, this study investigated differences in the steroid metabolome and network dynamics between ME/CFS patients and matched controls. Methods Blood plasma steroid levels were quantified using Ultra-Performance Supercritical Fluid Chromatography- Tandem Mass Spectrometry (UPSFC-MS/MS) in ME/CFS patients ( n = 24) and age and gender matched controls ( n = 24). Group comparisons of absolute steroid concentrations were performed using Mann-Whitney U tests. Partial Spearman correlation networks were evaluated to examine direct associations between steroids within each group, and centrality metrics were used to evaluate structural differences. Steroid-steroid ratios were analysed to reflect biochemical relationships. Multivariate analysis with Orthogonal Partial Least Squares Discriminant Analysis (OPLS-DA) was also conducted. Results No significant group differences in absolute steroid concentrations were observed following FDR correction. However, network analysis revealed a marked reduction in direct steroid-steroid relationships in ME/CFS, with controls exhibiting 52 significant partial correlations, while the ME/CFS group retained only one (cortisol - corticosterone). Centrality analysis further revealed a shift in network structure, with cortisone emerging as highly central in ME/CFS (degree = 7, betweenness = 16.7), despite being peripheral in controls, and progesterone showing reduced integration in ME/CFS (degree = 3 vs. 12, eigenvector = 0.40 vs. 0.93). Steroid-steroid ratio analysis revealed a higher cortisol-to-pregnanolone ratio and a lower pregnanolone-to-progesterone ratio in ME/CFS, although these findings did not remain significant after FDR correction. OPLS-DA indicated a modest relationship between steroid levels and group classification (R²Y = 22.8%), but negative Q² values suggested poor predictive power. Conclusions Despite no significant differences in absolute steroid levels, network analysis revealed profound disruptions in steroid-steroid relationships in ME/CFS compared to controls, suggesting disrupted steroid homeostasis. Collectively the results suggest dysregulation of HPA axis function and progestogen pathways, as demonstrated by altered partial correlations, centrality profiles, and steroid ratios. These findings illustrate the importance of hormone network dynamics in ME/CFS pathophysiology and underscores the need for more research into steroid metabolism.""]"
"Petre Stoica","","","Computer Science","https://openalex.org/A5007877889","[]"
"Henk Wymeersch","","","Computer Science","https://openalex.org/A5033860704","[]"
"A. Hallgren","","","Computer Science","https://openalex.org/A5052702737","[""The science program of the Radio Neutrino Observatory-Greenland (RNO-G) extends beyond particle astrophysics to include radioglaciology and, as we show herein, solar physics, as well. Impulsive solar flare observations not only permit direct measurements of light curves, spectral content, and polarization on time scales significantly shorter than most extant dedicated solar observatories, but also offer an extremely useful above-surface calibration source, with pointing precision of order tens of arc-minutes. Using the early RNO-G data from 2022-2023, observed flare characteristics are compared to well-established solar observatories. Also, a number of individual flares are used to highlight angular reconstruction and calibration methods. RNO-G observes signal excesses during solar flares reported by the solar-observing Callisto network and in coincidence with about 60% of the brightest excesses recorded by the SWAVES satellite, when the Sun is above the horizon for RNO-G. In these observed flares, there is significant impulsivity in the time-domain. In addition, the solar flares are used to calibrate the RNO-G absolute pointing on the radio signal arrival direction to sub-degree resolution."", ""Abstract We recently reported on the radio-frequency attenuation length of cold polar ice at Summit Station, Greenland, based on bi-static radar measurements of radio-frequency bedrock echo strengths taken during the summer of 2021. Those data also allow studies of (a) the relative contributions of coherent (such as discrete internal conducting layers with sub-centimeter transverse scale) vs incoherent (e.g. bulk volumetric) scattering, (b) the magnitude of internal layer reflection coefficients, (c) limits on signal propagation velocity asymmetries (‘birefringence’) and (d) limits on signal dispersion in-ice over a bandwidth of ~100 MHz. We find that (1) attenuation lengths approach 1 km in our band, (2) after averaging 10 000 echo triggers, reflected signals observable over the thermal floor (to depths of ~1500 m) are consistent with being entirely coherent, (3) internal layer reflectivities are ≈–60 $\\to$ –70 dB, (4) birefringent effects for vertically propagating signals are smaller by an order of magnitude relative to South Pole and (5) within our experimental limits, glacial ice is non-dispersive over the frequency band relevant for neutrino detection experiments."", ""Abstract The ARIANNA experiment is an Askaryan radio detector designed to measure high-energy neutrino induced cascades within the Antarctic ice. Ultra-high-energy neutrinos above 10 16 eV have an extremely low flux, so experimental data captured at trigger level need to be classified correctly to retain as much neutrino signal as possible. We first describe two new physics-based neutrino selection methods, or “cuts”, (the updown and dipole cut) that extend the previously published analysis to a specialized ARIANNA station with 8 antenna channels, which is double the number used in the prior analysis. For a standard trigger with a threshold signal to noise ratio at 4.4, the new cuts produce a neutrino efficiency of &gt; 95% per station-year of operation, while rejecting 99.93% of the background (corresponding to 53 remaining experimental background events). When the new cuts are combined with a previously developed cut using neutrino waveform templates, all background is removed at no change of efficiency. In addition, the neutrino efficiency is extrapolated to 1,000 station-years of operation, obtaining 91%. This work then introduces a new selection method (the deep learning cut) to augment the identification of neutrino events by using deep learning methods and compares the efficiency to the physics-based analysis. The deep learning cut gives 99% signal efficiency per station-year of operation while rejecting 99.997% of the background (corresponding to 2 remaining experimental background events), which are subsequently removed by the waveform template cut at no significant change in efficiency. The results of the deep learning cut were verified using measured cosmic rays which shows that the simulations do not introduce artifacts with respect to experimental data. The paper demonstrates that the background rejection and signal efficiency of near surface antennas meets the requirements of a large scale future array, as considered in baseline design of the radio component of IceCube-Gen2."", ""The Radio Neutrino Observatory in Greenland (RNO-G) is the only ultrahigh energy (UHE, ${\\gtrsim}30$ PeV) neutrino monitor of the Northern sky and will soon be the world's most sensitive high-uptime detector of UHE neutrinos. Because of this, RNO-G represents an important piece of the multimessenger landscape over the next decade. In this talk, we will highlight RNO-G's multimessenger capabilities and its potential to provide key information in the search for the most extreme astrophysical accelerators. In particular, we will highlight opportunities enabled by RNO-G's unique field-of-view, its potential to constrain the sources of UHE cosmic rays, and its complementarity with IceCube at lower energies."", ""The Radio Neutrino Observatory in Greenland (RNO-G) is an array of radio detector stations which has been designed to study ultra-high energy (𝐸 ≳ $10^{18}$ eV) neutrinos. The experiment, when completed, will have the best sensitivity in this energy range and will yield a major advancement in our understanding of the sources and propagation of the highest energy cosmic rays. While RNO-G will be sensitive to primarily 𝐸 ≳ 100 PeV neutrinos, the optical-based detectors only have a large enough exposure to study up to ∼ 1–10 PeV, leaving a gap in the energy range between the two detection methods. For RNO-G, the energy threshold is set by our ability to distinguish the Askaryan pulses, created from neutrino interactions, from the irreducible background of thermal noise. Using modern machine learning techniques, an online trigger can be implemented to identify small-amplitude pulses from in-ice cascades and thereby decrease the energy threshold of RNO-G. Such an advancement will increase the expected amount of observed neutrinos, as well as close the gap between radio- and optical-based observatories. We present a convolutional neural network for classification of neutrino events that can be run as a second-stage trigger.""]"
"J. G. Smith","","","Computer Science","https://openalex.org/A5008638540","[""Type 2 diabetes is a growing global concern with serious complications, including kidney damage and cardiovascular morbidity and mortality. Monitoring albuminuria, which is associated with these complications, is crucial in optimal diabetes management. Gut microbiota composition has been suggested to impact albuminuria, but large studies with granular data are lacking. We investigated the relationship between 1002 gut microbial species, 1308 plasma metabolites and albuminuria in 752 participants with type 2 diabetes from the Swedish CArdioPulmonary BioImage Study. To determine the relative abundance of species, we employed deep shotgun metagenomic sequencing of fecal samples. Plasma metabolites were analyzed using mass spectrometry-based methods. We identified three species that were associated with albuminuria, including Sellimonas intestinalis, Eggerthellales sp., Ellagibacter isourolithinifaciens. Two of these species were replicated in an independent pre-diabetic population (n=3,423) in SCAPIS. In total, 36 annotated metabolites were associated with the three albuminuria-signature species. Functional mapping of the signature species suggests a role in the regulation of the metabolites of imidazole propionate and trigonelline, which have previously been reported to play roles in the progression of albuminuria. These findings provide additional evidence of the potential impact of microbial species and contribute to our understanding of the complex relationship between the gut microbiome, plasma metabolites, and albuminuria in individuals with diabetes."", ""ABSTRACT Objective Although previous studies have shown reduced cardiovascular events following parathyroidectomy (PTX), it is unclear whether this extends to contemporary patients diagnosed and treated with milder disease than previously. The aim of this nation‐wide study was to determine the effect on cardiovascular events after PTX, and to comprehensively evaluate cardiovascular disease manifestations in patients with primary hyperparathyroidism, (pHPT). Design The cohort consisted of 5009 patients who underwent PTX and were identified from the Scandinavian Quality Register for Thyroid, Parathyroid and Adrenal Surgery. Patients were matched with 14,983 population controls. Methods Data was linked with the National Patient and Death Registries. Incidence rate ratios (IRRs) were estimated before and after PTX for recurrent events of acute myocardial infarction, stroke, transient ischemic attack (TIA), and first‐onset diagnoses of coronary artery disease, heart failure, aortic and mitral valve stenosis, carotid artery stenosis, peripheral artery disease, and aortic aneurysm (AA). Serum calcium and gland weight were analysed as predictors. Results TIA was increased in patients pre‐and postoperatively with a peak 1–4 years before PTX (IRR: 2.06, CI 95%: 1.31–3.25). The incidence rates for acute myocardial infarction and stroke were not increased pre‐ and postoperatively. Mitral valve stenosis (IRR: 3.22, 1.51–6.85), and heart failure (IRR: 1.37, 1.11–1.67) were increased preoperatively, but not postoperatively. AA was increased pre‐ and postoperatively. Conclusions The incidence rates for mitral valve stenosis and heart failure were increased preoperatively in patients with pHPT, normalizing after surgery. In contrast, the incidence of TIA and AA remained elevated postoperatively."", ""Abstract This collection gathers thirteen contributions by a number of historians, friends, colleagues and/or students of Jinty’s, who were asked to pick their favourite article by her and say a few words about it for an event held in her memory on 15 January 2025 at King’s College London. We offer this collection in print now for a wider audience not so much because it has any claim to be exhaustive or authoritative, but because taken all together these pieces seemed to add up to a useful retrospective on Jinty’s work, its wider context, and its impact on the field over the decades. We hope that, for those who know her work well already, this may be an opportunity to remember some of her classic (and a few less classic) articles, while at the same time serving as an accessible introduction to her research for anyone who knew her without necessarily knowing about her field, as well as for a new and younger generation of readers."", ""Heart failure (HF) is characterized by hemodynamic derangements that are likely to mediate systemic metabolic perturbations but limited data are available. Plasma metabolite profiling provides opportunities for comprehensive investigation of such perturbations. Here, we aimed to characterize plasma profiles that associate with HF and their relationship with central hemodynamics, symptom burden, and response to restoration of cardiac function by heart transplantation. Untargeted metabolite profiling was conducted with mass spectrometry in 2 independent case-control samples. In total, 89 of 797 studied metabolites were significantly associated with HF in both cohorts with concordant directionality. Amino acid, carbohydrate, and nucleotide metabolites were enriched for association with HF and were consistently increased in HF cases. A subset of patients with advanced HF subsequently underwent heart transplantation, after which 17 of the 89 metabolites returned significantly toward healthy control levels. These 17 metabolites represent increased catecholamine and heme metabolism, conjugated bile acids, kynurenine pathway mediators, spermidine metabolism, and allantoin, as well as tricarboxylic acid cycle and glycolysis intermediates. Most of these metabolites associated with symptom burden and at least 1 of 12 central hemodynamic parameters, primarily relating to either increased systemic or pulmonary venous congestion, lower cardiac output, or lower left ventricular stroke work. We comprehensively identified metabolite profiles associated with HF and central hemodynamics that reverse by cardiac transplantation. Increased levels of most metabolites also associated with higher symptom burden. Our findings provide perspectives on the metabolic consequences of HF with potential implications for noninvasive monitoring and tailored therapy.""]"
"Bo Edvardsson","","","Computer Science","https://openalex.org/A5012900375","[]"
"Anders Berglund","","","Computer Science","https://openalex.org/A5035616597","[""There is some evidence that mastocytosis patients are at increased risk of skin cancer. This study aimed to assess the risk of malignant melanoma (MM), melanoma in situ (Mis), and basal cell carcinoma (BCC). A dataset was generated by individual-level record linkages between Swedish population registers including the National Patient Register (NPR), the Swedish Cancer Register (SCR), and the Population Register (PR). Adult patients with a mastocytosis diagnosis between 2001 and 2018 were identified in the SCR and NPR. For each case, 5 mastocytosis-free comparators matched on age, sex, and county of residence were randomly chosen from the PR. Records of skin cancer were identified in the SCR and NPR. In total, the study encompassed 2,040 mastocytosis patients of whom 63 had a record of MM/Mis and 168 a record of BCC. Compared with comparators, the risk of MM/Mis was more than twofold higher (OR 2.39, 95% CI 1.8–3.2). Risk estimates for BCC were also elevated (OR 1.77, 95% CI 1.49–2.14). When assessing the timing of skin cancers, a substantial portion were diagnosed near index date. Taken together, in the present study these findings of increased risk of MM/Mis and BCC in mastocytosis patients may reflect an influence of detection bias."", ""Abstract Background Evolution is a fundamental concept in biology education, recently emphasized in the Swedish curriculum for Year 4–6. However, teaching evolution poses challenges, necessitating innovative educational tools. This study explores the development and use of a comic book, Cats on the Run–A Dizzying Evolutionary Journey , designed to teach evolutionary concepts to young students through a narrative involving two modern-day house cats traveling through time and space. Results To explore what function the material has for students’ meaning making we analyze what students describe to have learned working with the comic Cats on the Run, and how aspects of the comic book are reflected in the students’ self-reported learning. The study involved 159 students from Grades 4–6 who used the comic book in their biology lessons. Analysis of student survey responses revealed that the students draw on the comic’s narrative and imagery as they report on learning about key evolutionary concepts such as variation, natural selection, heredity, and evolutionary patterns. Analysis of student survey responses revealed that the comic facilitated meaning making about key evolutionary concepts such as variation, natural selection, heredity, and evolutionary patterns. Students were often referencing the comic's narrative and imagery as they reported their learning. Conclusions The findings suggest that the comic book is a valuable educational tool. More specifically, the narrative and multimodal aspects of the comic support meaning making and learning. This study highlights the importance of thoughtfully designed educational materials and suggests that combining different resources can enable discussion and learning of complex scientific concepts."", ""Abstract Background Intrinsically resistant glioma stem cells (GSCs) in the setting of a highly immunosuppressive tumor microenvironment (TME) remain the most predominant phenomenon leading to unfavorable therapeutic outcomes in glioblastoma (GBM). Hence there is an unmet need for novel anti-GBM therapeutic paradigms that can effectively target GSCs while simultaneously reprogramming the TME. Methods In this study, we leverage evidence from SMAC mimetic screening to evaluate and characterize the anti-tumor and immune TME modulating impacts of the lead SMAC mimetic Xevinapant at the single cell level in GBM. We utilized viability assays and orthotopic human and murine GBM models to assess the survival impacts of Xevinapant on GSCs in vitro and in vivo. Moreover, we employed single-cell RNA sequencing (scRNA-seq) to investigate the modulation impact of Xevinapant on GBM TME. Lastly, we investigated drug combination synergies to address potential mechanisms of tolerance or resistance to Xevinapant. Results According to our observations, in vitro exposure to Xevinapant induced apoptosis along with significant viability reduction in a dose-dependent manner, in both human and mouse GSCs. Moreover, Xevinapant treatment produced robust anti-tumor effects in vivo and significantly prolonged animal overall survival. Based on single-cell RNA seq analysis, Xevinapant did not only enhance GSCs apoptosis but also activated antitumor effector immune response leading to favorable reprogramming of immunosuppressive TME. Furthermore, we established and queried Xevinapant therapeutic signatures to the LINCS database in an effort to identify small molecules that could reverse treatment-induced tolerance to Xevinapant. We have identified a novel set of candidate small molecules with robust synergy when combined with Xevinapant. Conclusions In summary, Xevinapant exhibits robust anti-tumor activity on GSCs and favorable immune modulation of the TME in GBM, hence providing a rationale for further clinical investigation in GBM."", ""Background Papillary renal cell carcinoma (pRCC) is the second most common kidney cancer subtype, yet our understanding of its tumor immune microenvironment (TIME) remains limited. Objective We utilized multiplex immunofluorescence (mIF) and spatial transcriptomics (ST) to evaluate immune cell architecture in pRCC contrasted with clear cell RCC (ccRCC). Methods Localized RCC tumors (16 pRCC, 70 ccRCC) underwent mIF using markers for T cells, B cells, and tumor-associated macrophages (TAMs). Spatial data in both tumor and stromal compartments of the TIME were collected. A post hoc recurrence free survival analysis (RFS) was performed using Cox proportional hazard models. Single-cell ST was performed on a subset of samples, utilizing probes against 960 transcripts. Cell density, cell spatial clustering, and spatially varying gene expression were analyzed. Results Immune cell density was statistically lower in pRCC amongst functional CD8T cells, while cell clustering was higher amongst M2-like macrophages. Using ST, two genes ( CCL18 , GPNMB ) were enriched in clustered M2-like macrophages in pRCC (FDR &lt; 0.001) and are known markers of lipid-associated TAMs (LAMs). Conclusion Compared to ccRCC, pRCC has greater M2-like macrophage clustering. Using ST, M2-like macrophage clustering corresponds with lipid associated TAMs (LAMs), and therapeutics against this myeloid subset are currently being tested in pRCC.""]"
"Lars Alfredsson","","","Computer Science","https://openalex.org/A5071141746","[""Cardiovascular disease (CVD) is the leading cause of death in Europe, with myocardial infarction (MI) being one of its most severe manifestations. While many risk factors for CVD are well known, occupational exposures remain relatively understudied-especially in analyses that adjust for co-occurring workplace exposures. This study aimed to examine the association between occupational exposure to chemicals and particles and the risk of first-time MI. The cohort included all Swedish residents born between 1930 and 1990 who were employed between 1985 and 2013 and had no prior MI. Participants were followed from 1986 to 2017, and their occupational histories were linked to the Swedish Job Exposure Matrix (SweJEM) to estimate exposure to 31 chemicals and particles. MI cases were identified through national hospital discharge and cause of death registers. Using discrete time proportional hazards regression, we estimated gender-specific hazard ratios, adjusting for age, decision authority, physical workload, noise, other chemicals/particles, and within a subset smoking and body mass index (BMI). Among 225,366 incident MI cases, the strongest increased risk was associated with both cumulative, ever, and recent exposure to diesel exhaust, polycyclic aromatic hydrocarbons, sulfur dioxide, carbon dioxide, cadmium, chromium, iron, and lead. These associations remained after adjusting for smoking, BMI, and education. Hazard ratios were generally higher among women. The attributable proportion among exposed individuals was 9%, corresponding to over 20,000 cases during the follow-up period. Our findings highlight the importance of reducing occupational exposure to harmful substances to prevent future cases of MI."", ""The influence of body weight across the life course on multiple sclerosis (MS) progression remains incompletely understood. While excess body mass at diagnosis is associated with disability progression, it is unclear how early-life and adult BMI jointly affect long-term outcomes. We aimed to investigate the separate and combined effects of BMI at age 20 and at diagnosis on MS progression. We studied 2940 individuals with relapsing-onset MS from a population-based case-control study with prospective follow-up through the Swedish MS registry. BMI was calculated from self-reported weight at age 20 and at diagnosis. Outcomes included confirmed disability worsening (CDW), and time to reach EDSS 3 and EDSS 4. Cox regression and general linear models were used to examine associations between BMI and MS progression, including interaction terms. High BMI (> 28 kg/m2) at age 20 was associated with higher disability at diagnosis (β = 0.15, p = 0.0015), while BMI at diagnosis predicted increased risk of progression. Compared to individuals with BMI ≤ 28 kg/m2 at both time points, those with persistent elevated BMI had higher risks of CDW (HR 1.28, 95% CI 1.01-1.63), EDSS 3 (HR 1.64, 95% CI 1.21-2.24), and EDSS 4 (HR 1.51, 95% CI 1.00-2.39). Risks were increased, though less pronounced, among those with high BMI only at diagnosis. Early-life excess weight alone was not associated with progression. Interaction models suggested a stronger effect of adult BMI in the presence of early excess weight. High BMI at diagnosis was associated with faster disability progression, particularly when present since early adulthood. These findings underscore the potential benefits of early weight management in MS."", ""Abstract Background Rheumatoid arthritis (RA) is a chronic disease influenced by genetic and environmental factors, with viral infections potentially influencing the immune system and increase the risk of autoimmune diseases like RA. This study investigated the relationship between exposure to chikungunya and dengue infections, indicated by the presence of arboviral IgG antibodies, and risk of developing RA dichotomized by anti‐citrullinated protein antibody (ACPA) status. Methods Serum samples from the Malaysian Epidemiological Investigation of Rheumatoid Arthritis (MyEIRA) population‐based case‐control study involving 1235 RA cases and 1625 controls were assayed for IgG antibodies against chikungunya and dengue viruses. Positive results indicate previous exposure to the studied arboviral infections. Logistic regression and Mann–Whitney U analyses were performed to estimate the risk of developing ACPA‐positive/ACPA‐negative RA. Results We observed a low occurrence of chikungunya IgG antibody and a high occurrence of dengue IgG antibody in the overall RA cases and controls. No direct association was observed between ever exposure to chikungunya/dengue infection and risk of future RA in overall RA cases and controls. However, analysis by ethnicity showed a decreased risk for ACPA‐positive RA in chikungunya IgG antibody positive subjects of Malay (odds ratio [OR] = 0.38, 95% confidence interval [CI] = 0.16–0.90, p &lt; 0.05) and Chinese origins (OR = 0.12, 95% CI = 0.01–0.93, p &lt; 0.05). Additionally, we observed a decreased risk of ACPA‐positive RA in Indian among persons with positive dengue IgG antibody status (OR = 0.60, 95% CI = 0.38–0.95, p &lt; 0.05). No significant association was observed between chikungunya/dengue infection exposures and risk of ACPA‐negative RA. Conclusion This study found no overall association between chikungunya or dengue exposure, and the risk for developing RA."", ""A better understanding of factors associated with multiple sclerosis (MS) disease activity and disability is needed. Given the strong link between comorbid depression and MS disease activity and disability, we aimed to determine whether the depression genetic burden, as modelled using its polygenic score, is associated with MS disease activity and disability worsening. In this cohort study, we used samples from neurologist-defined adult people with MS (PwMS) followed in clinical care or during a clinical trial from existing cohorts: Canada, the United States (US), and Sweden with extensive longitudinal phenotypes. We computed the depression polygenic score (PGS) and tested its association with annualized relapse rate and worsening disability. In the US cohort, we additionally explored the time to relapse, number of enhancing lesions, and confirmed Expanded Disability Status Scale (EDSS) worsening during the study period. We included 3,420 relapsing-onset PwMS of European genetic ancestry with a median follow-up of 3 to 5 years. Meta-analyses revealed for each 1-standard deviation increase in the depression PGS, the relapse rate increased (incidence rate ratio: 1.23, 95% confidence interval [CI] = 1.01-1.50). In the US cohort, higher depression PGS was associated with protocol-defined relapses (hazard ratio [HR] = 1.58, 95% CI = 1.03-2.43), and time to confirmed EDSS worsening (HR = 1.51, 95% CI = 1.03-2.22) with this effect largely direct. Meta-analyses showed a higher depression genetic burden was associated with increased MS disease activity. In the US clinical trial cohort only, we found a significant association between higher depression PGS and time to relapse and confirmed EDSS worsening. These findings may provide insights into MS disease activity and disability worsening. ANN NEUROL 2025."", ""Importance The implications of socioeconomic factors, including educational level, for multiple sclerosis (MS) progression remain unclear. Understanding whether educational level directly affects MS outcomes or is confounded by lifestyle risk factors and treatment choices could inform personalized care strategies. Objective To investigate the association between educational level and outcomes related to MS, including worsening of disability, cognition, and health-related quality of life, after adjusting for potential confounding factors or mediation by lifestyle factors and treatment. Design, Setting, and Participants This cohort study used data from 2 large, population-based case-control studies conducted in Sweden from April 2005 to December 2019 that used Swedish MS Registry data with detailed clinical and sociodemographic information. Patients with relapsing-onset MS aged 25 years or older at disease onset after 1995 were followed up from diagnosis until April 6, 2022, with a mean (SD) follow-up time of 10.4 (5.4) years. Data analysis was performed from July 2024 to November 2024. Exposure Educational level categorized as presecondary (9 to 10 years of compulsory school), secondary (2 to 4 years of high school or college), and postsecondary (higher university education) July 2024 to November 2024. Main Outcomes and Measures The primary outcome was confirmed disability worsening defined as a 1-point increase in Expanded Disability Status Scale (EDSS) score sustained across 2 follow-up visits at least 24 weeks apart. Secondary outcomes were worsening of health-related quality of life, measured by the MS Impact Scale (MSIS-29) physical and psychological subscale scores, and cognitive disability worsening, measured by a decrease in Symbol Digit Modalities Test (SDMT) score. Cox proportional hazards regression was used to evaluate associations between educational level and disability progression. Results Of 3695 participants with MS, 2656 (71.9%) were female, with a mean (SD) age at diagnosis of 39.1 (9.1) years. Lower educational level was associated with older age at disease onset (mean [SD] age at onset: 42.2 [10.2] years for presecondary educational level vs 36.0 [8.4] years for postsecondary educational level), worse baseline clinical status (mean [SD] EDSS 2.7 [2.0] for presecondary education vs 1.7 [1.5] for postsecondary education), and lower likelihood of receiving second-line therapies (mean [SD] 164 [36.9%] for presecondary education vs 869 [54.1] for postsecondary education). In unadjusted analyses, lower educational level was associated with faster disability progression (ie, worsening), but this association was no longer significant after adjusting for treatment and lifestyle factors (adjusted hazard ratio [AHR], 1.14; 95% CI, 0.97-1.33). No associations were found between educational level and changes in MSIS-29 scores (AHR, 1.14 [95% CI, 0.90-1.44] for the MSIS-29 physical subscale and AHR, 1.00 [95% CI, 0.79-1.26] for the MSIS-29 psychological subscale) or SDMT performance over the 15-year follow-up (AHR, 1.05; 95% CI, 0.76-1.46). Mediation analysis revealed that treatment and lifestyle factors accounted for 79.9% of the observed association between education level and disability progression. Conclusions and Relevance In this cohort study of participants with MS, observed differences in disability worsening by educational level were largely accounted for by lifestyle and treatment factors, suggesting that educational level itself may not be independently associated with MS progression.""]"
"Kerry S. Courneya","","","Computer Science","https://openalex.org/A5047611713","[""ABSTRACT Purpose Wearable sensors that track physical activity in daily life may offer insights that help healthcare providers optimize care plans for individuals with cancer. Therefore, we examined the links between lower health-related fitness and worse patient-reported health and various step-based metrics. Methods The Alberta Moving Beyond Breast Cancer Study enrolled 1,528 women recently diagnosed with breast cancer and measured health-related fitness and patient-reported health outcomes near diagnosis, and one year later. Step counts and intensity (cadence, peak steps) were measured by activPAL® over seven days at baseline. We estimated cross-sectional associations (odds ratios (OR)) at baseline, and prospective associations between low baseline stepping and low fitness and poorer health at one year, adjusting for age, demographics, height, weight, and cancer diagnosis/treatment. Results At baseline 1,408 breast cancer survivors (mean age 56 yrs; early stage (90%)) provided valid activPAL measures (mean 5.5 days of wear). Taking &lt;5,000 steps/d (lower quintile) at baseline was associated with lower aerobic fitness, muscular strength and endurance, lower physical and mental quality of life, and greater fatigue and upper extremity disability at baseline and one year later. Taking &lt;5,000 steps/d at baseline was associated with greater risk of moving from favorable to unfavorable categories of aerobic fitness (OR = 2.64), curl-ups (OR = 1.84), chest endurance (OR = 2.38), self-reported health (OR = 2.37), physical quality of life (OR = 2.13), and fatigue (OR = 1.81) one year later. Preferred cadence and peak stepping were inconsistently associated after adjustment for total steps. Conclusions Although our findings need to be replicated, they suggest that simple step counts measured near diagnosis could help healthcare providers assess the fitness and health status of women recently diagnosed with breast cancer and improve their survivorship care plans."", ""Background: Current guidelines endorse the integration of exercise into cancer care. The diagnosis of cancer and its treatment, however, may introduce factors that make exercise engagement difficult, especially for individuals with advanced stages of disease. In this paper, we describe the baseline screening and triage process implemented for the Alberta Cancer Exercise (ACE) hybrid effectiveness-implementation study and share findings that highlight the multifaceted complexity of the process and the direct role of the clinical exercise physiologist (CEP). Methods: ACE was a hybrid effectiveness-implementation study examining the benefit of 12-week cancer-specific community-based exercise program. The ACE screening process was developed by integrating evidence-based guidelines with oncology rehabilitation expertise to ensure safe and standardized participation across cancer populations. The screening process involved four steps: (1) a pre-screen for high-risk cancers, (2) completion of a cancer-specific intake form and the Physical Activity Readiness Questionnaire for Everyone (PAR-Q+), (3) a CEP-led interview to further evaluate cancer status, cancer-related symptoms and other health issues (performed in-person or by phone), and (4) a baseline fitness assessment that included measurement of vital signs. Results: A total of 2596 individuals registered and underwent prescreening for ACE with 2570 (86.6%) consenting to participate. After full screening including the baseline fitness testing, 209 participants (8.1%) were identified as requiring further medical clearance. Of these, 191 (91.4%) had either a high-risk cancer, metastatic disease or were in the palliative end-stage of cancer, and 161 (84.3%) reported cancer-related symptoms potentially affecting their ability to exercise. In total, 806 (31.4%) participants were triaged to CEP-supervised in-person programming, 1754 (68.2%) participants to ACE community programming, and 8 (0.3%) specifically to virtual programming (post-COVID-19 option). Conclusions: The findings highlight the complexity and challenges of the screening and triage process, and the value of a highly trained CEP-led iterative approach that included the application of clinical reasoning."", ""Exercise is increasingly recognized by patients, clinicians, and allied health professionals globally as an important component of cancer care. In this paper, we provide a viewpoint on developments in exercise oncology over the past 4 decades leading up to the creation of the International Society of Exercise Oncology (ISEO). We briefly review research in adult and pediatric cancers from early foundation studies to larger randomized controlled trials published in mainstream oncology journals alongside critical work undertaken in exercise and cancer biological mechanisms. We also discuss potential strengths, weaknesses, opportunities, and threats facing ISEO in becoming a global forum for exercise oncology. Building on the foundational work undertaken over the past 4 decades by researchers, clinicians, and practitioners, ISEO provides an opportunity to support research, leverage collaborations and partnerships, facilitate education and training, increase awareness of exercise oncology, and support translation of research to clinical practice, ultimately improving the quality and quantity of life for people with cancer."", ""Exercise oncology is a multidisciplinary field that encompasses research across the translational continuum. Some of the major disciplines contributing to the field include biology, immunology, physiology, psychology, behavioral science, epidemiology, and clinical oncology. Here, we provide a brief overview of the field under the headings of preclinical studies, observational studies, interventional outcome studies, interventional behavioral studies, dissemination and implementation studies, and childhood cancer studies. Preclinical studies have generally demonstrated that exercise can reduce tumor growth, primarily by modulating the tumor microenvironment. Observational studies have generally demonstrated that higher postdiagnosis exercise is associated with lower rates of mortality, however, most studies have not considered the combination and sequencing of exercise with other cancer treatments. Interventional outcome studies have consistently demonstrated strong evidence that aerobic and/or resistance exercise have beneficial effects on fatigue, anxiety, depression, physical functioning, and quality of life in adult patients treated with curative intent. Childhood cancer studies have demonstrated beneficial effects on cardiorespiratory fitness and muscular strength; however, the quality of evidence is often low. Interventional behavioral studies have identified multiple effective exercise behavior change strategies, yet the evidence is limited by a lack of diversity, minimal attention to social determinants, and insufficient knowledge to tailor interventions. Dissemination and implementation studies are occurring globally, yet an evidence base identifying the most cost-effective, equitable, and sustainable strategies is limited. Notwithstanding substantial limitations and remaining research gaps, multidisciplinary exercise oncology research across the translational continuum has provided cancer patients with evidence-based recommendations for improving quality of life and possibly survival."", ""Abstract Numerous exercise oncology trials have been completed, greatly informing exercise recommendations for patients with cancer. Exercise medicine can be administered in various types, doses and schedules at various timepoints. Advancing precision exercise medicine requires understanding of how the effects of different exercise interventions vary by characteristics of individual patients. The Predicting OptimaL cAncer RehabilItation and Supportive care (POLARIS) study provides an international infrastructure and shared database to perform pooled analyses of individual patient data (IPD) from multiple randomised controlled trials. This commentary aims to highlight the value of pooled IPD analyses, summarize key findings from published pooled IPD analyses on the effects of physical exercise on various outcomes, and provide guidance to advance precision exercise medicine for patients with cancer. POLARIS currently includes IPD from 52 exercise trials. Findings to date indicate that exercise interventions in patients with cancer have beneficial effects on physical fitness, fatigue, health-related quality of life (HRQoL), self-reported cognition (post treatment), sleep disturbances, and symptoms of anxiety and depression. Additionally, it was determined that the exercise effects varied by characteristics of the patients, including the initial value of the outcome, age, marital status and education level, and by characteristics of the intervention, including exercise supervision and specificity. Future research opportunities to advance precision exercise medicine for patients with cancer include pooling of trial data from understudied populations, data on clinical outcomes and biomarkers, as well as applying machine learning models for identifying combinations of covariables that modify intervention effects and predictions of individual treatment effects.""]"
"T Arndt","","","Computer Science","https://openalex.org/A5055204464","[]"
"François R. Herrmann","","","Computer Science","https://openalex.org/A5034367999","[""Background/Objectives: The prevalence of diabetes in very old people is rising sharply worldwide, due not only to obesity, nutritional and sedentary lifestyles, but also to aging per se. Diabetes is associated with a higher incidence of sarcopenia, malnutrition and physical disabilities. However, many age-specific issues in the clinical management of very old diabetic patients remain unstudied. Methods: This is a case–control prospective study including 162 very old hospitalized diabetic patients and 301 controls. We explored the impact of diabetes on the prevalence of sarcopenia according to the EWGSOP2 criteria, using Jamar handgrip to assess muscle strength, BIA-derived fat-free mass index to assess muscle mass, and the timed up and go test to assess physical performance. We also explored factors associated with sarcopenia in both groups in multiple logistic analysis. Results: Mean age was 84.8 ± 6.0 years. We found a prevalence of sarcopenia of 8.0% and 16.7% in the diabetic and the control groups, respectively (p = 0.010). BMI was independently associated with sarcopenia in both groups, explaining 25% of the model in the diabetic group and 33% of the model in the control group. Conclusions: Sarcopenia was less prevalent in diabetic hospitalized older patients than in other patients, indicating that old frail patients are not the same patients as those that are in epidemiological studies on sarcopenia in diabetes. These results should be confirmed in further studies."", ""Introduction Hippocampal volume loss occurs physiologically with age, but an accelerated rate of volume loss is linked to neurodegenerative diseases. While evidence suggests that cross-sectional study designs tend to underestimate hippocampal atrophy rates compared to longitudinal approaches, few studies have directly examined the relationship between these two methods in the context of brain aging. This study aims to investigate the association between baseline hippocampal z-scores and hippocampal volume loss over time in a cohort of healthy older adults. Methods 182 healthy elderly subjects (mean age: 73.4 ± 3.5 years) who underwent structural Magnetic resonance imaging (MRI) at two timepoints (mean time between the scans 4.8 ± 1.0 years) were included. A subset of participants ( n = 103) also completed Positron emission tomography (PET) amyloid imaging. Hippocampal volumes were measured at baseline and follow-up using FreeSurfer (v7.1.1). Baseline volumes were adjusted for age and intracranial volume (ICV) and converted into z-scores. The annualized percent change (APC) in hippocampal volume was calculated for each participant. Neuropsychological assessments were conducted at baseline, 18, and 54 months, and APOE genotyping was performed. Correlation analyses examined the relationship between baseline hippocampal volumes and APC, while multiple regression models explored potential influencing factors. Results Hippocampal volumes decreased from baseline to follow-up [mean APC (SD): right −1.34% (0.94), left: −1.79% (1.00)]. Small, but statistically significant positive correlations were found between baseline hippocampal z-scores and APC of hippocampal volumes over time, indicating that the lower the volume at baseline, the greater the atrophy rate to timepoint two (right hippocampus: r = 0.17, p = 0.01; left hippocampus: r = 0.14, p = 0.03). No covariates significantly influenced this association ( p &amp;gt; 0.05). Conclusion Lower baseline hippocampal z-scores are associated with a greater rate of hippocampal atrophy to the follow-up examination. If validated in larger cohorts, these findings could help establish cut-off values for pathological atrophy in cross-sectional studies."", ""People tend to consider others' perspective when judging their own (altercentric interference, AI) or other (egocentric interference, EI) divergent views. Borderline (BDL) and antisocial personalities are associated with significant changes in EI and AI. Combining the dot perspective-taking task with high-density EEG recordings, our study explores the correlations between EI and AI in cases with BDL diagnosis and court-ordered measures (BDL-COM; n = 14) compared to age-matched healthy controls (n = 24). In Inconsistent trials, controls displayed significant activation of brain generators, which was absent in BDL-COM patients. For the Self-Inconsistent stimuli (altercentric bias), controls showed increased activity in the left superior frontal gyrus between 58 and 74 ms and the left inferior frontal gyrus between 279 and 303 ms. Similar differences were observed for Other-Inconsistent stimuli (egocentric bias) in the precentral gyri and inferior frontal gyrus between 274 and 296 ms. These findings suggest that AI involves an early activation of brain generators in central executive and mentalizing areas. EI is associated with an increased activation of the mirror neuron system based on self-other distinction. These EEG data indicate that BDL-COM patients display significant difficulties activating all of the brain generators involved in the processing of conflicting viewpoints in visual perspective-taking."", ""Obesity, defined by a body mass index (BMI) ≥ 30 kg/m2, is associated with higher mortality in the general population but shows a complex relationship with chronic diseases and critically illness. The aim of this study was to determine whether BMI predicted 30-day mortality (primary outcome) and intensive care unit (ICU) and hospital lengths of stay (LOS) (secondary outcome) in critically ill patients. This retrospective monocentric study encompassed adult patients admitted to the ICU of the Geneva University Hospitals between January 2010 and December 2022. They were categorized according to their BMI as underweight (<18.5 kg/m2), normal weight (18.5-24.9 kg/m2, reference category), overweight (25.0-29.9 kg/m2), obesity class I (30.0-34.9 kg/m2), class II (35.0-39.9) and class III (≥40.0 kg/m2). The association between BMI and outcomes was assessed by multivariate Cox Lasso (Least absolute shrinkage and selection operator) or linear Lasso regression models, adjusted for age, sex, Simplified Acute Physiology Score (SAPS) II, primary diagnosis (ICD-10 codes) and measurement time points. We included 5473 patients (34.2 % women), age 60 ± 17.6 years, SAPS II 49.6 ± 19.9, BMI 26.5 ± 6.0 kg/m2, admitted to the ICU mainly for trauma, cardio-vascular and infectious diseases. The 30-day mortality rate was 21.4 % for women and 22.1 % for men. In multivariate analyses, mortality risk, expressed as hazard ratio (HR), was lower in patients with a BMI of 35.0-39.9 kg/m2 (HR 0.72, p = 0.020) and higher in those with a BMI <18.5 kg/m2 (HR 1.44, p = 0.006) compared to normal weight. BMI categories in the overweight, obesity class I and class II range were all associated with longer ICU and hospital LOS (all p < 0.05). Patients with higher BMI, particularly with class II obesity, show a lower 30-day mortality in ICU settings, but a longer ICU and hospital LOS. This is likely a reflection of the obesity paradox and the complex role of body composition in critical illness outcomes. clinicaltrials.gov, identifier: NCT05834894."", ""Low skeletal mass, often present at hospital admission, has been associated with poor prognoses. To explore the association between computed tomography (CT)-derived skeletal muscle mass at the lumbar level and short- and long-term mortality in critically ill patients. Following PRISMA 2020 guidelines, we included studies on critically ill adults (≥ 18 years) hospitalized in intensive care units (ICU) that measured CT-derived skeletal muscle mass at the lumbar vertebral level within ± 7 days of ICU admission. The primary outcome was mortality, categorized as short-term (including ICU, hospital, 28- and 30-day mortality) and long-term (> 30 days) mortality. MEDLINE and Embase databases were searched without date restrictions. Study screening was performed using Rayyan, data extraction was guided by a custom-designed tool, and quality assessment was performed using the JBI Cohort Study Checklist. A meta-analysis was conducted, focusing on studies that reported short- and long-term mortality among patients with preserved and reduced skeletal muscle. A prevalence meta-analysis was also performed for studies that reported the size of subgroups with low muscle mass. Out of 1248 unique records, 35 studies met the inclusion criteria, involving 9366 participants. The majority were retrospective, single-centre studies conducted on four continents and included heterogeneous populations such as patients with sepsis, COVID-19 and trauma. Sample sizes ranged from 36 to 939, with a wide age range, from 40 to 70 s, and a predominance of male patients (62%). Skeletal mass was most commonly reported as skeletal muscle index at the third lumbar vertebra. Studies reported mainly short-term mortality on day 28 or 30. Long-term mortality, measured at 90 days, 6 months, and 1 year, was evaluated in 11 studies. Meta-analyses revealed that low skeletal muscle mass area and index were significantly associated with increased risks of both short (OR = 2.33, CI 1.90-2.87, I2 = 41.39%)-and long-term mortality (OR = 2.67, CI 1.45-4.92, I2 = 62.24%). The overall prevalence of low muscle mass was 42% (CI 34-49%, I2 = 98.2%). CT-assessed skeletal muscle mass at the lumbar level on admission to ICU is associated with both short- and long-term mortality. It may serve as a prognostic marker in critically ill patients. Standardized protocols for measuring and defining low skeletal muscle mass in this population are essential to improve comparability across studies.""]"
"Peter Svensson","","","Computer Science","https://openalex.org/A5043756816","[""In this narrative review, the authors aimed to provide a focused overview, grounded in scientific literature, of the most common primary and secondary headaches frequently observed in patients with orofacial pain as well as orofacial conditions that may mimic primary headache disorders. In addition, they highlighted the clinically significant overlap between headaches and temporomandibular disorders (TMDs). Information was sourced from the International Classification of Headache Disorders, Third Edition, for headache diagnoses and from the International Classification of Orofacial Pain for orofacial pain diagnoses. Management guidelines were from the European Academy of Neurology. Data on the overlap between headache disorders and TMDs were drawn from a systematic review and observational studies. The authors provided a concise, practical, clinical guide for identifying and managing primary and secondary headaches commonly encountered in dental practice, which is grounded in established guidelines and robust scientific evidence. The authors emphasized the importance of interdisciplinary management and a thorough diagnostic approach to enhance treatment outcomes for patients with concurrent headaches and TMD, highlighting the frequent overlap of these disorders. For dental care clinicians and other health care practitioners, recognizing the intersection of orofacial pain, headaches, and TMD is crucial. This understanding promotes better diagnosis, encourages interdisciplinary collaboration, and is the best approach to improved patient care and treatment outcomes."", ""Abstract The aim of this long-term follow-up study was to investigate the relationship between bite force, bruxism, and fractures of teeth and veneer porcelain of fixed dental prostheses. Patients previously assessed as probable bruxers (n = 30) and non-bruxers (n = 21), all rehabilitated with dental implant-supported restorations, underwent a clinical examination and measurement of maximum bite force. A univariate general linear model was used to compare regression lines showing the relationship between fractures and bite force. Bruxers had significantly higher maximum bite force ( p = 0.023) and higher proportion of tooth/veneer porcelain fractures per total number of tooth/prosthetic units ( p = 0.045). There was no significant difference in the relationship between frequency of tooth/veneer porcelain fractures and maximum bite force between probable bruxers and non-bruxers ( p = 0.054). However, there was a significant difference between probable bruxers and non-bruxers when the percentage of fractures in relation to the total number of units was considered instead of the frequency of fractures ( p = 0.035). Higher maximum bite force in probable bruxers was related to higher prevalence of fractures of teeth and veneer porcelain, emphasizing the potential benefits of pre-treatment assessment of bruxism as well as bite force. Easy-to-use reliable clinical methods for bite force measurement should be tested and implemented in dental practice."", ""Bruxism is receiving increasing attention from both clinicians and researchers over the past decades. Recently, it has become clear that some aspects of the currently proposed, expert-driven bruxism definitions raise questions and cause confusion among clinicians, researchers, educators and patients. The aim of this report is threefold: (1) to provide the reader with a glossary of the existing definitions, (2) to discuss frequently asked questions regarding these definitions and (3) to suggest a road map for the next steps to be taken towards a better understanding of bruxism. A closed (invitation-only) full-day workshop at the 2024 General Session & Exhibition of the International Association for Dental, Oral and Craniofacial Research (IADR) convened international bruxism experts to discuss the current definitions. Insights from these discussions were compiled, analysed and summarised. The present report provides a glossary of the constituent terms of the currently proposed definitions, an overview of the frequently asked questions and insights into the next steps to be taken. By current consensus and to avoid any further confusion, the addendum 'in otherwise healthy individuals' has been removed from the specific definitions of sleep and awake bruxism. In addition, the grading system's hierarchical organisation, as proposed previously, was revised and clarified, proposing the inclusion of terms based on self-reporting, clinical examination and device-based assessment tools. To ascertain that we all use the same terminology, we recommend using the current publication when referring to the definitions of bruxism and its constituent terms.""]"
"R. Poettgen","","","Computer Science","https://openalex.org/A5107839818","[""Baryon number violation (BNV) in R-parity violating (RPV) supersymmetry is studied with a focus on Delta B = 2 processes which allow neutron–anti-neutron oscillations. Simplified RPV-SUSY models, including only the relevant superpartners and couplings, are considered. Constraints from flavour physics, searches at the Large Hadron Collider and searches at dedicated BNV experiments are quantified for the various scenarios at the TeV scale. It is also shown that a proposed neutron oscillation experiment at the European Spallation Source has a sensitivity to a mass scale for new physics that goes beyond all other experiments and up to the PeV scale for certain regions of parameter space."", ""Baryon number violation (BNV) in R-parity violating (RPV) supersymmetry is studied with a focus on ΔB = 2 processes which allow neutron-anti-neutron (n-n) oscillations. Simplified RPV-SUSY models, including only the relevant superpartners and couplings, are considered. Constraints from flavour physics, searches at the Large Hadron Collider and searches at dedicated BNV experiments are quantified for the various scenarios at the TeV scale. It is also shown that a proposed n-n experiment at the European Spallation Source has a sensitivity to a mass scale for new physics that goes beyond all other experiments and up to the PeV scale for certain regions of parameter space."", ""For the next run of the LHC, the ATLAS Level-1 trigger system will include topological information on trigger objects from the calorimeters and muon detectors. In order to supply coarse grained muon topological information, the existing MUCTPI (Muon-to-Central-Trigger-Processor Interface) system has been upgraded. The MIOCT (Muon Octant) module firmware has been then modified to extract, encode and send topological information through the existing MUCTPI electrical trigger outputs. The topological information from the muon detectors will be sent to the Level-1 Topological Trigger Processor (L1Topo) through the MUCTPI-to-Level-1-Topological-Processor (MuCTPiToTopo) interface. Examples of physics searches involving muons are: search for Lepton Flavour Violation, Bs-physics, Beyond the Standard Model (BSM) physics and others. This paper describes the modifications to the MUCTPI and its integration with the full trigger chain."", ""An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures."", ""An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures.""]"
"Peter Lindqvist","","","Computer Science","https://openalex.org/A5029991391","[""<title>Abstract</title> Magnetic reconnection, a fundamental energy conversion process, underpins a multitude of eruptive phenomena across the universe. Compared to the traditional standard magnetic reconnection with ion coupling, the recently discovered electron-only magnetic reconnection operates at strikingly diminutive spatial scales, imposing formidable observational constraints on resolving its intrinsic physical processes, most critically within its core region—near the X-line. Consequently, the intrinsic mechanism governing electron-only magnetic reconnection remains largely enigmatic. Leveraging high-resolution data from NASA’s MMS mission, we present unprecedented observations of two types of whistler waves in electron-only magnetic reconnection: right-handed whistler waves near the X-line, excited by perpendicular anisotropy electron distributions via second order cyclotron resonance, contrasted by left-handed whistler waves in the outflow jet. The outflow-associated whistler waves reside within a magnetic hole coupled to the Hall magnetic field. Beyond these distinct features diverging from standard magnetic reconnection, hallmark phenomena analogous to standard magnetic reconnection—specifically flux pileup region in the outflow—is also identified for the first time in this electron-only regime. Our observations may provide novel insights into the magnetic reconnection and the microscale dynamics in space and astrophysical plasmas."", ""Abstract Dipolarization events with inductive, radial electric fields are investigated with multi‐spacecraft analysis techniques. Observations by Magnetospheric Multiscale with separations around ion scales are used to study spatial and temporal variations of these events in the inner magnetosphere. force, magnetic pressure force, and tension force are compared based on the Taylor expansion method, which includes the curlometer technique. The magnetic pressure force, possibly related to energetic particles and magnetic flux transported from the magnetotail, tends to contribute to the force for the background components near the equator, while the tension force, related to Alfvén waves, contributes to the fluctuating components or outside the equator. The scale length is thousands of km for the background components, probably related to meso‐scale structures, while that length is tens to a thousand km for fluctuating components. The small‐scale fluctuations would be related to particle acceleration. The fluctuations are inferred to be Alfvén waves with quasi‐perpendicular propagation directions and with finite temperature or kinetic effects. These fluctuations could be intermittent and not self‐similar between different inter‐spacecraft distances. Some energy transfer from fluctuations to particles would occur."", ""Abstract Wave‐particle interactions are essential for energy transport in the magnetosphere. In this study, we investigated an event during which electrons interact simultaneously with waves in different scales, using data from the Magnetospheric Multiscale mission. At the macroscale ( km), drift resonance between ultra‐low frequency (ULF) waves and 70–300 keV electrons is observed. At the microscale ( km), lower‐band chorus waves and electron cyclotron harmonic (ECH) waves are alternately generated, showing signatures of modulation by ULF waves. We found that compressional ULF waves affect the temperature anisotropy of 1–10 keV electrons, thereby periodically exciting chorus waves. Through linear instability analysis, we propose that ULF waves modulate ECH wave emissions by regulating the gradient of electron phase space density at the edge of the loss cone. Our results enhance the understanding of cross‐scale wave‐particle interactions, highlighting their importance in magnetospheric dynamics."", ""<title>Abstract</title> Chorus waves are electromagnetic emissions widely occurring in planetary (e.g., Earth, Mars, and Jupiter) environments and can cause hazardous space weather effects which threaten human safety and man-made electromagnetic devices. For decades, chorus waves have been intensively investigated, both experimentally and theoretically, in the context of planetary dipolar fields. However, the underlying electron dynamics which governs the wave generation and evolution, has not been well diagnosed by <italic>in situ</italic> observations, due to the limited capacity of previous spacecraft missions in geospace. Here we report unexpected observations of repetitive, falling-tone chorus waves in the terrestrial neutral sheet. Using unprecedently high-resolution data from NASA’s MMS mission, we present ultrafast measurements of electron dynamics and electric current within the waves, finding energy transfer from local electrons to the waves and development of electron hill in the wave phase space. The results provide smoking-gun evidence for the nonlinear generation mechanism for falling-tone chorus waves, and reveal that magnetic field inhomogeneity, which has been traditionally believed to be crucial for chorus generation, is not necessarily required. The current observations provide important new insights into understanding energetic electron dynamics in space."", ""Abstract High‐speed electron flows (HSEFs) play a crucial role in the energy dissipation and conversion processes within the terrestrial magnetosphere and can drive various types of plasma waves and instabilities, affecting the electron‐scale dynamics. The existence, spatial distribution, and general properties of HSEFs in the Earth magnetotail are still unknown. In this study, we conduct a comprehensive survey of HSEFs in the Earth magnetotail, utilizing NASA's Magnetospheric Multiscale (MMS) mission observations from 2017 to 2021. A total of 642 events characterized by electron bulk speeds exceeding 5,000 km/s are identified. The main statistical properties are: (a) The duration of almost all HSEFs are less than 4 s, and the average duration is 0.74 s. (b) HSEFs exhibit a strong dawn‐dusk (30%–70%) asymmetry. (c) 39.6%, 29.0%, and 31.4% of the events are located in the plasma sheet, plasma sheet boundary layer (PSBL), and lobe region, respectively. (d) In the plasma sheet, HSEFs have arbitrary moving directions regarding the ambient magnetic field, and the events near the neutral line predominantly move along the same direction as the ion outflows, indicating outflow electrons generated by magnetic reconnection. (e) HSEFs in the PSBL and lobe mainly move along the ambient magnetic field, and 70% of HSEFs in the PSBL exhibit features of reconnection inflow. The HSEFs in lobe regions may locate near the reconnection electron edges. Our study reveals that the HSEFs in magnetotail are closely associated with magnetic reconnection, and the statistical results deepen the understanding of HSEF fundamental properties in collisionless plasma.""]"
"Sverker Sörlin","","","Computer Science","https://openalex.org/A5015853724","[""Abstract This article presents a new way of understanding Global Environmental Governance (GEG), historically and functionally. We outline a revised analytical framing, which connects the post-WWII moment of early globalizing conservation with the intensifying attempts to govern the human-earth relationship through an ever-growing assemblage of governable environmental objects and their quantifiable indicators as proxies . Our argument is as follows: (1) GEG has followed a trajectory of dispersal of actors, institutions, conceptual tools and responsibilities from the micro- and local scales to the planetary. We analyze how these trajectories unfold in three essential domains: Earth System science, sovereignty, and neoliberalization. (2) GEG is performative . The governance itself has created the dynamic environmental objects under governance. (3) In this way, GEG has normalized the environment as a policy object."", ""Global Environmental Governance (GEG) has been a growing phenomenon since the middle of the 20th century, although the concept itself and its acronym are more recent and were in fact rarely used before 2000. The early interest in GEG was much preoccupied with environmental diplomacy and international legal agreements from the Stockholm UN conference 1972 onwards. The addition of the ‘global’ reflected the general rise in awareness of the significance of globalization since the 1980s. The further growth, and the transformation of GEG, in earnest since the Millennium has been increasingly marked by yet another category, ‘the planetary’, mirroring the increasing influence of Earth System Science on environmental and climate discourse. This has affected both GEG and ‘the environment’ itself. The conceptual shifts, including the rising interest in the Anthropocene, reflected profound changes in the human-Earth relationship. To analyze these shifts, which is the aim of this paper, I will use the ‘Planetary Boundaries’-concept, launched in a highly cited paper in the journal Nature in 2009 and further developed in later publications, seminally in 2015 and 2023. I will consider the PB-concept, and versions of the often shown diagram that accompanied it, as both a case of ‘planetary modelling’ and, at the same time an ‘environing technology’ that is performative and shapes what the environment ‘is’ while modelling it. I also emphasize that the PB-model has led a ‘social life’, which has defined its continued evolution as a key agency in the formation of an Anthropocene Weltanschauung, with normative properties. I posit that the model helped shape the shift towards the epistemic and temporal ‘environment’ that has become mainstreamed in the UN Sustainable Development Goals and other goal setting projects.""]"
"M. Wolke","","","Computer Science","https://openalex.org/A5110067108","[""The European Spallation Source, currently under construction in Lund, Sweden, is a multidisciplinary international laboratory. Once completed to full specifications, it will operate the world’s most powerful pulsed neutron source. Supported by a 3 million Euro Research and Innovation Action within the EU Horizon 2020 program, a design study (HighNESS) has been completed to develop a second neutron source located below the spallation target. Compared to the first source, designed for high cold and thermal brightness, the new source has been optimized to deliver higher intensity, and a shift to longer wavelengths in the spectral regions of cold (CN, 2–20 Å), very cold (VCN, 10–120 Å), and ultracold (UCN, >500 Å) neutrons. The second source comprises a large liquid deuterium moderator designed to produce CN and support secondary VCN and UCN sources. Various options have been explored in the proposed designs, aiming for world-leading performance in neutronics. These designs will enable the development of several new instrument concepts and facilitate the implementation of a high-sensitivity neutron-antineutron oscillation experiment (NNBAR). This document serves as the Conceptual Design Report for the HighNESS project, representing its final deliverable."", ""A key aim of the HighNESS project for the European Spallation Source is to enable cutting-edge particle physics experiments. This volume presents a conceptual design report for the NNBAR experiment. NNBAR would exploit a new cold lower moderator to make the first search in over thirty years for free neutrons converting to anti-neutrons. The observation of such a baryon-number-violating signature would be of fundamental significance and tackle open questions in modern physics, including the origin of the matter-antimatter asymmetry. This report shows the design of the beamline, supermirror focusing system, magnetic and radiation shielding, and anti-neutron detector necessary for the experiment. A range of simulation programs are employed to quantify the performance of the experiment and show how background can be suppressed. For a search with full background suppression, a sensitivity improvement of three orders of magnitude is expected, as compared with the previous search. Civil engineering studies for the NNBAR beamline are also shown, as is a costing model for the experiment."", ""Tests of the T, CP and CPT symmetries in the neutral kaon system are performed by the direct comparison of the probabilities of a kaon transition process to its symmetry-conjugate. The exchange of in and out states required for a genuine test involving an antiunitary transformation implied by time-reversal is implemented exploiting the entanglement of K0K‾0 pairs produced at a ϕ-factory. A data sample collected by the KLOE experiment at DAΦNE corresponding to an integrated luminosity of about 1.7 fb−1 is analysed to study the Δt distributions of the ϕ→KSKL→π+π−π±e∓ν and ϕ→KSKL→π±e∓ν3π0 processes, with Δt the difference of the kaon decay times. A comparison of the measured Δt distributions in the asymptotic region Δt≫τS allows to test for the first time T and CPT symmetries in kaon transitions with a precision of few percent, and to observe CP violation with this novel method."", ""The NNBAR experiment for the European Spallation Source will search for free neutrons converting to antineutrons with an expected sensitivity improvement of three orders of magnitude compared to the last such search. This paper describes both the simulations of a key component for the experiment, the neutron optical reflector and the expected gains in sensitivity."", ""A bstract The ratio $$ \\mathcal{R} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mi>R</mml:mi> </mml:math> = Γ( K S → πeν ) / Γ( K S → π + π − ) has been measured with a sample of 300 million K S mesons produced in ϕ → K L K S decays recorded by the KLOE experiment at the DAΦNE e + e − collider. K S → πeν events are selected by a boosted decision tree built with kinematic variables and time-of-flight measurements. Data control samples of K L → πeν decays are used to evaluate signal selection efficiencies. With 49647 ± 316 signal events we measure $$ \\mathcal{R} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mi>R</mml:mi> </mml:math> = (1 . 0421 ± 0 . 0066 stat ± 0 . 0075 syst ) × 10 − 3 . The combination with our previous measurement gives $$ \\mathcal{R} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mi>R</mml:mi> </mml:math> = (1 . 0338 ± 0 . 0054 stat ± 0 . 0064 syst ) × 10 − 3 . From this value we derive the branching fraction $$ \\mathcal{B} $$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:mi>B</mml:mi> </mml:math> ( K S → πeν ) = (7 . 153 ± 0 . 037 stat ± 0 . 044 syst ) × 10 − 4 and f + (0) |V us | = 0 . 2170 ± 0 . 009.""]"
"M. Ughetto","","","Computer Science","https://openalex.org/A5103120392","[""The limited amount of data available renders it challenging to characterize which biological processes are relevant to a rare disease. Hence, there is a need to leverage the knowledge of disease pathogenesis and treatment from the wider disease landscape to understand rare disease mechanisms. Furthermore, it is well understood that rare disease discoveries can inform the our knowledge of common diseases. In this paper, we introduce Dis2Vec (Disease to Vector), a new representation learning method for characterizing diseases with a focus on learning the underlying biological mechanisms, which is a step toward developing a foundation model for disease-association learning. Dis2Vec is trained on human genetic evidence and observed symptoms, and then evaluated through cross-modal transfer-learning scenarios based on a proposed drug association learning benchmark with drug targets (positive controls) and Orphanet Rare Disease Ontology (negative controls). Finally, we argue that clustering diseases in the Dis2Vec space, which captures biological mechanisms instead of drug-repurposing information, could increase the efficiency of translational research in rare and common diseases, and ultimately improve treatment strategies for patients."", ""Gold nanoparticles are used in a range of applications, but their properties depend on their shape, size, and polydispersity. A quick, easy, and accurate characterization of the particles is therefore of high importance, especially in flow synthesis settings where continuous monitoring of the characteristics is desired. Our hypothesis was that convolutional neural networks can be used to extract detailed information about structural parameters of gold nanoparticles from their UV–vis spectra, and we have shown that this is possible by predicting size distributions from in silico UV–vis spectra for colloidal gold with high accuracy. Here this was done for both spherical and rod-shaped gold nanoparticles. We also show that the addition of noise makes the prediction of diameter polydispersity more challenging, but the average diameter, and for rods also aspect ratio distribution, can be accurately predicted even with the highest evaluated level of noise. The model structure is promising and worthy of implementation to enable predictions beyond in silico generated spectra. The model, for instance, can find application in flow synthesis settings to create a machine learning-driven feedback loop for automated synthesis."", ""ABSTRACT Machine learning applications for the drug discovery pipeline have exponentially increased in the last few years. An example of these applications is the biological Knowledge Graph. These graphs represent biological entities and the relations between them based on existing knowledge. Graph machine learning models such as Graph Neural Networks can be applied on top of knowledge graphs to support the development of novel therapeutics. Nevertheless, Graph Neural Networks present an improved performance at the expense of complexity, becoming difficult to explain their decisions. State-of-the-art explanation algorithms for Graph Neural Networks focus on determining the most relevant subgraphs involved in their decision-making while considering graph elements (nodes and edges) as independent entities and ignoring any communities these graphs could present. We explore in this work the idea that graph community structure in biological Knowledge Graphs could provide a better grasp of the decision-making of Graph Neural Networks. For that purpose, we introduce XP-GNN , a novel explanation technique for Graph Neural Networks in Knowledge Graphs. XP-GNN exploits the communities of nodes or edges in graphs to refine their explanations, inspired by cooperative game theory . We characterize XP-GNN in a basic example and in terms of scalability and stability. In two relevant use cases for the drug discovery pipeline, XP-GNN provides more relevant explanations than previous techniques, being evaluated quantitatively and by domain experts. At the same time, XP-GNN presents limitations on scalability and stability, which we will address. ACM Reference Format Andrés Martínez Mora, Dimitris Polychronopoulos, Michaël Ughetto, and Sebastian Nilsson. 2024. Community-aware explanations in knowledge graphs with XP-GNN. In Proceedings of ACM Conference (Conference’17) . ACM, New York, NY, USA, 21 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn This work has been funded by AstraZeneca AB, Mölndal, Sweden and AstraZeneca Cambridge. Unfortunately, due to proprietary reasons from AstraZeneca AB, the data used in this work cannot be shared ."", ""In this paper, we introduce ChemicalX, a PyTorch-based deep learning library designed for providing a range of state of the art models to solve the drug pair scoring task. The primary objective of the library is to make deep drug pair scoring models accessible to machine learning researchers and practitioners in a streamlined framework. The design of ChemicalX reuses existing high level model training utilities, geometric deep learning, and deep chemistry layers from the PyTorch ecosystem. Our system provides neural network layers, custom pair scoring architectures, data loaders, and batch iterators for end users. We showcase these features with example code snippets and case studies to highlight the characteristics of ChemicalX. A range of experiments on real world drug-drug interaction, polypharmacy side effect, and combination synergy prediction tasks demonstrate that the models available in ChemicalX are effective at solving the pair scoring task. Finally, we show that ChemicalX could be used to train and score machine learning models on large drug pair datasets with hundreds of thousands of compounds on commodity hardware.""]"
"Maria Strömme","","","Computer Science","https://openalex.org/A5037359873","[""Covalent organic framework (COF) membranes hold significant promise for applications in separation, catalysis, and energy conversion; however, their industrial adoption has been hindered by the lack of scalable and efficient fabrication methods. Here, we present a fast, versatile, and broadly applicable strategy for fabricating free-standing and flexible COF membranes by casting precursor suspensions, followed by heat treatment under controlled humidity. This approach enables the fabrication of COF membranes with lateral dimensions up to several square decimeters and thicknesses that are tunable down to submicron levels within 1 h. It demonstrates remarkable versatility for producing a family of ketoenamine-linked COF membranes through the condensation of 1,3,5-triformylphloroglucinol with various amine monomers differing in length, side groups, and geometry. The resulting crack-free COF membranes exhibit high mechanical strength, with ultimate tensile strength up to 60 MPa and Young's modulus up to 1.7 GPa, as well as exceptionally high porosity, with Brunauer-Emmett-Teller (BET) surface areas reaching up to 2226 m2 g-1. More importantly, the morphology, porosity, and crystallinity of the membranes can be finely tuned by modulating the heating conditions. The membranes with optimized microstructures demonstrate excellent separation performance, achieving over 99% rejection in nanofiltration of aqueous dye solutions, and a separation factor of 11 with an H2 permeance of 2857 GPU in H2/CO2 gas separation. This approach provides a scalable and effective pathway toward large-scale COF membrane manufacturing for advanced molecular separations and other membrane-based technologies."", ""Lignin is an aromatic biomacromolecule with many promising properties that can be beneficial to polymer blends. The main objective of this work was to investigate the processability, compatibility, and recyclability of lignin blends with poly(lactic acid). Two different commercial kraft lignins and a phenolated organosolv lignin were blended with poly(lactic acid) at various weight percentages, targeting high lignin content (30, 50, and 70 wt %). Obtained blends were used in additive manufacturing via fused deposition modeling. All obtained materials were thoroughly characterized by tensile tests, thermogravimetric analysis, differential scanning calorimetry, and 31P NMR. The recyclability of the polymer blend materials was evaluated by re-extruding them up to four times, and their printability was also assessed. The results showed that the material retained its mechanical properties relatively well for up to three cycles after which its tensile strength decreased by 30%. Phenolated organosolv lignin exhibited better printability across a broader range of lignin content compared to kraft lignin analogs while maintaining similar thermal and mechanical properties.""]"
"B. Lund-Jensen","","","Computer Science","https://openalex.org/A5072437032","[""A summary is presented of ATLAS searches for gluinos and first- and second-generation squarks in final states containing jets and missing transverse momentum, with or without leptons or b-jets, in the root s = 8 TeV data set collected at the Large Hadron Collider in 2012. This paper reports the results of new interpretations and statistical combinations of previously published analyses, as well as a new analysis. Since no significant excess of events over the Standard Model expectation is observed, the data are used to set limits in a variety of models. In all the considered simplified models that assume R-parity conservation, the limit on the gluino mass exceeds 1150 GeV at 95% confidence level, for an LSP mass smaller than 100 GeV. Furthermore, exclusion limits are set for left-handed squarks in a phenomenological MSSM model, a minimal Supergravity/Constrained MSSM model, R-parity-violation scenarios, a minimal gauge-mediated supersymmetry breaking model, a natural gauge mediation model, a non-universal Higgs mass model with gaugino mediation and a minimal model of universal extra dimensions."", ""The Sileye3/Alteino experiment is devoted to the investigation of the light flash phenomenon and particle composition of the cosmic ray spectrum inside the ISS. The particle detector is a silicon telescope consisting of eight planes, each divided into 32 strips. Data acquisition was initiated in 2002 in the Russian Pirs module. The data on nuclei from C to Fe in the energy range above about 60 MeV/n presented here were taken as part of the ESA Altcriss project [ 1] from late 2005 through 2007. Here we report on LET, from different locations and orientations, in both the Pirs and Zvezda modules. Taking solar modulation into account the results are in agreement with ALTEA measurements from USLab [ 2]. To convert the energy deposition in Si to the equivalent in water, the logarithmic relation between LET in Si and water adopted from [ 3]. In Fig. 1, the LET spectra in water for Alteino and ALTEA are compared with DOSTEL spectrum from 2001 [ 4], and we see a good overall agreement. We are currently in the process of preparing a detailed paper on the dose and dose equivalent rates in different places inside the Zvezda and Pirs modules and a novel analysis of the contribution to the different doses as a function of strip hit multiplicity.Fig. 1.Open in new tabDownload slideLET spectra in water from Alteino (red triangle), ALTEA (black diamond) [ 2], DOSTEL 2001 (solid line) [ 4]."", ""Rapidity gap cross sections measured with the ATLAS detector in pp collisions at sqrt(s) = 7 TeV"", ""Search for excited leptons in proton-proton collisions at sqrt(s) = 7 TeV with the ATLAS detector"", ""The ATLAS detector has been built to study the reactions produced by the Large Hadron Collider (LHC). ATLAS includes a system of liquid argon calorimeters for energy measurements. The electronics for amplifying, shaping, sampling, pipelining, and digitizing the calorimeter signals is implemented on a set of front-end electronic boards. The front-end boards are installed in crates mounted between the calorimeters, where they will be subjected to significant levels of radiation during LHC operation. As a result, all components used on the front-end boards had to be subjected to an extensive set of radiation qualification tests. This paper describes radiation-tolerant designs, radiation testing, and radiation qualification of the front-end readout system for the ATLAS liquid argon calorimeters.""]"
"Ingmar Skoog","","","Computer Science","https://openalex.org/A5058286274","[""Glial fibrillary acidic protein (GFAP) is a well-established biomarker of astrocytic activation associated with neurodegenerative diseases, neuroinflammatory disorders, and traumatic brain injury. With increasing interest in blood-based biomarkers, the need for analytically validated assays and reliable reference intervals is critical for routine clinical implementation. This study aimed to analytically validate the MSD S-Plex® GFAP immunoassay for plasma and to establish age-stratified reference intervals in an apparently healthy population. This study was conducted in two phases. First, key analytical validation parameters - including repeatability, intermediate precision, measurement range, interferences, and sample stability - were evaluated following Clinical and Laboratory Standards Institute (CLSI) and published protocol guidelines. Second, reference intervals were derived from 579 apparently healthy individuals aged 17-91 years using a right-sided non-parametric percentile method. Age-specific upper reference limits were calculated for three predefined age groups, and a continuous age-dependent centile model was applied. MSD S-Plex® GFAP assay demonstrated strong analytical performance, with coefficients of variation for repeatability and intermediate precision below 12 %. After accounting for the 1:2 dilution ratio, the validated measurement range was 0.425-1760 ng/L, with all calibration residuals remaining within ±15 %. GFAP concentrations were unaffected by hemolysis (p=0.85) and remained stable for up to 7 days at 4 °C and under frozen storage conditions. Age-stratified upper reference limits for plasma GFAP were established as 38 pg/mL (18-<50 years), 73 pg/mL (≥50-<70 years), and 156 pg/mL (≥70 years). Additionally, sex-related differences were observed after age 50, with females showing higher absolute GFAP levels than males. A strong positive correlation between age and plasma GFAP levels was observed (Spearman's r=0.832, p<0.0001). This study demonstrates the robust analytical performance of the MSD S-Plex® GFAP assay and establishes age-related reference values for plasma GFAP. These findings support its suitability for routine clinical use and enhance its applicability in the diagnosis and monitoring of central nervous system (CNS) pathologies, such as neurodegenerative diseases, neuroinflammatory disorders, and acute brain injuries, within biomarker-supported clinical algorithms."", ""Abstract Background The EAT-Lancet Commission has proposed a global reference diet aimed at promoting both human health and environmental sustainability. While adherence to this dietary pattern has been associated with reduced risks of chronic disease and lower environmental impact, concerns remain about its ability to meet nutritional requirements - particularly among older adults. The aim was to explore the association between adherence to the EAT-Lancet diet and nutrient intake and adequacy among 70-year-old adults in Gothenburg, Sweden. Methods This cross-sectional study included 861 participants from the Swedish population-based Gothenburg H70 Birth Cohort Study (mean age 70.5 years, 55% women). Dietary intake was assessed using a validated diet history interview, and adherence to the EAT-Lancet diet was scored based on 14 food components. Nutrient intake was evaluated against age- and sex-specific recommended intake (RI) levels. Cardiometabolic risk markers and biomarkers of nutritional status, including homocysteine and haemoglobin, were measured. Linear and logistic regression models were used to examine trends across sex-specific tertiles of diet adherence, with sensitivity analyses adjusting for energy intake and comparing adequacy based on average requirement (AR) thresholds. Results Higher adherence to the EAT-Lancet diet was linked to higher intake of fibre and polyunsaturated fats, and lower intake of saturated fat and alcohol. Mean protein intake per kilogram body weight/day was similar across adherence tertiles. Intake of beta-carotene, folate, vitamin C, magnesium, potassium, and iron was higher with greater adherence, while retinol equivalents, vitamin B12, niacin equivalents was lower– patterns that remained consistent after energy adjustment. Despite lower B12 intake, homocysteine levels were lowest in the group with highest adherence, and anaemia prevalence did not differ. Micronutrient adequacy improved with higher adherence for vitamin E, folate, vitamin C, magnesium, potassium, and iron. Similar results were observed using average requirement (AR) thresholds in sensitivity analyses. Conclusions Adherence to the EAT-Lancet diet was associated with a more favourable nutrient profile in this cohort of older adults, without evidence of widespread micronutrient inadequacy. These findings suggest that environmentally sustainable diets can support adequate nutrition when well-balanced, even in nutritionally vulnerable populations such as older adults."", ""Abstract Background It is largely unknown how alcohol use affects the risk of Alzheimer`s disease (AD). Therefore, studies on the influence of alcohol use on cerebrospinal fluid (CSF) biomarkers for the earliest preclinical phase of AD are needed. Methods This was a cross-sectional cohort study. The sample ( n = 301) was derived from the 2014–2016 examinations of the Gothenburg H70 Birth Cohort Studies. The study cohort consisted of 301 70-year-old women and men, where of 246 cognitively unimpaired and 55 with mild cognitive deficits. Information on alcohol consumption (g/week and type of alcohol) was collected and CSF amyloid-β 1−42 (Aβ42), total-tau (T-tau), tau phosphorylated at threonine 181 (P-tau181), neurofilament light protein (NfL) and neurogranin (Ng) were measured. We tested the association between the CSF biomarkers and alcohol consumption types using correlation and linear regression, adjusting for possible confounders when necessary according to the performed sensitivity analysis. Results There were no correlations between weekly alcohol consumption and any of the CSF markers studied in the total sample of cognitively unimpaired participants ( n = 246). After adjustments for multiplicity with FDR, there was an association between white wine and Ng in women with CDR = 0 (β:0.254, CI: ( 0.069: 0.439), p = 0.0076, FDR = 0.0455). Interaction analysis between female sex and red wine intake was a significant predictor of high Ng levels (β:0.410, CI: ( 0.099: 0.721), p = 0.0100, FDR = 0.0500). There were no correlations between consumption of specific types of alcohol (spirits, white wine, red wine, fortified wine, and beer) and any of the biomarkers studied in the total sample of cognitively unimpaired participants. Conclusions Our findings indicate that higher alcohol use in older cognitively unimpaired women correlates with a biomarker of synaptic dysfunction in AD, which is an important observation in a time when alcohol use is increasing among women.""]"
"R. Brenner","","","Computer Science","https://openalex.org/A5107946798","[""•The 12-gene Oncotype DX Colon Recurrence Score® assay quantifies recurrence risk in mismatch repair-proficient stage II/III CC.•This real-world clinical practice study found that this assay provides independent prognostic information in these patients.•The study found that CT seems to confer clinical benefit in patients with high risk according to the assay.•Our results further validate the assay in such patients and support its use for guiding adjuvant treatment decisions. BackgroundThe 12-gene Oncotype DX Colon Recurrence Score® result quantifies the recurrence risk in stage II/III colon cancer (CC). This real-world study investigated stage II CC patients whose treatment decisions incorporated the Recurrence Score® (RS) result.Materials and methodsThis retrospective analysis of a prospectively designed cohort included all stage II, mismatch repair-proficient CC patients who underwent 12-gene testing through Clalit between January 2011 and December 2016 and had available data with a minimum 3-year follow-up.ResultsThe analysis included 938 patients {median age 68 [interquartile range (IQR) 60-76] years; 96% T3 tumors}. The median RS was 26 (IQR 19-33) and the three RS categories (0-29, 30-40, 41-100) included 65%, 24%, and 11% of patients, respectively. Chemotherapy (CT) use differed significantly between the three RS categories (14%, 36%, and 60%, respectively; P < 0.001). The CT and observation-only groups were imbalanced with worse clinicopathologic characteristics in the former. Among observation-only patients, Kaplan–Meier (KM) estimates for recurrence-free interval (RFI) and CC-specific survival (CCSS) differed significantly between the three RS categories (P < 0.001). Clinical outcomes by treatment (CT versus observation) within each RS category revealed no differences in RFI and CCSS in the RS 0-29 and 30-40 categories. In contrast, in the RS 41-100 category, the difference in RFI trended toward significance (P = 0.066), and for CCSS, a statistically significant difference was observed, with better outcomes among CT-treated patients (P = 0.035).ConclusionsRS results are prognostic in stage II CC. Among RS 41-100 patients, outcomes were better in CT-treated versus observation-only patients despite worse clinicopathologic characteristics, suggesting that CT confers clinical benefit in high-risk patients. The 12-gene Oncotype DX Colon Recurrence Score® result quantifies the recurrence risk in stage II/III colon cancer (CC). This real-world study investigated stage II CC patients whose treatment decisions incorporated the Recurrence Score® (RS) result. This retrospective analysis of a prospectively designed cohort included all stage II, mismatch repair-proficient CC patients who underwent 12-gene testing through Clalit between January 2011 and December 2016 and had available data with a minimum 3-year follow-up. The analysis included 938 patients {median age 68 [interquartile range (IQR) 60-76] years; 96% T3 tumors}. The median RS was 26 (IQR 19-33) and the three RS categories (0-29, 30-40, 41-100) included 65%, 24%, and 11% of patients, respectively. Chemotherapy (CT) use differed significantly between the three RS categories (14%, 36%, and 60%, respectively; P < 0.001). The CT and observation-only groups were imbalanced with worse clinicopathologic characteristics in the former. Among observation-only patients, Kaplan–Meier (KM) estimates for recurrence-free interval (RFI) and CC-specific survival (CCSS) differed significantly between the three RS categories (P < 0.001). Clinical outcomes by treatment (CT versus observation) within each RS category revealed no differences in RFI and CCSS in the RS 0-29 and 30-40 categories. In contrast, in the RS 41-100 category, the difference in RFI trended toward significance (P = 0.066), and for CCSS, a statistically significant difference was observed, with better outcomes among CT-treated patients (P = 0.035). RS results are prognostic in stage II CC. Among RS 41-100 patients, outcomes were better in CT-treated versus observation-only patients despite worse clinicopathologic characteristics, suggesting that CT confers clinical benefit in high-risk patients."", ""In July 2018 an optimization run for the proposed charm cross section measurement for SHiP was performed at the CERN SPS. A heavy, moving target instrumented with nuclear emulsion films followed by a silicon pixel tracker was installed in front of the Goliath magnet at the H4 proton beamline. Behind the magnet, scintillating-fibre, drift-tube and RPC detectors were placed. The purpose of this run was to validate the measurement's feasibility, to develop the required analysis tools and fine-tune the detector layout. In this paper, we present the track reconstruction in the pixel tracker and the track matching with the moving emulsion detector. The pixel detector performed as expected and it is shown that, after proper alignment, a vertex matching rate of 87 % is achieved."", ""Abstract The operation at the Z-pole of the FCC-ee machine will deliver the highest possible instantaneous luminosities with the goal of collecting the largest Z boson datasets (Tera-Z), and enable a programme of standard model physics studies with unprecedented precision. The data acquisition and trigger systems of the FCC-ee experiments must be designed to be as unbiased and robust as possible, with the goal of containing the systematic uncertainties associated with these datasets at the smallest possible level, in order to not compromise the extremely small statistical uncertainties. In designing these experiments, we are confronted by questions on detector read-out speeds with an extremely tight material and power budget, trigger systems with a first hardware level or implemented exclusively on software, impact of background sources on event sizes, ultimate precision luminosity monitoring (to the $$10^{-5}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msup> <mml:mn>10</mml:mn> <mml:mrow> <mml:mo>-</mml:mo> <mml:mn>5</mml:mn> </mml:mrow> </mml:msup> </mml:math> – $$10^{-4}$$ <mml:math xmlns:mml=\""http://www.w3.org/1998/Math/MathML\""> <mml:msup> <mml:mn>10</mml:mn> <mml:mrow> <mml:mo>-</mml:mo> <mml:mn>4</mml:mn> </mml:mrow> </mml:msup> </mml:math> level) and sensitivity to a broad range of non-conventional exotic signatures, such as long-lived non-relativistic particles. We will review the various challenges on online selection for the most demanding Tera-Z running scenario and the constraints they pose on the design of FCC-ee detectors."", ""The operation at the Z-pole of the FCC-ee machine will deliver the highest possible instantaneous luminosities with the goal of collecting the largest Z boson datasets (Tera-Z), and enable a programme of Standard Model physics studies with unprecedented precision. The data acquisition and trigger systems of the FCC-ee experiments must be designed to be as unbiased and robust as possible, with the goal of containing the systematic uncertainties associated with these datasets at the smallest possible level, in order to not compromise the extremely small statistical uncertainties. In designing these experiments, we are confronted by questions on detector readout speeds with an extremely tight material and power budget, trigger systems with a first hardware level or implemented exclusively on software, impact of background sources on event sizes, ultimate precision luminosity monitoring (to the $10^{-5} - 10^{-4}$ level), and sensitivity to a broad range of non-conventional exotic signatures, such as long-lived non-relativistic particles. We will review the various challenges on online selection for the most demanding Tera-Z running scenario and the constraints they pose on the design of FCC-ee detectors."", ""Dark photons are hypothetical massive vector particles that could mix with ordinary photons. The simplest theoretical model is fully characterised by only two parameters: the mass of the dark photon m(gamma)D and its mixing parameter with the photon, epsilon. The sensitivity of the SHiP detector is reviewed for dark photons in the mass range between 0.002 and 10 GeV. Different productionmechanisms are simulated, with the dark photons decaying to pairs of visible fermions, including both leptons and quarks. Exclusion contours are presented and compared with those of past experiments. The SHiP detector is expected to have a unique sensitivity for m. D ranging between 0.8 and 3.3(-0.5)(+0.2) GeV, and epsilon(2) ranging between 10(-11) and 10(-17).""]"
"Nancy C. Andreasen","","","Computer Science","https://openalex.org/A5091277767","[""Abstract Background Schizophrenia is a severe neuropsychiatric disorder accompanied by debilitating cognitive and psychosocial impairments over the course of the disease. As disease trajectories exhibit considerable inter-individual heterogeneity, early clinical and neurobiological predictors of long-term outcome are desirable for personalized treatment and care strategies. Methods In a naturalistic longitudinal approach, 381 schizophrenia patients from the Iowa Lon-gitudinal Study (ILS) cohort underwent an extensive characterization, including repeated magnetic resonance imaging (MRI) scans, over a mean surveillance period of 11.07 years. We explored whether pre-diagnostic markers, clinical markers at the first psychotic episode, or magnetic resonance imaging (MRI) measures at the onset of the disease were predictive of relapse or remission of specific symptom patterns later in life. Results We identified a set of clinical parameters - namely premorbid adjustment during adolescence, symptom patterns, and neuropsychological profiles at disease onset – that were highly correlated with future disease trajectories. In general, brain measures at baseline did not correlate with outcome. Progressive regional brain volume losses over the observation period, however, were highly correlated with relapse patterns and symptom severity. Conclusions Our findings provide clinicians with a set of highly robust, easily acquirable, and cost-effective predictors for long-term outcome in schizophrenia. These results can be directly translated to a clinical setting to improve prospective care and treatment planning for schizophrenia patients. (Funding sources: NIH MH68380, MH31593, MH40856, and MH43271)."", ""16p11.2 copy number variations have been associated with neurodevelopmental disorders. Human induced pluripotent stem cells were generated from fibroblasts obtained from a patient diagnosed with schizophrenia with a 16p11.2 deletion. The generated cell line was further validated for its pluripotency and potential to differentiate into the three germ layers.""]"
"Math Bollen","","","Computer Science","https://openalex.org/A5046391029","[""Transmission network-expansion planning research requires reproducibility of results and comparability of research from various sources. This paper presents a process for modifying a published electricity network model so that the model can be used for exploration of transmission expansion planning problems for different load and generation profiles. Nodal voltages and branch currents are kept within performance limits by following the applicable planning codes, with reinforcements selected based on a defined strategy to achieve compliance with the applicable standards. The process can be applied to any published model and any set of planning standards to result in a base model that is suitably up to date and realistic for transmission network-expansion planning research. A case study is presented, whereby the process is followed for the “Nordic-32”—a popular reference model based on the Swedish transmission network of the 1980s—with the result being a reproducible and updatable model suitable for exploring transmission expansion planning using 2024 generation-and-demand assumptions from Sweden and network design guidelines based on the Nordel Grid Code."", ""Digital substation technology adhering to the IEC 61850 standard has provided several opportunities and flexibility for the rapid growth and complexity of the present and future electrical grid. The communication infrastructure allows complete interoperability between legacy and modern devices. The emergence of 5G wireless communication and its utilization in substation operation presents significant advantages in terms of cost and scalability, while also introducing challenges. This paper identifies research gaps in the literature and offers valuable insights for future analysis by providing a simulation study using an empirical latency dataset of a 5G network to illustrate three aspects of substation operational challenges: coordination of protection schemes, sequential reception of packet data streams, and time synchronization processes. The findings show a mean latency of 8.5 ms for the 5G network, which is significantly higher than that of a wired Ethernet network. The results also indicate that the high latency and jitter compromise the selectivity of protection schemes. The variability in latency disrupts the sequence of arriving data packets such that the packet buffering and processing delay increases from around 1.5 ms to 11.0 ms and the buffer size would need to increase by 6 to 10 times to handle out-of-sequence packets. Additionally, a time synchronization success rate of 14.3% within a 0.1 ms accuracy range found in this study indicates that the IEEE 1588 protocol is severely affected by the latency fluctuations."", ""This paper examines the random nature of interharmonics generated by power converters connected to sustainable energy sources and loads, such as wind turbines, photovoltaic (PV) panels, and electric vehicles (EVs). Current research often overlooks the stochastic behavior of interharmonics and their impact on power system reliability and resilience, leading to gaps in effective modeling and mitigation strategies. Thus, this study examines a low-voltage installation with a PV panel, an EV and a microwave operating simultaneously, providing practical insights into real-world scenarios of interharmonic related disruptions and solutions for enhancing the reliability and resilience of sustainable energy grids. By leveraging real-time measurements of interharmonics, suitable probability distribution functions (PDFs) are initialized to develop a probabilistic model using Monte Carlo simulation. This enables the derivation of a time-domain aggregation model of interharmonics from multiple sources operating together at the point of common coupling (PCC). The findings reveal that the peak values of voltage or current fluctuations at the PCC are influenced by the randomness in the number of devices connected and the frequency components originating from different sources. Through multiple case studies, the dependency of these fluctuations on stochastic parameters is systematically established. Empirical relationships are formulated to predict aggregated interharmonic values under varying scenarios, enhancing the accuracy and applicability of the model. The results demonstrate that higher interharmonic frequencies and fewer randomly connected devices significantly increase the probability of elevated aggregated peak values. These insights can serve as benchmarks for grid operators and policymakers in mitigating interharmonic related issues in modern power systems."", ""Alternative solutions, instead of building new transmission lines, are needed to enable the fast electrification of sectors such as industry, transportation, and heating. This includes smart-grid technology allowing for an increase in hosting capacity without the need for building new transmission lines. The increase in hosting capacity beyond the firm hosting capacity is, with many of the schemes, related to the ability of the installation to tolerate curtailment. The paper gives an overview of different smart-grid schemes and how they potentially increase the hosting capacity. The connection of a large electrolyser installation for hydrogen production is used as an illustrative example introducing some of the issues.""]"
"Monica Nordberg","","Professor","Computer Science","https://openalex.org/A5006834808","Cadmium is widely recognized as an important environmental toxicant that may give rise to kidney dysfunction, bone disease, and cancer in humans and animals. Kidney dysfunction occurs at very low exposures and is often considered as the most sensitive or critical effect. Cadmium exposures of concern occur in many countries. In low- and middle-income countries with small-scale mining, excessive exposure to cadmium and other metals occurs in occupational and environmental settings. This is of particular importance in view of the growing demand for metals in global climate change mitigation. Since the 1970s, the present authors have contributed evidence concerning the role of metallothionein and other factors in influencing the toxicokinetics and toxicity of cadmium, particularly as it relates to the development of adverse effects on kidneys in humans and animals. The findings gave a background to the development of biomarkers employed in epidemiological studies, demonstrating the important role of metallothionein in protection against cadmium-induced kidney dysfunction in humans. Studies in cadmium-exposed population groups demonstrated how biomarkers of kidney dysfunction changed during 8 years after drastic lowering of environmental cadmium exposure. Other epidemiological studies showed the impact of a good zinc status in lowering the prevalence of cadmium-related kidney dysfunction. Increased susceptibility to Cd-induced kidney dysfunction was shown in a population with high exposure to inorganic arsenic when compared with a group with low such exposure. Several national and international organizations have used part of the reviewed information, but the metallothionein-related biomarkers and the interaction effects have not been fully considered. We hope that these data sets will also be included and improve risk assessments and preventive measures."Monica Nordberg","","Professor","Computer Science","https://openalex.org/A5006834808","Cadmium is widely recognized as an important environmental toxicant that may give rise to kidney dysfunction, bone disease, and cancer in humans and animals. Kidney dysfunction occurs at very low exposures and is often considered as the most sensitive or critical effect. Cadmium exposures of concern occur in many countries. In low- and middle-income countries with small-scale mining, excessive exposure to cadmium and other metals occurs in occupational and environmental settings. This is of particular importance in view of the growing demand for metals in global climate change mitigation. Since the 1970s, the present authors have contributed evidence concerning the role of metallothionein and other factors in influencing the toxicokinetics and toxicity of cadmium, particularly as it relates to the development of adverse effects on kidneys in humans and animals. The findings gave a background to the development of biomarkers employed in epidemiological studies, demonstrating the importa...

Biomarkers or biological indicators of metal-induced renal toxicity have been employed for several decades in monitoring the early clinical effects of nephrotoxic metals such as lead (Pb), cadmium (Cd), and mercury (Hg). These indicators, which are early biochemical tests for assessing the biochemical responses of the kidney to these metals prior to overt clinical disease, have proven useful for indicating not only metal exposure, but that a sufficient quantity of a given metal is biologically available to produce a cellular response. In order to be really useful, a good biomarker must be highly sensitive, easily measurable, relatively chemical-specific, and interpretable as to whether it is an index of exposure or toxic effect. A major research need is to provide correlative morphological/biochemical data so that the prognostic value to a given biomarker may be determined (Fowler, 1983).

More than one and a half centuries ago, adverse human health effects were reported after use of a cadmium-containing silver polishing agent. Long-term cadmium exposure gives rise to kidney or bone disease, reproductive toxicity and cancer in animals and humans. At present, high human exposures to cadmium occur in small-scale mining, underlining the need for preventive measures. This is particularly urgent in view of the growing demand for minerals and metals in global climate change mitigation. This review deals with a specific part of cadmium toxicology that is important for understanding when toxic effects appear and, thus, is crucial for risk assessment. The discovery of the low-molecular-weight protein metallothionein (MT) in 1957 was an important milestone because, when this protein binds cadmium, it modifies cellular cadmium toxicity. The present authors contributed evidence in the 1970s concerning cadmium binding to MT and synthesis of the protein in tissues. We showed that bind..."
"Uwe Windhorst","","Professor","Computer Science","https://openalex.org/A5053138224","As if the anatomical architecture of the nociceptive and pain system were not complex enough, a huge variety of additional neurotransmitters, neuromodulators and hormones adds on to it. This new dimension will be treated here. The review is structured from two complementary angles: first, the anatomical and neurophysiological perspective, covering key structures from peripheral nociceptors to central brain regions such as the cortex, basal ganglia, hypothalamus, and various brainstem nuclei; and second, the molecular and biochemical perspective, outlining the array of neuroactive substances – including neuropeptides, classical neurotransmitters, and neuromodulators – involved in nociception and acute pain modulation. In selected sections, appropriate case reports are presented to illustrate specific mechanisms or phenomena, and at the end, some clinical syndromes are mentioned to link basic concepts with clinical relevance. Sub-cellular processes and therapeutic approaches are beyond t...

Pain is – a pain in the neck, isn´t it? Yes and no. Yes, it hurts. Yet, it helps. At least do acute and transient pain. Acute pain resulting from an acute event is a fundamental condition for survival, insofar as it warns against imminent or actual tissue damage, a potentially life-endangering threat. The importance of the physiological, protective role of nociceptive pain is underscored by cases, in which a failure to sense pain such as in the case of congenital insensitivity often leads to self-mutilation, bone fractures, joint deformities, amputations, and even early death. While the goal of acute pain based on nociception thus appears to be clear, its implementation is anything but that because to achieve this goal calls for a number of requirements to be fulfilled. The first is the identification of a noxious stimulus, including its intensity and location on the body surface or within the body. The second is the orchestration of counter-measures, including arousal, emotional and v...

An untrained couch potato involuntarily forced to undergo a long strenuous exercise towards exhaustion will feel it at the end as a discomforting fatigue. A well-trained athlete will feel the same but after a longer time. Patients of various neurological diseases will do so much earlier. All may also finally experience muscle pain and soreness the next day.Muscle fatigue is an exercise-induced reduction in maximal voluntary muscle force. Muscle fatigability varies with muscle use, age and sex. It is commonly divided into two broad categories: peripheral and central fatigue. The first refers to the many processes in the fatiguing muscle(s), the second to subsequent processes in the central nervous system (CNS). Peripheral muscle fatigue can be caused by numerous different mechanisms, ranging from the accumulation of metabolites within muscle fibers to their damage. Central fatigue involves the inadequacy of the CNS to generate and maintain sufficiently strong motor command. There must o...

The term `motor control&acute; encompasses a wide range of mechanisms thought to be implicated in the organization of movements. Their study, like that of any other scientific field, must use specific notions to get to grips with them. This review is therefore organized along a series of notions that are frequently used in motor control, and we will discuss them with particular emphasis on neurological conditions, which may disrupt normal motor functioning. We will start with a short description of the roles of space and time, in which movements take place. Subsequently we will deal with kinematics and kinetics (dynamics) of movements. Then, we will list the inputs to motoneurons (MNs), which, in different forms, convey signals from the central nervous system (CNS) to skeletal muscles and muscle spindles, including central pattern generators (CPGs), sensory inputs and supraspinal descending fiber systems. Some helpers in movement organization will be introduced, such as internal models...

In the past, the spinal cord was considered a hard-wired network responsible for spinal reflexes and a conduit for long-range connections. This view has changed dramatically over the past few decades. It is now recognized as a plastic structure that has the potential to adapt to changing environments. While such changes occur under physiological conditions, the most dramatic alterations take place in response to pathological events. Many of the changes that occur following such pathological events are maladaptive, but some appear to help adapt to the new conditions. Although a number of studies have been devoted to elucidating the underlying mechanisms, in humans and animal models, the etiology and pathophysiology of various diseases impacting the spinal cord are still not well understood. In this review, we summarize current understanding and outstanding challenges for a number of diseases, including spinal muscular atrophy (SMA), amyotrophic laterals sclerosis (ALS), and spinal cord ..."
"Jian Zhang","","Professor","Computer Science","https://openalex.org/A5100410082","The photocatalytic reduction of carbon dioxide (CO2) to chemicals holds significant importance for mitigating the current energy crisis. Rational design of catalytic centers within well-defined structures can effectively enhance the reaction activity and selectivity. In this study, we constructed interrupted zeolitic boron imidazolate frameworks (BIFs) featuring unsaturated coordination at the central Co ion. The gas adsorption measurement results indicate that BIF-92-Co(FA) modified with terminal formate ligand exhibits excellent stability and high porosity. In the photocatalytic CO2 reduction reaction, BIF-92-Co(FA) achieved a CO production rate of 3866.5 μmol g-1 h-1 and a selectivity of 87.8% for CO as the primary product. This work paves the way for the design and development of efficient catalysts through unsaturated metal sites.

Abstract We propose a metal-dielectric hybrid photonic crystal (PhC) slab for generating orbital angular momentum (OAM) vortex beams. The approach exploits multiple bound states in the continuum (BICs) supported by the PhC slab, which serve as polarization vortex center in momentum space. These vortices carry distinct topological charges, leading to the generation of diverse OAM modes. Unlike conventional spiral phase plates, our design scheme is composed of double-layer periodic units and without a distinct optical geometric center. This PhC slab exhibits insensitivity to the position of the incident wave and is conducive to the flexible configuration of the OAM generator. We further investigate a compact OAM antenna system based on the proposed PhC slab lens. Simulation and experimental results demonstrate that OAM modes with topological charges l=-2 (6.5–7.0 GHz) and l=+2 (5.7–6.0 GHz) are generated as a right-handed circularly polarized (RCP) wave is incident on the PhC slab, The O...

Abstract A high‐density green body is critical for producing fine‐grained ceramics at low sintering temperatures. However, achieving high density from submicron powders is challenging due to their high specific surface area. In this study, an alumina slurry using a super‐fine α‐Al 2 O 3 powder ( d 50 = 180 nm) and a low molecular weight dispersant (CE‐64) was prepared with a high solid loading of 55 vol% and a low viscosity of 0.40 Pa·s. The slurry was post‐treated by pressure filtration at 0.4 MPa. After de‐binding at 750°C, the resulting green bodies exhibited a relative density of 67.4%, 0.9% higher than that of the counterparts formed without pressure filtration. Pressureless sintering at 1300°C in air yielded ceramics with a relative density of 99.97%, an average grain size of 0.70 µm, a flexural strength of 631 MPa, and a hardness of 18.5 GPa. These findings demonstrate the feasibility of achieving high‐performance alumina ceramics with fine‐grained microstructures under simplifi..."
"O. Smirnova","","Professor","Computer Science","https://openalex.org/A5022564127","The article updates the problem of maintaining inflation at a consistently low level. The subjects of management of this process have been identified. &#x0D; The study provides price formation mechanisms using the example of the construction industry. The dynamics of the cost of basic materials used in construction is presented and described. Using these examples from the construction industry, the main drivers of local inflation are identified. &#x0D; The concept of “cost-push inflation scissors” is considered. Some features and prerequisites for inflation and its impact on the economic security of the country's macroeconomic system have been identified. &#x0D; The meaning of effective demand and its role in ensuring the economic security of the macroeconomic system of the state is clarified. The mechanism of inflation targeting is described. Conclusions are drawn about possible scenarios for combating inflation and opportunities to minimize the consequences of its impact. The mechani...

A time projection chamber (TPC) with micropattern gaseous detector (MPGD) readout is investigated as main tracking device of the International Large Detector (ILD) concept at the planned International Linear Collider (ILC). A prototype TPC equipped with a triple gas electron multiplier (GEM) readout has been built and operated in an electron test beam. The TPC was placed in a 1 T solenoidal field at the DESY II Test Beam Facility, which provides an electron beam up to 6 GeV/c. The performance of the readout modules, in particular the spatial point resolution, is determined and compared to earlier tests. New studies are presented with first results on the separation of close-by tracks and the capability of the system to measure the specific energy loss dE/dx. This is complemented by a simulation study on the optimization of the readout granularity to improve particle identification by dE/dx.

Full detector simulation is known to consume a large proportion of computing resources available to the LHC experiments, and reducing time consumed by simulation will allow for more profound physics studies. There are many avenues to exploit, and in this work we investigate those that do not require changes in the GEANT4 simulation suite. In this study, several factors affecting the full GEANT4 simulation execution time are investigated. A broad range of configurations has been tested to ensure consistency of physical results. The effect of a single dynamic library GEANT4 build type has been investigated and the impact of different primary particles at different energies has been evaluated using GDML and GeoModel geometries. Some configurations have an impact on the physics results and are, therefore, excluded from further analysis. Usage of the single dynamic library is shown to increase execution time and does not represent a viable option for optimization. Lastly, the static build t...

A review article on investment vehicles for companies in the modern digital economy. The article presents the main financial instruments available to organizations that allow them to invest free funds to generate additional income. Deposits, bonds, Federal loan bonds, stocks, mutual funds, ETFs and REITs are considered. Their main parameters are analyzed from the point of view of applied use, as well as the possibility of implementing transactions for the placement of funds in these instruments through digital channels. The article provides up-to-date quotes for rates and profitability parameters of instruments. Different banks and their terms of placement are compared on deposits. The main idea of the article is to convey the need to study, select and use specific tools in accordance with the objectives of the organization in order to obtain maximum additional income."
"R. Gonzalez Suarez","","Professor","Computer Science","https://openalex.org/A5106331204","Abstract The CODEX- β apparatus is a demonstrator for the proposed future CODEX-b experiment, a long-lived-particle detector foreseen for operation at IP8 during HL-LHC data-taking. The demonstrator project, intended to collect data in 2025, is described, with a particular focus on the design, construction, and installation of the new apparatus.

A bstract This paper investigates the search for long-lived dark scalars from exotic Higgs boson decays at the Future Circular Collider in its e + e − stage, FCC-ee, considering an integrated luminosity of 10 . 8 ab −1 collected during the ZH run at a center-of-mass energy $$ \sqrt{s} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> </mml:math> = 240 GeV. The work considers Zh events where the Z boson decays leptonically and the Higgs boson h decays into two long-lived dark scalars s which further decay into bottom anti-bottom quark pairs. The analysis is performed using a parametrized simulation of the IDEA detector concept and targets dark scalar decays in the tracking volume, resulting in multiple displaced vertices in the final state. The sensitivity towards long-lived dark scalars at FCC-ee is estimated using an event selection requiring two opposite-charge, same-flavor leptons compatible with the Z boson, and at least two di...

This study presents a novel method for the definition of signal regions in searches for new physics at collider experiments. By leveraging multi-dimensional histograms with precise arithmetic and utilizing the SparkDensityTree library, it is possible to identify high-density regions within the available phase space, potentially improving sensitivity to very small signals. Inspired by a search for dark mesons at the ATLAS experiment, CMS open data is used for this proof-of-concept intentionally targeting an already excluded signal. Signal regions are defined based on density estimates of signal and background. These preliminary regions align well with the physical properties of the signal while effectively rejecting background events.

We examine the possibility to detect new SM-neutral vector bosons (<a:math xmlns:a=""http://www.w3.org/1998/Math/MathML"" display=""inline""><a:msup><a:mi>Z</a:mi><a:mo>′</a:mo></a:msup></a:math>) that couple exclusively to leptons in the electron-positron mode of the Future Circular Collider (FCC-ee). Focusing on the <c:math xmlns:c=""http://www.w3.org/1998/Math/MathML"" display=""inline""><c:msup><c:mi>Z</c:mi><c:mo>′</c:mo></c:msup></c:math> production with a radiated photon search channel, we show that the FCC-ee can significantly extend the unprobed parameter space by increasing the exclusion in the coupling by one to two orders of magnitude in the kinematically allowed mass range (from 10 to 365 GeV), with the leading sensitivity being driven by the muon channel. In doing so, it outperforms other proposed lepton collider options such as CLIC and ILC in this range of masses. Further, we discuss the possibility of improving the sensitivity of the FCC-ee to this model through the modificati...

Abstract Quantum spin Hall (QSH) insulators are materials with nontrivial topological properties, characterized by helical edge currents. In 2D strips, the application of a bias voltage along the edge generates a magnetization that can be measured using quantum sensors and magnetometry techniques. In this work, we calculate the magnetic field in the vicinity of the edge and explore the potential role of nitrogen-vacancy (NV) centers in diamond as local probes for the characterization of QSH edge states in topological insulators. We characterize the magnetic field near the edges produced by both electron currents and spin accumulation at the edge. We focus on identifying the position from the edge at which the effects of spin accumulation become detectable. We observe that a larger gap between the conduction and valence bands, along with a lower Fermi velocity, results in a stronger magnetic field, with the detectable spin accumulation being more concentrated near the edge. Conversely, ..."
"J. Strandberg","","Professor","Computer Science","https://openalex.org/A5106944425",""
"T. P. A. Åkesson","","Professor","Computer Science","https://openalex.org/A5031324575","Abstract The jet energy calibration and its uncertainties are derived from measurements of the calorimeter response to single particles in both data and Monte Carlo simulation using proton–proton collisions at $$\sqrt{s} = 13$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> <mml:mo>=</mml:mo> <mml:mn>13</mml:mn> </mml:mrow> </mml:math> TeV collected with the ATLAS detector during Run 2 at the Large Hadron Collider. The jet calibration uncertainty for anti- $$k_T$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mi>k</mml:mi> <mml:mi>T</mml:mi> </mml:msub> </mml:math> jets with a jet radius parameter of R $$_\textrm{jet} = 0.4$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mmultiscripts> <mml:mrow/> <mml:mtext>jet</mml:mtext> <mml:mrow/> </mml:mmultiscripts> <mml:mo>=</mml:mo> <mml:mn>0.4</mml:mn> </mml:mrow> </mml:math> and in the central jet rapidity region is about...

Abstract A search for a pseudoscalar a produced in association with a top-quark pair, or in association with a single top quark plus a W boson, with the pseudoscalar decaying into b -quarks ( $$a\rightarrow b\bar{b}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mi>a</mml:mi> <mml:mo>→</mml:mo> <mml:mi>b</mml:mi> <mml:mover> <mml:mrow> <mml:mi>b</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> </mml:mrow> </mml:math> ), is performed using the full Run 2 data sample using a dileptonic decay mode signature. The search covers pseudoscalar boson masses between 12 and 100 GeV and involves both the kinematic regime where the decay products of the pseudoscalar are reconstructed as two standard b -tagged small-radius jets, or merged into a large-radius jet due to its Lorentz boost. No significant excess relative to expectations is observed. Assuming a branching ratio $$\text {BR}(a\rightarrow b\bar{b})=100\% $$ <mml:math xmlns:mml=""http:/..."
"M. Ellert","","Professor","Computer Science","https://openalex.org/A5031620607","Abstract The fast reconstruction of charged particle tracks with high efficiency and track quality is an essential part of the online data selection for the ATLAS experiment at the High Luminosity LHC. Dedicated custom designed hardware boards and software simulations have been developed to assess the feasibility of a Hardware Tracking Trigger (HTT) system. The Pattern Recognition Mezzanine (PRM), as part of the HTT system, has been designed to recognize track candidates in silicon detectors with Associative Memory ASICs and to select and reconstruct tracks using linearized algorithms implemented in an Intel Stratix 10 MX FPGA. The highly parallelized FPGA design makes extensive use of the integrated High-Bandwidth-Memory. In this paper, the FPGA design for the PRM board is presented. Its functionalities have been verified in both simulations and hardware tests on an Intel Stratix 10 MX development kit.

The fast reconstruction of charged particle tracks with high efficiency and track quality is an essential part of the online data selection for the ATLAS experiment at the High-Luminosity LHC. Dedicated custom designed hardware boards and software simulations have been developed to assess the feasibility of a Hardware Tracking Trigger (HTT) system. The Pattern Recognition Mezzanine (PRM), as part of the HTT system, has been designed to recognize track candidates in silicon detectors with Associative Memory ASICs and to select and reconstruct tracks using linearized algorithms implemented in an Intel Stratix 10 MX FPGA. The highly parallelized FPGA design makes extensive use of the integrated High-Bandwidth-Memory. In this paper, the FPGA design for the PRM board is presented. Its functionalities have been verified in both simulations and hardware tests on an Intel Stratix 10 MX development kit.

The Worldwide LHC Computing Grid (WLCG) is today comprised of a range of different types of resources such as cloud centers, large and small HPC centers, volunteer computing as well as the traditional grid resources. The Nordic Tier 1 (NT1) is a WLCG computing infrastructure distributed over the Nordic countries. The NT1 deploys the Nordugrid ARC-CE, which is non-intrusive and lightweight, originally developed to cater for HPC centers where no middleware could be installed on the worker nodes. The NT1 runs ARC in the native Nordugrid mode which contrary to the Pilot mode leaves jobs data transfers up to ARC. ARCs data transfer capabilities together with the ARC Cache are the most important features of ARC. In this article we will describe the datastaging and cache functionality of the ARC-CE set up as an edge service to an HPC or cloud resource, and show the gain in efficiency this model provides compared to a traditional pilot model, especially for sites with remote storage.

General-purpose Computing on Graphics Processing Units (GPGPU) has been introduced to many areas of scientific research such as bioinformatics, cryptography, computer vision, and deep learning. However, computing models in the High-energy Physics (HEP) community are still mainly centred around traditional CPU resources. Tasks such as track fitting, particle reconstruction, and Monte Carlo simulation could benefit greatly from a high-throughput GPGPU computing model, streamlining bottlenecks in analysis turnover. This technical note describes the basis of an implementation of an integrated GPU discovery mechanism in GRID middleware to facilitate GPGPU.

General-purpose Computing on Graphics Processing Units (GPGPU) has been introduced to many areas of scientific research such as bioinformatics, cryptography, computer vision, and deep learning. However, computing models in the High-energy Physics (HEP) community are still mainly centred around traditional CPU resources. Tasks such as track fitting, particle reconstruction, and Monte Carlo simulation could benefit greatly from a high-throughput GPGPU computing model, streamlining bottlenecks in analysis turnover. This technical note describes the basis of an implementation of an integrated GPU discovery mechanism in GRID middleware to facilitate GPGPU."
"C. C. Ohm","","Professor","Computer Science","https://openalex.org/A5047441454","Relational databases often suffer from uninformative descriptors of table contents, such as ambiguous columns and hard-to-interpret values, impacting both human users and text-to-SQL models. In this paper, we explore the use of large language models (LLMs) to automatically generate detailed natural language descriptions for SQL database columns, aiming to improve text-to-SQL performance and automate metadata creation. We create a dataset of gold column descriptions based on the BIRD-Bench benchmark, manually refining its column descriptions and creating a taxonomy for categorizing column difficulty. Through evaluating several LLMs, we find that incorporating these column descriptions consistently enhances text-to-SQL model performance, particularly for larger models like GPT-4o, Qwen2 72B and Mixtral 22Bx8. However, models struggle with columns that exhibit inherent ambiguity, highlighting the need for manual expert input. Notably, Qwen2-generated descriptions, containing by annotators...

In many social networks, besides peer-to-peer communication, people share information via groups. An interesting problem arises in this scenario: for such networks, which are the best groups to start information diffusion so that the number of eventually informed nodes can be maximized? In this study, we formulate a novel information coverage maximization problem in the context of hypergraphs, wherein nodes are connected by arbitrary-size hyperedges (i.e., groups). In contrast to the existing literature on influence maximization, which aims to find authority nodes with high influence, we are interested in identifying the key groups. To address this problem, we present a new information diffusion model for hypergraphs, namely Hypergraph- Independent-Cascade (HIC). HIC generalizes the popular independent cascade model to hypergraphs to allow capturing group-level information diffusion. We prove the NP- hardness of the proposed problem under HIC, and the submodular monotone property of th...

A technique is presented to measure the efficiency with which $c$-jets are mistagged as b-jets (mistagging efficiency) using $t\bar{t}$ events, where one of the $W$ bosons decays into an electron or muon and a neutrino and the other decays into a quark-antiquark pair. The measurement utilises the relatively large and known $W\to cs$ branching ratio, which allows a measurement to be made in an inclusive $c$-jet sample. The data sample used was collected by the ATLAS detector at $\sqrt{s} = 13$ TeV and corresponds to an integrated luminosity of 139 fb$^{-1}$. Events are reconstructed using a kinematic likelihood technique which selects the mapping between jets and $t\bar{t}$ decay products that yields the highest likelihood value. The distribution of the $b$-tagging discriminant for jets from the hadronic $W$ decays in data is compared with that in simulation to extract the mistagging efficiency as a function of jet transverse momentum. The total uncertainties are in the range 3%-17%. Th...

Heavy-flavour hadron production provides information about the transport properties and microscopic structure of the quark-gluon plasma created in ultra-relativistic heavy-ion collisions. A measurement of the muons from semileptonic decays of charm and bottom hadrons produced in Pb+Pb and $pp$ collisions at a nucleon-nucleon centre-of-mass energy of 5.02 TeV with the ATLAS detector at the Large Hadron Collider is presented. The Pb+Pb data were collected in 2015 and 2018 with sampled integrated luminosities of $208~\mathrm{\mu b}^{-1}$ and $38~\mathrm{\mu b^{-1}}$, respectively, and $pp$ data with a sampled integrated luminosity of $1.17~\mathrm{pb}^{-1}$ were collected in 2017. Muons from heavy-flavour semileptonic decays are separated from the light-flavour hadronic background using the momentum imbalance between the inner detector and muon spectrometer measurements, and muons originating from charm and bottom decays are further separated via the muon track's transverse impact paramet..."
"C. Doglioni","","Professor","Computer Science","https://openalex.org/A5050166026","The fundamental nature of Dark Matter is a central theme of the Snowmass 2021 process, extending across all frontiers. In the last decade, advances in detector technology, analysis techniques and theoretical modeling have enabled a new generation of experiments and searches while broadening the types of candidates we can pursue. Over the next decade, there is great potential for discoveries that would transform our understanding of dark matter. In the following, we outline a road map for discovery developed in collaboration among the frontiers. A strong portfolio of experiments that delves deep, searches wide, and harnesses the complementarity between techniques is key to tackling this complicated problem, requiring expertise, results, and planning from all Frontiers of the Snowmass 2021 process.

Semi-visible jets arise from a hypothetical, strongly interacting ``dark sector'' -- a dark counterpart of quantum chromodynamics whose partial decays back to Standard Model particles introduce new types of collider BSM signature. CMS and ATLAS have have searched for semi-visible jets in the resonant and non-resonant production modes and set constraints on mediator mass values. In this work, indirect constraints on various model parameters, such as dark hadron masses and coupling strengths, are explored using LHC measurements.

Abstract In modern High Energy Physics (HEP) experiments, triggers perform the important task of selecting, in real time, the data to be recorded and saved for physics analyses. As a result, trigger strategies play a key role in extracting relevant information from the vast streams of data produced at facilities like the Large Hadron Collider (LHC). As the energy and luminosity of the collisions increase, these strategies must be upgraded and maintained to suit the experimental needs. This whitepaper presents a high-level overview and reviews recent developments of triggering practices employed at the LHC. The general trigger principles applied at modern HEP experiments are highlighted, with specific reference to the current trigger state-of-the-art within the ALICE, ATLAS, CMS and LHCb collaborations. Furthermore, a brief synopsis of the new trigger paradigm required by the upcoming high-luminosity upgrade of the LHC is provided.

In modern High Energy Physics (HEP) experiments, triggers perform the important task of selecting, in real time, the data to be recorded and saved for physics analyses. As a result, trigger strategies play a key role in extracting relevant information from the vast streams of data produced at facilities like the Large Hadron Collider (LHC). As the energy and luminosity of the collisions increase, these strategies must be upgraded and maintained to suit the experimental needs. This whitepaper compiled by the SMARTHEP Early Stage Researchers presents a high-level overview and reviews recent developments of triggering practices employed at the LHC. The general trigger principles applied at modern HEP experiments are highlighted, with specific reference to the current trigger state-of-the-art within the ALICE, ATLAS, CMS and LHCb collaborations. Furthermore, a brief synopsis of the new trigger paradigm required by the upcoming high-luminosity upgrade of the LHC is provided."
"T. Johansson","","Professor","Computer Science","https://openalex.org/A5072376954","Abstract The Multi-Blade (MB) Boron-10-based neutron detector is the chosen technology for three instruments at the European Spallation Source (ESS): the two ESS reflectometers, ESTIA and FREIA, and the Test Beam Line. A fourth MB detector has been built, installed and commissioned for the user operation of the reflectometer Amor at PSI (Switzerland). Amor can be considered a downscaled version of the ESS reflectometer ESTIA. They are based on the same Selene guide concept, optimized for performing focusing reflectometry on small samples. The experience gained at Amor is invaluable for the future deployment of the MB detector at the ESS. This manuscript describes the MB detector construction and installation at Amor along with the readout electronics chain based on the VMM3a ASIC. The readout chain deployed at Amor is equivalent of that of the ESS, including the readout master module (RMM), event-formation-units (EFUs), Kafka, FileWriter and live visualisation tools.

A new generation of experiments is being developed, where the challenge of separating rare signal processes from background at high intensities requires a change of trigger paradigm. At the future PANDA experiment at FAIR, hardware triggers will be abandoned and instead a purely software-based system will be used. This requires novel reconstruction methods with the ability to process data from many events simultaneously. A 4D tracking algorithm based on the cellular automaton has been developed which will utilize the timing information from detector signals. Simulation studies have been performed to test its performance on the foreseen free-streaming data from the PANDA detector. For this purpose, a quality assurance procedure for tracking on free-streaming data was implemented in the PANDA software. The studies show that at higher interaction rates, 4D tracking performs better than the 3D algorithm in terms of efficiency, 84% compared to 77%. The fake track suppression is also greatly..."
"T. Dorigo","","Professor","Computer Science","https://openalex.org/A5037652790","In this work we consider the problem of determining the identity of hadrons at high energies based on the topology of their energy depositions in dense matter, along with the time of the interactions. Using GEANT4 simulations of a homogeneous lead tungstate calorimeter with high transverse and longitudinal segmentation, we investigated the discrimination of protons, positive pions, and positive kaons at 100 GeV. The analysis focuses on the impact of calorimeter granularity by progressively merging detector cells and extracting features like energy deposition patterns and timing information. Two machine learning approaches, XGBoost and fully connected deep neural networks, were employed to assess the classification performance across particle pairs. The results indicate that fine segmentation improves particle discrimination, with higher granularity yielding more detailed characterization of energy showers. Additionally, the results highlight the importance of shower radius, energy frac...

The TomOpt software package is designed to optimise the geometric configuration and the specifications of detectors intended for muon scattering tomography, an imaging technique exploiting cosmic-ray muons. The software employs an end-to-end differentiable pipeline that models the interactions of muons with detectors and scanned volumes, infers properties of the scanned materials, and performs an optimisation cycle minimising a user-defined loss function. This article presents the implementation of a case study related to cargo scanning applications in the context of homeland security.

We simulate hadrons impinging on a homogeneous lead tungstate (PbWO4) calorimeter using GEANT4 software to investigate how the resulting light yield and its temporal structure, as detected by an array of light-sensitive sensors, can be processed by a neuromorphic computing system. Our model encodes temporal photon distributions as spike trains and employs a fully connected spiking neural network to estimate the total deposited energy, as well as the position and spatial distribution of the light emissions within the sensitive material. The extracted primitives offer valuable topological information about the shower development in the material, achieved without requiring a segmentation of the active medium. A potential nanophotonic implementation using III-V semiconductor nanowires is discussed. It can be both fast and energy efficient.

Recent advances in machine learning have opened new avenues for optimizing detector designs in high-energy physics, where the complex interplay of geometry, materials, and physics processes has traditionally posed a significant challenge. In this work, we introduce the end-to-end. AI Detector Optimization framework (AIDO), which leverages a diffusion model as a surrogate for the full simulation and reconstruction chain, enabling gradient-based design exploration in both continuous and discrete parameter spaces. Although this framework is applicable to a broad range of detectors, we illustrate its power using the specific example of a sampling calorimeter, focusing on charged pions and photons as representative incident particles. Our results demonstrate that the diffusion model effectively captures critical performance metrics for calorimeter design, guiding the automatic search for a layer arrangement and material composition that align with known calorimeter principles. The success o..."
"Gerhard Andersson","","Professor","Computer Science","https://openalex.org/A5080370996","Abstract The Patient Health Questionnaire-9 (PHQ-9) is a widely used tool for assessing depressive symptom severity and as a screening tool in the diagnosis of major depression. Designed as both a diagnostic instrument and a severity index, it is commonly used in primary care and research. However, findings regarding its reliability and validity for these dual purposes have been mixed. This study aimed to review the history of the PHQ-9, evaluate its factorial validity, and temporal measurement invariance using the dynamic fit index cutoffs framework for model fit evaluations. In a clinical sample of 3384 participants, strong correlations were found between items 1 and 2 (i.e., anhedonia and feeling depressed), indicating a substantial overlap in their coverage. Although a unidimensional factor structure was obtained, the model fit was suboptimal when contrasted with the sample dependent dynamic fit indices. Furthermore, measurement invariance between different treatment groups could n...

Provides tools for assessing and selecting auxiliary variables using LASSO. The package includes functions for variable selection and diagnostics, facilitating survey calibration analysis with emphasis on robust auxiliary vector selection. For more details see Tibshirani (1996) <<a href=""https://doi.org/10.1111%2Fj.2517-6161.1996.tb02080.x"" target=""_top"">doi:10.1111/j.2517-6161.1996.tb02080.x</a>> and Caughrey and Hartman (2017) <<a href=""https://doi.org/10.2139%2Fssrn.3494436"" target=""_top"">doi:10.2139/ssrn.3494436</a>>.

<sec> <title>BACKGROUND</title> Refugees commonly experience numerous adverse and traumatic events and are therefore at increased risk of mental health problems. Despite the high need for mental health interventions, services tend to be under-utilized by refugees, and various barriers compromise access. Digital, efficient screening, adapted for refugees, could facilitate initial assessment and increase accessibility to mental health services. We developed an internet-based tiered screening procedure, i-TAP, aiming to identify clinically relevant symptoms of major depressive disorder, anxiety disorders, post-traumatic stress disorder (PTSD) and insomnia disorder among refugees. The i-TAP is an adaptive procedure with three tiers aiming to identify general mental distress in tier 1, differentiate between symptoms in tier 2, and assess severity of symptoms in tier 3. Each tier additionally functions as a gateway to further assessment, as a negative outcome terminates the procedure. </sec>...

Despite efforts to implement evidence-based guidelines, little is known about the quality of cognitive-behavioural therapy (CBT) for depression, anxiety disorders, obsessive-compulsive disorder, and posttraumatic stress disorder in routine clinical practice. The present study aimed to investigate therapist adherence to CBT, and related aspects of quality of care, from a patient perspective. In a cross-sectional study, 90 participants from 21 routine psychiatric outpatient units in Stockholm completed a web-based survey post-CBT. Participants reported a high degree of therapist adherence to CBT techniques and procedures (M = 3.02 on a scale of 0 to 4), with higher adherence for social anxiety disorder and posttraumatic stress disorder than for depression (p = .002, ω2 = 0.21). Therapist adherence was moderately correlated with patient improvement (τs = .37-.38, ps < .001). Participants reported a high degree of symptom improvement (M = 3.10) and treatment satisfaction (M = 3.38) and rec...

The use of internet-based treatments has increased significantly in recent years. As remote technologies continue to evolve, psychotherapy research is progressively shifting toward these approaches. Anxiety and depressive disorders are highly prevalent in adolescents, imposing significant personal and societal costs. Identifying effective and scalable treatments for this age group is therefore essential. This study aimed to compare the efficacy of face-to-face and internet-based unified transdiagnostic treatment in reducing symptoms and improving functioning in adolescents with anxiety and depressive disorders. A pilot randomized controlled trial (RCT) with pre-test, post-test, and follow-up assessments was conducted to compare the efficacy of face-to-face and internet-based treatments. Forty-nine adolescents (aged 13-18 years) from Tehran, Alborz, Gilan, and Kerman were randomly assigned to one of three groups: face-to-face treatment, internet-based treatment, or a control group. Asse..."
"Oskar Hansson","","Professor","Computer Science","https://openalex.org/A5049277633","The objective of this study was to determine the predictive value of amyloid-positron emission tomography (PET) versus the plasma ratio of phosphorylated tau at threonine 217 (p-tau217) to non-phosphorylated tau217 (%p-tau217) for tau-PET transitions (T- to T+). The added value of combining plasma amyloid-β 42 and amyloid-β 40 (Aβ42/40) and %p-tau217 into an amyloid probability score (APS2) was also assessed. Mayo Clinic Study of Aging (MCSA) participants had plasma markers measured at via mass spectrometry (MS), an amyloid-PET scan, and a tau-PET (meta-temporal region of interest [ROI]) negative scan (standardized uptake value ratio [SUVR] <1.29) at the index (baseline) date, along with one or more follow-up tau-PET scans. The BioFINDER-2 cohort was used for validation. Cox proportional hazards models adjusted for age, sex, and apolipoprotein (APOE) ε4 were used to assess predictors, with scaling to the interquartile range (IQR) for comparability of hazard ratios (HR). Among 255 tau-P...

The distribution of tau pathology in Alzheimer's disease (AD) shows remarkable inter-individual heterogeneity, including hemispheric asymmetry. However, the factors driving this asymmetry remain poorly understood. Here we explore whether tau asymmetry is linked to i) reduced inter-hemispheric brain connectivity (potentially restricting tau spread), or ii) asymmetry in amyloid-beta (Aβ) distribution (indicating greater hemisphere-specific vulnerability to AD pathology). We include 452 participants from the Swedish BioFINDER-2 cohort with evidence of both Aβ pathology (CSF Aβ42/40 or neocortical Aβ-PET) and tau pathology (temporal tau-PET), categorising them as left asymmetric (n = 102), symmetric (n = 306), or right asymmetric (n = 44) based on temporal lobe tau-PET uptake distribution. We assess edge-wise inter-hemispheric functional (RSfMRI; n = 318) and structural connectivity (dMRI; n = 352) but find no association between tau asymmetry and connectivity. In contrast, we observe a st...

Developing and validating sensitive visual read algorithms for assessing Alzheimer disease–related tau in tau PET imaging is imperative, considering the implementation of the methodology in clinical practice and trials. Our aim was to compare 2 visual read algorithms for tau PET images to semiquantitative measurements and plasma phospho-tau 217 (p-tau217) status. <b>Methods:</b> In total, 1,654 participants were consecutively recruited from secondary memory clinics in southern Sweden as part of the prospective BioFINDER-2 cohort study (May 2017–September 2023). All participants underwent [<sup>18</sup>F]RO948 scans, and 37 participants underwent an additional [<sup>18</sup>F]flortaucipir scan. PET scans were read visually in accordance with the BioFINDER visual read (BF-VR) protocol and the established visual read method for [<sup>18</sup>F]flortaucipir (FTP-VR). Comparative analyses were conducted with semiquantitative SUV ratios (SUVRs) in the entorhinal cortex (ERC) and a temporal m...

Plasma biomarkers' utility for predicting incident mild cognitive impairment (MCI) remains unclear. We evaluated associations of plasma Alzheimer's disease (AD) biomarkers and amyloid positron emission tomography (PET) with transitions from cognitively unimpaired (CU) to MCI in the Mayo Clinic Study of Aging (MCSA) and BioFINDER-2 studies. Associations of continuous baseline plasma biomarker levels and amyloid PET Centiloid with progression to MCI, adjusting for age, sex, and education, were evaluated with Cox proportional hazards models. The study included 381 MCSA and 584 BioFINDER-2 participants. Amyloid PET and percent phosphorylated to non-phosphorylated tau217 (%p-tau217) were strong predictors of progression to MCI in both cohorts: hazard ratios of 1.49 and 1.23 in the MCSA and 1.72 and 1.65 in BioFINDER, respectively. Amyloid beta 42/40 was a significant predictor in BioFINDER-2 only (hazard ratio 2.20). Plasma %p-tau217 was associated with progression from CU to MCI in both co..."
"Karl Henrik Johansson","","Professor","Computer Science","https://openalex.org/A5045975901",""
"Rajeev Ahuja","","Professor","Computer Science","https://openalex.org/A5037243808","Abstract Inventing new approaches to transform abundant plant matter swiftly into valuable products is crucial for sustainable development. Here, the first direct and continuous conversion of lignocellulosic solid mass into synthetic biogas is reported using blue laser irradiation at multi‐kW/cm 2 intensity. It is demonstrated that the on‐demand generation of a biogas jet, achieving rapid (in milliseconds) and continuous production of a flame jet 300 times larger than the mm‐size laser focus. Remarkably, biogas production occurs exclusively during laser illumination, without triggering bulk combustion, and achieves up to 92% mass conversion. A comprehensive phase diagram of the blue light–wood interaction highlighting three distinct regimes accessed is reported by varying laser parameters and provide mechanistic insight using photothermal simulations. Furthermore, the molecular composition of the synthetic biogas is identified and is shown to result from the photothermal decomposition ...

Abstract Hydroxyapatite (HAP) can serve as a critical permeable reactive barrier for the in situ remediation of 90 Sr radionuclides from groundwater through promoting co‐precipitation. The Sr 2+ leaching associated with the structural degradation of Sr‐HAP in the geological field environment is crucial for lowering groundwater radioactivity; thus, evaluating HAP chemical durability is critical. However, the degradation mechanism of Sr‐doped HAP upon water solutions under the influence of surface charge effects remains unclear and requires further investigation. In this work, the surface degradation and penetration diffusion of Sr‐doped HAP are systematically investigated based on density functional theory and dissolution experiments. Some specific issues such as the preparation process of single‐phase ceramics and surface morphology are discussed in details, along with uncovering the relationship between Sr–O bond and Sr leaching activity. The results reveal that the degradation behavi...

Abstract The interplay of superconducting and topological states in two-dimensional materials has gained intensive attention for exploring novel quantum phenomena and their applications in quantum computing. However, two-dimensional materials exhibiting both superconductivity and topological phases are exceptionally rare. In this context, we investigated two-dimensional CrH 2&#xD;(chromium dihydride) in P6m2 (hexagonal) and P3m1 (trigonal) symmetries using first-principles calculations. We verified the stability of these phases using phonon dispersion and mechanical stability analyses. Based on our Z 2 invariant&#xD;calculations, CrH 2 is topologically nontrivial for the P6m2 symmetry, while it is topologically trivial for the P3m1 symmetry. Using anisotropic Migdal-Eliashberg equations, we find both phases as single-gap superconductors, with transition temperatures of ∼11K for the hexagonal phase and ∼8K for the trigonal phase. The superconducting properties are attributed to electron...

Planar -conjugated borides not only attract fundamental interests in bonding chemistry, but also offers wide-ranging applications in catalysis, bioimaging, and biosensing. To stabilize -conjugated <a:math xmlns:a=""http://www.w3.org/1998/Math/MathML""><a:msub><a:mi mathvariant=""normal"">B</a:mi><a:mn>6</a:mn></a:msub></a:math> rings without out-of-plane bonding, we propose a design strategy that involves inserting low-compressible <c:math xmlns:c=""http://www.w3.org/1998/Math/MathML""><c:mrow><c:mi mathvariant=""normal"">C</c:mi><c:msub><c:mi mathvariant=""normal"">N</c:mi><c:mn>3</c:mn></c:msub></c:mrow></c:math> triangles into tessellation of <f:math xmlns:f=""http://www.w3.org/1998/Math/MathML""><f:msub><f:mi mathvariant=""normal"">B</f:mi><f:mn>6</f:mn></f:msub></f:math> rings to create planar borides. Based on this concept, we designed the planar hypercoordinate boride <h:math xmlns:h=""http://www.w3.org/1998/Math/MathML""><h:mrow><h:mi>ScC</h:mi><h:msub><h:mi mathvariant=""normal"">N</h:mi><h:mn>..."
"Per Svenningsson","","Professor","Computer Science","https://openalex.org/A5036364090","L-DOPA-induced dyskinesia (LID) is a significant and treatment-limiting complication in Parkinson's disease (PD) therapy, yet its mechanisms remain poorly understood. We used high-resolution mass spectrometry imaging to map brain-region-specific alterations of glycerophospholipids and sphingolipids in a female macaque model of PD with and without LID following chronic L-DOPA treatment. LID was associated with depletion of antioxidant plasmalogen phosphatidylcholines in the globus pallidus interna, claustrum, and precentral gyrus-regions critical for motor function-and elevations of polyunsaturated fatty acid-containing glycerophospholipids, indicative of increased membrane fluidity. This lipid profile differed from similarly treated non-dyskinetic animals, suggesting lipid composition mediates differential susceptibility to LID. Lipid alterations correlated strongly with dyskinesia severity, dopamine, and L-DOPA concentrations, supporting a mechanistic link between lipid metabolism, ne...

Dyskinesia is a debilitating complication of dopaminergic therapy in advanced Parkinson's disease. To evaluate the effect of levodopa-carbidopa intestinal gel (LCIG) on dyskinesia burden. This is a post hoc analysis of the retrospective, observational COmedication Study assessing Mono- and cOmbination therapy with levodopa-carbidopa inteStinal gel (COSMOS; NCT03362879). Change in dyskinesia was assessed by LCIG treatment group (monotherapy, daytime monotherapy, polytherapy), baseline dyskinesia duration (<4 vs. ≥4 hours), and dyskinesia severity (troublesome vs. non-troublesome). Correlations between changes in dyskinesia and patient-reported outcomes (Parkinson's Disease Questionnaire-8 [PDQ-8], Parkinson's Disease Sleep Scale-2 [PDSS-2], non-motor symptoms scale [NMSS]) were assessed using Spearman correlation coefficients. The Unified Parkinson's Disease Rating Scale IV measured dyskinesia duration (Item 32), severity (Item 33), and pain (Item 34). Data were collected cross-sectiona...

How dopamine depletion in Parkinson's disease (PD) and subsequent dopamine replacement therapy (DRT) affect brain activity is poorly understood. Typically, brain activity is analysed for its spectral properties and the temporal structure of the activity is often ignored. We quantified the time irreversibility of cortical activity in terms of entropy production rate (EPR). Using the Neural Estimator for Entropy Production (NEEP) algorithm on source-reconstructed resting-state magnetoencephalogram (MEG) data, we estimated the EPR in various brain regions of persons with PD (PwPD) (n = 17) and matched healthy controls (HC, n = 20). PwPD were recorded in two conditions: OFF and ON DRT (one hour after levodopa intake). HC were also recorded in two sessions separated by one hour. Motor symptoms were assessed with Movement Disorders Society-revised Unified Parkinson's Disease Rating Scale (MDS-UPDRS-III). Despite the lack of significant group differences in EPR between the HC and PD groups, w...

Background The potential role of Alzheimer's disease (AD) pathology in contributing to cognitive impairment in α-synucleinopathies warrants in vivo detection of the pathological burden in patients with α-synucleinopathies. Objective Using a fully automated platform, we measured the levels of cerebrospinal fluid (CSF) AD biomarkers, namely Aβ 42 , P-tau181 and T-tau, as well as their derived ratios, P-tau181/Aβ 42 ratio and T-tau/Aβ 42 ratio, in a cohort of Parkinson's disease (PD), dementia with Lewy bodies (DLB), preclinical and prodromal AD (pAD) patients, as well as cognitively normal controls. We binarized the CSF biomarkers and determined the proportion of individuals with abnormal biomarker levels within each diagnostic group. We explored the associations between these biomarkers and cognitive performance. Methods This study consisted of 51 controls, 46 PD, 65 DLB, and 50 pAD patients. CSF biomarkers were measured using Roche Elecsys ® immunoassays. Cognitive performance was base...

Parkinson's disease (PD) is a neurodegenerative movement disorder of high global burden. Uncertainties regarding its exact etiology have been hindering the development of curative therapies. As microglia, the brain's immune cells, are suspected to contribute to neurodegeneration by instigating neuroinflammation, existing anti-inflammatory agents could potentially serve as disease-modifying treatments for PD. Here we evaluated the impact of montelukast, a leukotriene receptor antagonist and anti-inflammatory drug, on motor symptoms and neuropathology in an α-synuclein transgenic mouse model (Line 61) for early onset/genetic PD. Two -weeks -old male Line 61 mice and non-transgenic littermates received daily 10 ​mg/kg montelukast or vehicle orally for 10 weeks. Motor functions were assessed through behavioral tests. Brain tissue was analyzed via unbiased transcriptomics, biochemically, and histologically for various parameters, including microglial and inflammation mediators. Upon montelu..."
"Jens Nielsen","","Professor","Computer Science","https://openalex.org/A5083238115","The presence of organelles is a hallmark distinguishing eukarya from bacteria and archaea, and this culminates in compartmentalization of cellular metabolism and subsequent metabolic specialization. Here we established a dataset encompassing over 300 absolute quantitative proteomes, the largest to date, across two yeast species under diverse experimental conditions. Leveraging big data analysis, formula fitting, and machine learning models, quantitative correlations among protein abundance, organelle-level resource distribution, and cellular phenotypes were elucidated at a system level. We found that protein resources always exhibit robust and precise distribution at the organelle level across distinct conditions. Specifically, at high specific growth rates, the protein mass fraction from some main organelles, i.e., peroxisome and nucleus, is consistently reduced to offset the increasing protein resource demand from the ribosome. Meanwhile, we found that the nutrition limitation could ...

<title>Abstract</title> Cardiometabolic diseases (CMD) are on the rise globally with one billion people expected to suffer from obesity and 643 million from type 2 diabetes by 2030, of which one-third will likely develop chronic kidney disease and two-thirds will die from cardiovascular disease (CVD). However, the mechanistic and molecular drivers of the transition from health to disease remain elusive. Here, in 275 metabolically healthy individuals recruited to the MetaCardis study, we identify a gut microbiome-kidney-heart axis that is predictive of future cardiovascular events. This axis, as evidenced by the associations between gut microbial metabolism of phenylalanine and tyrosine with variations in both kidney functon(as measured by estimated glomerular filtration rate) and circulating pro-atrial natriuretic peptide concentration, shows a depletion pattern in metabolically unhealthy participants of the MetaCardis study (n = 1,602) indicating a loss of health-sustaining microbiome...

The famous model organism Saccharomyces cerevisiae is widely present in a variety of natural and human-associated habitats. Despite extensive studies of this organism, the metabolic mechanisms driving its adaptation to varying niches remain elusive. We here gathered genomic resources from 1,807 S. cerevisiae strains and assembled them into a high-quality pangenome, facilitating the comprehensive characterization of genetic diversity across isolates. Utilizing the pangenome, 1,807 strain-specific genome-scale metabolic models (ssGEMs) were generated, which performed well in quantitative predictions of cellular phenotypes, thus helping to examine the metabolic disparities among all S. cerevisiae strains. Integrative analyses of fluxomics and transcriptomics with ssGEMs showcased ubiquitous transcriptional regulation of metabolic flux in specific pathways (i.e., amino acid synthesis) at a population level. Additionally, the gene/reaction inactivation analysis through the ssGEMs refined by...

Abstract Generating longitudinal and multi-layered big biological data is crucial for effectively implementing artificial intelligence (AI) and systems biology approaches in characterising whole-body biological functions in health and complex disease states. Big biological data consists of multi-omics, clinical, wearable device, and imaging data, and information on diet, drugs, toxins, and other environmental factors. Given the significant advancements in omics technologies, human metabologenomics, and computational capabilities, several multi-omics studies are underway. Here, we first review the recent application of AI and systems biology in integrating and interpreting multi-omics data, highlighting their contributions to the creation of digital twins and the discovery of novel biomarkers and drug targets. Next, we review the multi-omics datasets generated worldwide to reveal interactions across multiple biological layers of information over time, which enhance precision health and ..."
"Arthur Llewellyn Photographer Basham","","Professor","Computer Science","https://openalex.org/A5111014648","The centralizing bureaucratic Mauryan Empire produced a political atmosphere very different from that which was to be found in the quasi-feudal kingdoms of later times. Speculations from all the periods have become part of the common Indian heritage. Ancient India allowed considerable freedom of speculation. It is hardly necessary to point out that, as well as the six orthodox philosophical systems, schismatic and heterodox schools of thought flourished freely, and differences in metaphysics and theology were to some extent reflected in the realm of political ideas. The orthodox conception of kingship was certainly the more influential in the thought of pre-Muslim India, and we find even Buddhist and Jain kings laying claim to divinity. Rajya, the term generally translated 'state' and used in that sense in modern Indian languages, is a secondary nominal formation from the word raja, and etymologically implies 'that which pertains to the king'."
"B. Meirose","","Professor","Computer Science","https://openalex.org/A5016994750","Abstract We explore the decay of free neutrons into exotic long-lived particles, whose decays could be detected in the next-generation free neutron experiments. We show that such a possibility is viable as long as the exotic particle is highly mass-degenerate with the neutron, avoiding exclusion by large-volume detectors. We estimate the number of observable events and identify the most promising final states from both theoretical and experimental perspectives. Our analysis highlights the unique capability of the HIBEAM-NNBAR experiment at the European Spallation Source to probe this unexplored region of parameter space, opening a new avenue for exploring physics beyond the Standard Model. We estimate that several events per year could be observed in the NNBAR experiment.

The European Spallation Source, currently under construction in Lund, Sweden, is a multidisciplinary international laboratory. Once completed to full specifications, it will operate the world’s most powerful pulsed neutron source. Supported by a 3 million Euro Research and Innovation Action within the EU Horizon 2020 program, a design study (HighNESS) has been completed to develop a second neutron source located below the spallation target. Compared to the first source, designed for high cold and thermal brightness, the new source has been optimized to deliver higher intensity, and a shift to longer wavelengths in the spectral regions of cold (CN, 2–20 Å), very cold (VCN, 10–120 Å), and ultracold (UCN, >500 Å) neutrons. The second source comprises a large liquid deuterium moderator designed to produce CN and support secondary VCN and UCN sources. Various options have been explored in the proposed designs, aiming for world-leading performance in neutronics. These designs will enable the...

A key aim of the HighNESS project for the European Spallation Source is to enable cutting-edge particle physics experiments. This volume presents a conceptual design report for the NNBAR experiment. NNBAR would exploit a new cold lower moderator to make the first search in over thirty years for free neutrons converting to anti-neutrons. The observation of such a baryon-number-violating signature would be of fundamental significance and tackle open questions in modern physics, including the origin of the matter-antimatter asymmetry. This report shows the design of the beamline, supermirror focusing system, magnetic and radiation shielding, and anti-neutron detector necessary for the experiment. A range of simulation programs are employed to quantify the performance of the experiment and show how background can be suppressed. For a search with full background suppression, a sensitivity improvement of three orders of magnitude is expected, as compared with the previous search. Civil engin...

The HIBEAM-NNBAR program is a proposed two-stage experiment at the European Spallation Source (ESS) designed to search for baryon number violation, which is – together with C and CP violation – one of the three fundamental Sakharov conditions to explain the observed baryon asymmetry of the Universe. Taking advantage of the ESS' unique capabilities as the future brightest neutron source, the experiment will make high sensitivity searches for neutrons converting into antineutrons and/or sterile neutrons.

The European Spallation Source (ESS), presently under construction in Lund, Sweden, is a multidisciplinary international laboratory that, once completed at full specifications, will operate the world’s most powerful pulsed neutron source. Supported by a 3 M Euro Research and Innovation Action within the European Union Horizon 2020 program, a design study (HighNESS) is now underway to develop a second neutron source located below the spallation target. Compared to the first source, which is located above the spallation target and designed for high cold and thermal brightness, the new source is being optimized to deliver higher intensity and a shift to longer wavelengths in the spectral regions of cold neutrons (CNs) (2 to 20 Å), very cold neutrons (VCNs) (10 to 120 Å), and ultracold neutrons (UCNs) (>500 Å). The second source consists of a large liquid deuterium moderator to deliver CNs and serve secondary VCN and UCN sources, for which different options are under study. These new sourc..."
"Sven Ove Ögren","","Professor","Computer Science","https://openalex.org/A5035523228","Background: The beat-by-beat fluctuation of heart rate (HR) in its temporal sequence (HR dynamics) provides information on HR regulation by the autonomic nervous system (ANS) and its dysregulation in pathological states. Commonly, linear analyses of HR and its variability (HRV) are used to draw conclusions about pathological states despite clear statistical and translational limitations. Objective: The main aim of this study was to compare linear and nonlinear HR measures, including detrended fluctuation analysis (DFA), based on ECG recordings by radiotelemetry in C57BL/6N mice to identify pathological HR dynamics. Methods: We investigated different behavioral and a wide range of pharmacological interventions which alter ANS regulation through various peripheral and/or central mechanisms including receptors implicated in psychiatric disorders. This spectrum of interventions served as a reference system for comparison of linear and nonlinear HR measures to identify pathological states. ...

Abstract Background Major depressive disorder (MDD) is defined as a complex mental disorder which is characterized by a pervasive low mood and aversion to activity. Several types of neurotransmitter systems e.g. serotonergic, glutamatergic and noradrenergic systems have been suggested to play an important role in the origination of depression, but neurotrophins such as brain derived neurotrophic factor (BDNF) have also been implicated in the disease process. Objectives The purpose of this study was to examine the effects of a newly developed class of molecules, characterized as positive allosteric modulators of neurotrophin/Trk receptor mediated signaling (Trk-PAM), on neurotransmitter release and depression-like behavior in vivo. Methods The effect of and possible interaction of neurotrophin/Trk signaling pathways with serotonergic and glutamatergic systems in the modulation of depression-related responses was studied using newly developed Trk-PAM compounds (ACD855, ACD856 and AC26845...

There is evidence that interaction between the neuropeptide galanin and the 5-HT1A receptor represents an integrative mechanism in the regulation of serotonergic neurotransmission. Thus, in rats intracerebroventricular (i.c.v.) galanin did not impair retention in the passive avoidance (PA) test 24 h after training, but attenuated the retention deficit caused by subcutaneous (s.c.) administration of the 5-HT1A receptor agonist 8-OH-DPAT. This impairment has been linked to postsynaptic 5-HT1A receptor activation. To confirm these results in mice, galanin was infused i.c.v. (1 nmol/mouse) in C57BL/6/Bkl mice 30 min prior to training followed by s.c. injection (0.3 mg/kg) of 8-OH-DPAT or saline 15 min before PA training. In line with previous results, i.c.v. galanin significantly attenuated the PA impairment caused by 5-HT1A receptor activation in mice. To study if the galanin 5-HT1A receptor interaction involved the dorsal hippocampus, galanin (1 nmol/mouse) was directly infused into this...

Abstract Adult neurogenesis, the production of newborn neurons from neural stem cells (NSCs) has been suggested to be decreased in patients with schizophrenia. A similar finding was observed in an animal model of schizophrenia, as indicated by decreased bromodeoxyuridine (BrdU) labelling cells in response to a non-competitive N-methyl-d-aspartate (NMDA) receptor antagonist. The antipsychotic drug clozapine was shown to counteract the observed decrease in BrdU-labelled cells in hippocampal dentate gyrus (DG). However, phenotypic determination by immunohistochemistry analysis could not reveal whether BrdU-positive cells were indeed NSCs. Using a previously established cell model for analysing NSC protection in vitro, we investigated a protective effect of clozapine on NSCs. Primary NSCs were isolated from the mouse subventricular zone (SVZ), we show that clozapine had a NSC protective activity alone, as evident by employing an ATP cell viability assay. In contrast, haloperidol did not sh..."
"L. Eklund","","Professor","Computer Science","https://openalex.org/A5106498434","Abstract The LHCb detector has undergone a major upgrade for LHC Run 3. This Upgrade I detector facilitates operation at higher luminosity and utilises full-detector information at the LHC collision rate, critically including the use of vertex information. A new vertex locator system, the VELO Upgrade, has been constructed. The core element of the new VELO are the double-sided pixelated hybrid silicon detector modules which operate in vacuum close to the LHC beam in a high radiation environment. The construction and quality assurance tests of these modules are described in this paper. The modules incorporate 200 μm thick, n-on-p silicon sensors bump-bonded to 130 nm technology ASICs. These are attached with high precision to a silicon microchannel substrate that uses evaporative CO 2 cooling. The ASICs are controlled and read out with flexible printed circuits that are glued to the substrate and wire-bonded to the chips. The mechanical support of the module is given by a carbon fibre p...

The LHCb detector has undergone a major upgrade for LHC Run 3. This Upgrade I detector facilitates operation at higher luminosity and utilises full-detector information at the LHC collision rate, critically including the use of vertex information. A new vertex locator system, the VELO Upgrade, has been constructed. The core element of the new VELO are the double-sided pixelated hybrid silicon detector modules which operate in vacuum close to the LHC beam in a high radiation environment. The construction and quality assurance tests of these modules are described in this paper. The modules incorporate 200 \mum thick, n-on-p silicon sensors bump-bonded to 130 \nm technology ASICs. These are attached with high precision to a silicon microchannel substrate that uses evaporative CO$_2$ cooling. The ASICs are controlled and read out with flexible printed circuits that are glued to the substrate and wire-bonded to the chips. The mechanical support of the module is given by a carbon fibre plate...

Abstract The thermal properties of the LHCb Vertex Locator (VELO) are studied using the real-time detector alignment procedure. The variation of the position and orientation of the detector elements as a function of the operating temperature of the VELO is presented. This study uses a dataset collected by the LHCb experiment during a VELO temperature scan performed at the end of LHC Run 2 (October 2018). Significant shrinkage of the VELO modules is observed at the operating temperature of -30°C compared to the laboratory measurements on a single module taken at a range of temperatures from +45°C to -25°C. The thermal shrinkage expected from the extrapolation of laboratory measurements to lower temperatures, and the results of this alignment study are in good agreement.

The NNBAR experiment for the European Spallation Source will search for free neutrons converting to antineutrons with an expected sensitivity improvement of three orders of magnitude compared to the last such search. This paper describes both the simulations of a key component for the experiment, the neutron optical reflector and the expected gains in sensitivity."
"A. Kupść","","Professor","Computer Science","https://openalex.org/A5002445204","We propose a novel approach to measure the spin polarization of protons produced in electron-positron collisions. Using existing tracking devices and supporting structure material, general-purpose spectrometers can be utilized as a large-acceptance polarimeter without a hardware upgrade. With the proposed approach, the spin polarization of protons can be revealed, providing a complementary and accurate description of the final-state particles. This could have far-reaching implications, such as enabling the complete determination of the nucleon timelike electromagnetic form factors.

We present the results of Phase I of an ongoing review of Monte Carlo tools relevant for low-energy hadronic cross sections. This includes a detailed comparison of Monte Carlo codes for electron–positron scattering into a muon pair, pion pair, and electron pair, for scan and radiative-return experiments. After discussing the various approaches that are used and effects that are included, we show differential cross sections obtained with AfkQed, BabaYaga@NLO, KKMC, MCGPJ, McMule, Phokhara, and Sherpa, for scenarios that are inspired by experiments providing input for the dispersive evaluation of the hadronic vacuum polarisation.

Abstract Decays of charmonium into hyperon and antihyperon pairs provide a pristine laboratory for exploring hyperon properties, such as their polarization and decay parameters, and for conducting tests of fundamental symmetries. This brief review highlights the significant progress made in precise tests of CP symmetry at BESIII using entangled hyperon-antihyperon pairs, including ΛΛ, ΣΣ, ΞΞ and ΛΣ, selected from the high statistics of J/ψ and ψ (3686) events produced in e + e - annihilations. These recent findings have sparked renewed interest in both theoretical and experimental aspects of hyperon physics, but there is still much room for improvement to reach the Standard Model expectations. To address this challenge, the prospects for future investigations on CP asymmetry at next-generation experiments are discussed.

Decays of charmonium into hyperon and antihyperon pairs provide a pristine laboratory for exploring hyperon properties, such as their polarization and decay parameters, and for conducting tests of fundamental symmetries. This brief review highlights the significant progress made in precise tests of CP symmetry at BESIII using entangled hyperon-antihyperon pairs, including $\Lambda\bar{\Lambda}$, $\Sigma\bar{\Sigma}$, $\Xi\bar{\Xi}$ and $\Lambda\bar{\Sigma}$, selected from the high statistics of $J/\psi$ and $\psi(3686)$ events produced in $e^+e^-$ annihilations. These recent findings have sparked renewed interest in both theoretical and experimental aspects of hyperon physics, but there is still much room for improvement to reach the Standard Model expectations. To address this challenge, the prospects for future investigations on CP asymmetry at next-generation experiments are discussed."
"V. Vorobyev","","Professor","Computer Science","https://openalex.org/A5061158447","The paper is devoted to the problem of state variables observers synthesis for linear stationary system operating under condition of noise or disturbances in the measurement channel. The paper considers a completely observable linear stationary system with known parameters. It is assumed that the state variables are not measured, and the measured output variable contains a small amplitude (in general, modulo less than one) additive noise or disturbance. It is also assumed that there is no a priori information about the disturbance or noise in the measurement channel (for example, frequency spectrum, covariance, etc.). It is well known that many observer synthesis methods have been obtained for this type of systems, including the Kalman filter, which has proven itself in practice. Under the condition of complete observability and the presence of some a priori information about a random process (which is typical for the case when a disturbance in the measurement channel can be represente...

Аннотация.Рассмотрено классическое линейное регрессионное уравнение, содержащее в левой и правой частях: измеряемый сигнал и сумму из n слагаемых, состоящих из произведения неизвестных параметров и известных функций (регрессоров).Отличительной особенностью рассматриваемого уравнения, по сравнению с классическим, является допущение о том, что неизвестные параметры являются нелинейными комбинациями от одного, а именно: каждый из неизвестных параметров является числом, полученным при возведении в степень одного неизвестного параметра"
"Olle Eriksson","","Professor","Computer Science","https://openalex.org/A5064187188","LiFe6Ge4, with a theoretically predicted saturation magnetization of 1 T, a magnetocrystalline anisotropy energy of 1.78 MJ/m3 and a Curie temperature of 620 K was suggested to be a promising permanent magnet as an outcome of a data-mining search. Magnetic measurements of the synthesized sample are reported here. Unfortunately, experiments revealed a weak ferromagnetic behaviour with magnetization values much below that predicted by theory. This discrepancy is analyzed in detail, and is attributed to the trigonal crystal symmetry that was missed in the previous characterisation of the material. The correct crystal structure is R 3‾ mH (space group 166) and it is found here to have an antiferromagnetic ground state, as opposed to a theoretically predicted ferromagnetic state of the previously reported monoclinic crystal structure. Theoretical calculations show that element substitution can stabilize a ferromagnetic state of the trigonal crystal structure, with high values of saturation ...

The nonstoichiometric <a:math xmlns:a=""http://www.w3.org/1998/Math/MathML""><a:mrow><a:msub><a:mi>Fe</a:mi><a:mn>2</a:mn></a:msub><a:mi mathvariant=""normal"">P</a:mi></a:mrow></a:math>-type <c:math xmlns:c=""http://www.w3.org/1998/Math/MathML""><c:mrow><c:msub><c:mi>FeMn</c:mi><c:mrow><c:mo>(</c:mo><c:mn>1</c:mn><c:mo>−</c:mo><c:mi>x</c:mi><c:mo>)</c:mo></c:mrow></c:msub><c:msub><c:mi mathvariant=""normal"">V</c:mi><c:mi>x</c:mi></c:msub><c:msub><c:mrow><c:mo>(</c:mo><c:msub><c:mi mathvariant=""normal"">P</c:mi><c:mrow><c:mn>0.5</c:mn></c:mrow></c:msub><c:msub><c:mi>Si</c:mi><c:mrow><c:mn>0.5</c:mn></c:mrow></c:msub><c:mo>)</c:mo></c:mrow><c:mrow><c:mn>1</c:mn><c:mo>−</c:mo><c:mi>x</c:mi></c:mrow></c:msub></c:mrow></c:math> alloys <f:math xmlns:f=""http://www.w3.org/1998/Math/MathML""><f:mrow><f:mo>(</f:mo><f:mi>x</f:mi><f:mo>=</f:mo><f:mn>0</f:mn><f:mo>,</f:mo><f:mn>0.01</f:mn><f:mo>,</f:mo><f:mo> </f:mo><f:mn>0.02</f:mn><f:mo>,</f:mo><f:mo> </f:mo><f:mtext>and</f:mtext><f:mo> </f:mo><f:mn>0.03..."
"Reinhilde Jacobs","","Professor","Computer Science","https://openalex.org/A5041955556","Medication-related osteonecrosis of the jaw (MRONJ) is an adverse event often associated with the use of antiresorptive drugs. This systematic review aims to identify genes and their polymorphisms associated with the risk of developing MRONJ in patients with oncological or skeletal-related diseases treated with antiresorptive drugs. A systematic literature review was conducted in accordance with PRISMA guidelines. Three electronic databases (PubMed, Scopus, and Web of Science) were searched for studies published up to December 2024. The search strategy included terms such as ""MRONJ"", ""bone density conservation agents"", and ""genetics"". Eligible studies were case control in design, investigating genetic polymorphisms in MRONJ patients compared to controls, which included either healthy individuals or patients receiving antiresorptive drugs without developing MRONJ. Study quality was assessed using the Q-genie tool for genetic association studies. Out of 833 retrieved articles, 27 met the...

Nanoparticles are emerging as transformative agents in endodontics, addressing challenges in treating the dentin-pulp complex. This scoping review aims to explore multifunctional applications of nanoparticles in endodontics, with a focus on their roles in promoting tissue regeneration through therapeutic effects, enhancing material properties, and serving a carrier function. Following PRISMA-Scoping Review guidelines, a comprehensive literature search was conducted across Web of Science, PubMed, and Scopus. A total of 490 articles were initially identified, of which 92 met the preliminary eligibility criteria. Following full-text screening, 70 studies were included in the qualitative synthesis. Key findings from both in vitro and in vivo studies are summarized in tabular form. Results reveal a notable imbalance in the types of nanomaterials studied: inorganic nanomaterials were reported in 77% of the studies, while only 23% investigated organic nanomaterials. Despite their lower repres...

More than 1 billion individuals worldwide have experienced dental trauma, particularly children aged 7 to 12 y, predominantly affecting the anterior teeth, which has a significant impact on oral health and esthetics. Rapid emergency restorations using composite resin are followed by medium-term lab-fabricated mock-ups. Recent advancements in artificial intelligence (AI) assist dental restorations, and the objective of this study was to compare the performances of different AI approaches for the learning and reconstruction of central incisors. The study was approved by ethical committees and followed AI in dentistry recommendations. STL files of mature permanent maxillary incisors without severe wear were collected from 3 universities. Principal component analysis (PCA) and Deep Learning of Signed Distance Functions (DeepSDF) models were trained using these files. The learning of PCA and DeepSDF approaches were 3-fold cross-validated, and their performances were assessed using the follo..."
"Cynthia M. Bulik","","Professor","Computer Science","https://openalex.org/A5003258692","According to DSM-5-TR, avoidant/restrictive food intake disorder (ARFID) cannot be diagnosed alongside anorexia nervosa (AN), bulimia nervosa (BN), or any other body image disturbance. This does not accurately reflect real-world symptomatology and recent research findings, indicating the potential need to revise DSM-5-TR Criteria. In this study, we investigated the co-occurrence of weight- and/or shape-motivated restriction (WSR) in a large sample of adults who screened positive for ARFID, thereby providing evidence to inform such changes. The sample comprised 5,747 adults who consented to participate in the ARFID-Genes and Environment (ARFID-GEN) research study, screened positive for ARFID on the NIAS and PARDI-AR-Q, and completed the EDE-Q. Based on EDE-Q responses, participants were placed into four groups: groups one and two additionally screened positive for AN (ARFID-AN; n=147) or BN (ARFID-BN; n=193), group three also endorsed significant WSR without meeting AN or BN screening c...

Abstract Rare copy number variants (CNVs) are a key component of the genetic basis of psychiatric conditions, but have not been well characterized for most. We conducted a genome-wide CNV analysis across six diagnostic categories (N = 574,965): autism (ASD), ADHD, bipolar disorder (BD), major depressive disorder (MDD), PTSD, and schizophrenia (SCZ). We identified 35 genome-wide significant associations at 18 loci, including novel associations in SCZ ( SMYD3, USP7 - HAPSTR1 ) and in the combined cross-disorder analysis ( ASTN2 ). Rare CNVs accounted for 1–3% of heritability across diagnoses. In ASD, associations were uniformly positive, consistent with autism having diverse etiologies and clinical presentations. By contrast, CNVs showed a dose-dependent relationship for other diagnoses, including SCZ and PTSD, with reciprocal deletions and duplications having inversely correlated effects and distinct genotype-phenotype relationships. Our findings suggest that genes have effects that are...

Psychiatric conditions share common genes, but mechanisms that differentiate diagnoses remain unclear. We present a multidimensional framework for functional analysis of rare copy number variants (CNVs) across 6 diagnostic categories, including schizophrenia (SCZ), autism (ASD), bipolar disorder (BD), depression (MDD), PTSD, and ADHD (N = 574,965). Using gene-set burden analysis (GSBA), we tested duplication (DUP) and deletion (DEL) burden across 2,645 functional gene sets defined by the intersections of pathways, cell types, and cortical regions. While diagnoses converge on shared pathways, mixed-effects modeling revealed divergence of pathway effects by cell type, brain region, and gene dosage. Factor analysis identified latent dimensions aligned with clinical axes. A primary factor (F1) captured reciprocal dose-dependent effects of DUP and DEL in SCZ reflecting positive and negative effects in excitatory versus inhibitory neurons and association versus sensory cortex. SCZ and ASD we...

Eating disorders arise from a complex interaction of genetic and environmental influences. Here we provide comprehensive population-level estimates of the heritability of eating disorders and their genetic relationships with various mental health and cardiometabolic disorders (CMDs), expanding beyond genome-wide association studies. We examined the heritability of three eating disorders-anorexia nervosa (AN), bulimia nervosa (BN), and other eating disorders (OED)-and investigated shared familial and genetic risk factors with mental health disorders and CMDs. Using national register data from Denmark and Sweden (1972-2016), we analysed clinical diagnoses for over 67,000 individuals with eating disorders, their first-degree relatives, and matched controls from populations totalling 17 million. Heritability estimates were moderate, h2AN = 36%, h2BN = 39%, and h2OED = 30% and genetic correlations revealed substantial overlap between AN and obsessive-compulsive disorder (rg = 0.65) and mode...

The Psychiatric Genomics Consortium (PGC) has fueled discoveries of common and rare genetic variation contributing to liability to many psychiatric and neurodevelopmental conditions. This narrative review reflects on major findings from the past half decade of research by this international group of investigators in five priority areas: discovery of common variants using GWAS; rare variation and its interplay with polygenic risk; leveraging genetics to go beyond diagnostic boundaries; ascribing functional attributes to genomic discoveries; and developing and implementing processes for data sharing, outreach to various communities, and training. The insights gained in these domains frame the agenda for the next phase of PGC research. In addition to accelerating integrative common and rare variant-, within- and across-disorder findings for multiple psychiatric and neurodevelopmental conditions, the next phase will leverage multiple populations to further elucidate genetic etiologies, int..."
"Nancy L. Pedersen","","Professor","Computer Science","https://openalex.org/A5011350655","Abstract Background Polygenic scores (PGSs) may help assess genetic predisposition to multifactorial traits. We examined whether age, sex, and leisure-time physical activity (LTPA) modify the association between a PGS for handgrip strength (HGS) and measured HGS in older adults. Methods PGS HGS, based on Pan-UK Biobank GWAS data, was calculated for 5103 participants (aged 40–96; 44% women) from eight twin cohorts in Denmark, Sweden, Australia, the United States, and Finland within the IGEMS consortium. Sex-standardized HGS and self-reported LTPA were assessed cross-sectionally. Linear mixed models estimated associations between PGS and HGS, including interactions with age, country, and LTPA, as well as an association between PGS and LTPA. Fixed-effect within-pair models were conducted to assess environmental contributions. Results Higher PGS was associated with greater HGS (β = 2.14, SE = 0.15, p < 0.001), explaining 4.6% of HGS variance overall, with modest variation across countries....

Abstract Low educational attainment is recognized as a modifiable risk factor for dementia. Despite the commonly accepted notion that greater educational attainment confers lower dementia risk, few family-based studies have investigated the causal bases for the association. Using data from seven twin samples from Sweden, Denmark, Australia, and the US participating in the IGEMS (Interplay of Genes and Environment in Multiple Studies) consortium ( N = 60,027, 10.92% with dementia), we tested whether twins who achieve higher education than their co-twins have lower risk of dementia. The primary analysis applied a multilevel between-within regression framework, supported by descriptive statistics of within-pair differences. Results confirmed an overall association between educational attainment and dementia risk, such that individuals with higher educational attainment had less likelihood of developing dementia (phenotypic regression coefficient = -0.68, p <.0001). Within twin pairs, howe...

In later older adulthood, individuals report increased depressive symptoms, whereas gender differences in depressive symptoms narrow. We evaluated whether terminal decline (i.e., accelerated worsening in proximity to death) explained these patterns. We examined the longitudinal trajectories of depressive symptoms in 2,411 participants (baseline age: 29–95 years) from the Interplay of Genes and Environments Across Multiple Studies consortium representing three countries (Sweden, Denmark, and Australia). Joint modeling revealed that individuals reporting larger annual increases in depressive symptoms after age 70 were at increased risk of death. Piecewise linear multilevel models with random changepoints revealed accelerated increases in depressive symptoms approximately 4 years before death. Co-twin control analyses with 98 twin pairs found that the deceased twin had significantly larger accelerations in depressive symptoms compared with the surviving twin. Men experienced more severe m...

Advanced age is the most important risk factor for dementia. Measures of biological ageing such as DNA methylation age (DNAmAge) can give more information about the accumulation of age-related molecular damage in different organs than chronological age alone. Using post-mortem brain tissue from Swedish Twin Registry participants, we explored the relationship between lifestyle factors, dementia and DNAmAge measures from prefrontal cortex and cerebellum (n = 27 individuals) and paired blood samples (n = 20 individuals). We observed that smoking was associated with a higher DNAmAge deviation (PCBrainAge + 6.4 years in prefrontal cortex, CI [2.5, 10.3], p = 0.004). Conversely, a longer time spent in formal education was associated with a lower DNAmAge deviation (DNAmClockCortical - 4.8 years in prefrontal cortex, CI [-7.9, -1.8], p = 0.007). We found no significant differences between DNAmAge deviation of dementia cases versus controls, though among dementia cases there was a tendency towa..."
"K. Jon-And","","Professor","Computer Science","https://openalex.org/A5018094254","Silicon pixel detectors are at the core of the current and planned upgrade of the ATLAS experiment at the LHC. Given their close proximity to the interaction point, these detectors will be exposed ...

This Technical Design Report describes the project to upgrade the ATLAS Tile Calorimeter for the operation at the High Luminosity LHC. The High Luminosity LHC is planned to begin operation in 2026 ...

The ATLAS Collaboration measures the inclusive production of Z bosons via their decays into electron and muon pairs in p+Pb collisions at sNN=5.02TeV at the Large Hadron Collider. The measurements are made using data corresponding to integrated luminosities of 29.4 and 28.1 nb−1 for Z→ee and Z→μμ, respectively. The results from the two channels are consistent and combined to obtain a cross section times the Z→ℓℓ branching ratio, integrated over the rapidity region |yZ*|<3.5, of 139.8±4.8(statistical)±6.2(systematic)±3.8 (luminosity) nb. Differential cross sections are presented as functions of the Z boson rapidity and transverse momentum and compared with models based on parton distributions both with and without nuclear corrections. The centrality dependence of Z boson production in p+Pb collisions is measured and analyzed within the framework of a standard Glauber model and the model's extension for fluctuations of the underlying nucleon-nucleon scattering cross section.2 MoreReceive...

This paper summarises the mechanical construction and installation of the Tile Calorimeter for the ATLAS experiment at the Large Hadron Collider in CERN, Switzerland. The Tile Calorimeter is a sampling calorimeter using scintillator as the sensitive detector and steel as the absorber and covers the central region of the ATLAS experiment up to pseudorapidities ±1.7. The mechanical construction of the Tile Calorimeter occurred over a period of about 10 years beginning in 1995 with the completion of the Technical Design Report and ending in 2006 with the installation of the final module in the ATLAS cavern. During this period approximately 2600 metric tons of steel were transformed into a laminated structure to form the absorber of the sampling calorimeter. Following instrumentation and testing, which is described elsewhere, the modules were installed in the ATLAS cavern with a remarkable accuracy for a structure of this size and weight.

The Tile Calorimeter, covering the central region of the ATLAS experiment up to pseudorapidities of ±1.7, is a sampling device built with scintillating tiles that alternate with iron plates. The light is collected in wave-length shifting (WLS) fibers and is read out with photomultipliers. In the characteristic geometry of this calorimeter the tiles lie in planes perpendicular to the beams, resulting in a very simple and modular mechanical and optical layout. This paper focuses on the procedures applied in the optical instrumentation of the calorimeter, which involved the assembly of about 460,000 scintillator tiles and 550,000 WLS fibers. The outcome is a hadronic calorimeter that meets the ATLAS performance requirements, as shown in this paper."
"T. Krojer","","Professor","Computer Science","https://openalex.org/A5016800053","Coronavirus outbreaks have occurred over the past 25 years with SARS-CoV-2 (severe acute respiratory syndrome coronavirus-2) causing a global pandemic. The SARS-CoV-2 non-structural proteins 10 (nsp10) and 14 (nsp14) are considered as potential drug targets. Nsp10 stimulates the 3'-to-5' exoribonuclease (ExoN) activity of nsp14. The ExoN domain excises mis-incorporated nucleotides from the nascent RNA chain and therefore causes resistance to nucleoside analogue drugs. We crystallized the nsp10-nsp14 ExoN complex in distinct space groups, allowing us to describe conformational changes. In particular, the general base, His268, classifying the ExoN domain as a member of the DEDDh family, is trapped in the inactive and active orientations. By X-ray fragment screening, we identified five novel fragment binding sites in the nsp10-nsp14 interface, the hinge region connecting ExoN and N7-methyltransferase domains, and on nsp10. One new site in the nsp10-nsp14 interface accommodates nine struct...

The first multi-bend achromat based synchrotron MAX IV operates two protein crystallography beamlines, BioMAX and MicroMAX. BioMAX is designed as a versatile, stable, high-throughput beamline catering for most protein crystallography experiments. MicroMAX is a more ambitious beamline dedicated to serial crystallography including time-resolved experiments. Both beamlines exploit the special characteristics of fourth-generation beamlines provided by the 3 GeV ring of MAX IV. In addition, the fragment-based drug discovery platform, FragMAX, is hosted and, at the FemtoMAX beamline, protein diffraction experiments exploring ultrafast time resolution can be performed. A technical and operational overview of the different beamlines and the platform is given as well as an outlook for protein crystallography embedded in the wider possibilities that MAX IV offers to users in the life sciences.

Fragment approaches are long‐established in target‐based ligand discovery yet their full transformative potential lies dormant, because progressing hits to potency remains underserved by methodological work. The only credible progression paradigm is multiple cycles of costly conventional design‐make‐test‐analyse (DMTA) medicinal chemistry, necessitating picking winners early and discarding others. It is effective to cheaply parallelize large numbers of non‐uniform multi‐step reactions, because, even without compound purification, a high‐quality readout of binding is available, viz. crystallography. This can detect low‐level binding of slightly active compounds, which the targeted binding site extracts directly from crude reaction mixtures (CRMs). In this proof‐of‐concept study, we expand a fragment hit from a crystal‐based screen of the bromodomain PHIP2, using array synthesis on low‐cost robotics to implement 6 independent multi‐step reaction routes of up to 5 steps, attempting the sy...

Fragment approaches are long‐established in target‐based ligand discovery yet their full transformative potential lies dormant, because progressing hits to potency remains underserved by methodological work. The only credible progression paradigm is multiple cycles of costly conventional design‐make‐test‐analyse (DMTA) medicinal chemistry, necessitating picking winners early and discarding others. It is effective to cheaply parallelize large numbers of non‐uniform multi‐step reactions, because, even without compound purification, a high‐quality readout of binding is available, viz. crystallography. This can detect low‐level binding of slightly active compounds, which the targeted binding site extracts directly from crude reaction mixtures (CRMs). In this proof‐of‐concept study, we expand a fragment hit from a crystal‐based screen of the bromodomain PHIP2, using array synthesis on low‐cost robotics to implement 6 independent multi‐step reaction routes of up to 5 steps, attempting the sy...

ABSTRACT The FragMAX facility at MAX IV Laboratory is a state‐of‐the‐art platform for crystallographic fragment screening, designed to support structure‐based drug and chemical tool compound discovery. This facility offers a comprehensive workflow, from high‐throughput crystal preparation and automated diffraction data collection at the BioMAX beamline to advanced data processing and analysis using custom software tools like FragMAXapp and FragMAXproc. Key components include an extensive relational SQLite database, various fragment libraries, laboratory automation equipment, and a range of bespoke software solutions. FragMAX has conducted numerous successful screening campaigns, serving both academic and industrial users. Users benefit from comprehensive support, and stringent data management. Here, we provide an overview of the different components of the facility and details of their practical implementation."
"Henrik Larsson","","Professor","Computer Science","https://openalex.org/A5021664662","Background Subclinical hypomanic symptoms are fairly common in the general population but are linked to psychiatric and neurodevelopmental conditions. However, the genetic and environmental origins of these associations are unclear. This twin study examined the phenotypic and aetiological associations between subclinical hypomania and psychiatric and neurodevelopmental diagnoses. Methods Participants were 4,932 twin pairs from the Child and Adolescent Twin Study in Sweden. Hypomanic symptoms were assessed using the parent‐rated Mood Disorders Questionnaire when the twins were aged 18. Specialist diagnoses of 14 conditions and symptoms were ascertained from Swedish population registries. Phenotypic associations between hypomania and these conditions/symptoms were investigated, and their aetiological overlap was examined using the twin method. Results Subclinical hypomania was significantly associated with all 14 diagnoses. The highest odds were for psychotic disorders (odds ratio [OR] =...

An abstract is not available for this content so a preview has been provided. As you have access to this content, a full PDF is available via the 'Save PDF' action button.

The mental health impact of increasing recreational screen use among youth has raised substantial concerns, yet questions about causality remain unresolved. Using data from ~22,000 Swedish twins followed from age 9 to 24, we examined associations between screen use and internalizing problems using multiple designs, including co-twin control comparisons, to strengthen causal inference. Associations between longer screen time and elevated internalizing symptoms during adolescence persisted in co-twin comparisons, supporting a potential causal link. Bidirectional associations were observed, with internalizing problems at younger ages also associated with later screen time increase. Adolescents who exceeded international screen time recommendations at ages 15 and 18 showed elevated internalizing symptoms at later ages, whereas those who reduced screen time to recommended levels did not. Heavy screen use at age 15, particularly during the weekdays, was associated with higher risk of clinica...

Parental criminality is a risk factor for crime, but little is known about why some individuals exposed to this risk refrain from crime. We explored associations of resting heart rate (RHR), systolic blood pressure (SBP), cognitive ability (CA), and psychological functioning (PF) with criminal convictions among men with a convicted parent, accounting for unmeasured familial factors in sibling analyses. Data were obtained from Swedish registers, including all men born in Sweden between 1958 and 1992 with a convicted parent (N = 495,109), followed for up to 48 years. The potential protective factors were measured at mandatory conscription. Outcomes were conviction of any, violent, and non-violent crime. Survival analyses were used to test for associations, adjusting for measured covariates and unmeasured familial factors. Higher levels of RHR, SBP, CA, and PF were associated with reduced risk of criminality after adjusting for covariates. RHR associations were largely explained by famili..."
"Paul Lichtenstein","","Professor","Computer Science","https://openalex.org/A5046450925","Background Subclinical hypomanic symptoms are fairly common in the general population but are linked to psychiatric and neurodevelopmental conditions. However, the genetic and environmental origins of these associations are unclear. This twin study examined the phenotypic and aetiological associations between subclinical hypomania and psychiatric and neurodevelopmental diagnoses. Methods Participants were 4,932 twin pairs from the Child and Adolescent Twin Study in Sweden. Hypomanic symptoms were assessed using the parent‐rated Mood Disorders Questionnaire when the twins were aged 18. Specialist diagnoses of 14 conditions and symptoms were ascertained from Swedish population registries. Phenotypic associations between hypomania and these conditions/symptoms were investigated, and their aetiological overlap was examined using the twin method. Results Subclinical hypomania was significantly associated with all 14 diagnoses. The highest odds were for psychotic disorders (odds ratio [OR] =...

Abstract Nonsuicidal self-injury (NSSI) often temporally precedes suicide attempts (SA), and SA predicts suicide. The genetic and environmental aetiologies of the transition from NSSI to SA have not been studied. This study aims to investigate whether NSSI reported at age 18 influences the incidence of SA between ages 18 and 24, and to what extent these transitions from NSSI to SA are influenced by shared genetic and environmental factors. Twins born in Sweden were enrolled in this longitudinal population-based twin cohort study. Self-reports of NSSI and SA were collected at ages 18 and 24. The majority of individuals in the analytical sample (N = 3 934) were female (64.9%) and dizygotic twins (65.0%). We found that NSSI reported at age 18 was associated with an increased risk of SA between ages 18 and 24 (Odds Ratio 5.4, 95% CI 3.3–8.7), after adjusting for sex and childhood psychopathology. There was a strong genetic correlation between NSSI reported at age 18 and incidence of SA bet...

The mental health impact of increasing recreational screen use among youth has raised substantial concerns, yet questions about causality remain unresolved. Using data from ~22,000 Swedish twins followed from age 9 to 24, we examined associations between screen use and internalizing problems using multiple designs, including co-twin control comparisons, to strengthen causal inference. Associations between longer screen time and elevated internalizing symptoms during adolescence persisted in co-twin comparisons, supporting a potential causal link. Bidirectional associations were observed, with internalizing problems at younger ages also associated with later screen time increase. Adolescents who exceeded international screen time recommendations at ages 15 and 18 showed elevated internalizing symptoms at later ages, whereas those who reduced screen time to recommended levels did not. Heavy screen use at age 15, particularly during the weekdays, was associated with higher risk of clinica...

Parental criminality is a risk factor for crime, but little is known about why some individuals exposed to this risk refrain from crime. We explored associations of resting heart rate (RHR), systolic blood pressure (SBP), cognitive ability (CA), and psychological functioning (PF) with criminal convictions among men with a convicted parent, accounting for unmeasured familial factors in sibling analyses. Data were obtained from Swedish registers, including all men born in Sweden between 1958 and 1992 with a convicted parent (N = 495,109), followed for up to 48 years. The potential protective factors were measured at mandatory conscription. Outcomes were conviction of any, violent, and non-violent crime. Survival analyses were used to test for associations, adjusting for measured covariates and unmeasured familial factors. Higher levels of RHR, SBP, CA, and PF were associated with reduced risk of criminality after adjusting for covariates. RHR associations were largely explained by famili..."
"G. A. Mullier","","Professor","Computer Science","https://openalex.org/A5016424332","A bstract The Light Dark Matter eXperiment (LDMX) is an electron-beam fixed-target experiment designed to achieve comprehensive model independent sensitivity to dark matter particles in the sub-GeV mass region. An upgrade to the LCLS-II accelerator will increase the beam energy available to LDMX from 4 to 8 GeV. Using detailed GEANT4-based simulations, we investigate the effect of the increased beam energy on the capabilities to separate signal and background, and demonstrate that the veto methodology developed for 4 GeV successfully rejects photon-induced backgrounds for at least 2 × 10 14 electrons on target at 8 GeV.

Abstract The Inner Tracker silicon strip detector (ITk Strip) is a part of the ATLAS upgrade for the HL-LHC. The detector readout and control is accomplished by the interaction of three on-module custom ASICs (ABCStarv1, HCCStarv1 and AMACstar). All ASICs are designed with protections against Single Event Errors. Their resilience at the system-level can be tested using the Board for Evaluation of Triple-chip Single Event Effects (BETSEE). This special board places all three ASICs into the beam-spot of a test beam facility concurrently and allows for module-like operation. The results from irradiating BETSEE with heavy ions and protons will be presented.

The constituents of dark matter are still unknown, and the viable possibilities span a vast range of masses. The physics community has established searching for sub-GeV dark matter as a high priority and identified accelerator-based experiments as an essential facet of this search strategy. A key goal of the accelerator-based dark matter program is testing the broad idea of thermally produced sub-GeV dark matter through experiments designed to directly produce dark matter particles. The most sensitive way to search for the production of light dark matter is to use a primary electron beam to produce it in fixed-target collisions. The Light Dark Matter eXperiment (LDMX) is an electron-beam fixed-target missing-momentum experiment that realizes this approach and provides unique sensitivity to light dark matter in the sub-GeV range. This contribution provides an overview of the theoretical motivation, the main experimental challenges, how LDMX addresses these challenges, and projected sens...

Particle physics experiments rely extensively on computing and data services, making e-infrastructure an integral part of the research collaboration. Constructing and operating distributed computing can however be challenging for a smaller-scale collaboration. The Light Dark Matter eXperiment (LDMX) is a planned small-scale accelerator-based experiment to search for dark matter in the sub-GeV mass region. Finalizing the design of the detector relies on Monte-Carlo simulation of expected physics processes. A distributed computing pilot project was proposed to better utilize available resources at the collaborating institutes, and to improve scalability and reproducibility. This paper outlines the chosen lightweight distributed solution, presenting requirements, the component integration steps, and the experiences using a pilot system for tests with large-scale simulations. The system leverages existing technologies wherever possible, minimizing the need for software development, and dep...

Particle physics experiments rely extensively on computing and data services, making e-infrastructure an integral part of the research collaboration. Constructing and operating distributed computing can however be challenging for a smaller-scale collaboration. The Light Dark Matter eXperiment (LDMX) is a planned small-scale accelerator-based experiment to search for dark matter in the sub-GeV mass region. Finalizing the design of the detector relies on Monte-Carlo simulation of expected physics processes. A distributed computing pilot project was proposed to better utilize available resources at the collaborating institutes, and to improve scalability and reproducibility. This paper outlines the chosen lightweight distributed solution, presenting requirements, the component integration steps, and the experiences using a pilot system for tests with large-scale simulations. The system leverages existing technologies wherever possible, minimizing the need for software development, and dep..."
"Bengt Sundén","","Professor","Computer Science","https://openalex.org/A5066856244",""
"Mathias Uhlén","","Professor","Computer Science","https://openalex.org/A5060241561","Abstract Generating longitudinal and multi-layered big biological data is crucial for effectively implementing artificial intelligence (AI) and systems biology approaches in characterising whole-body biological functions in health and complex disease states. Big biological data consists of multi-omics, clinical, wearable device, and imaging data, and information on diet, drugs, toxins, and other environmental factors. Given the significant advancements in omics technologies, human metabologenomics, and computational capabilities, several multi-omics studies are underway. Here, we first review the recent application of AI and systems biology in integrating and interpreting multi-omics data, highlighting their contributions to the creation of digital twins and the discovery of novel biomarkers and drug targets. Next, we review the multi-omics datasets generated worldwide to reveal interactions across multiple biological layers of information over time, which enhance precision health and ..."
"Per E. Andrén","","Professor","Computer Science","https://openalex.org/A5068745437","<ns3:p>We summarize research reports from 2024 relevant to Tourette syndrome, which the authors consider the most important or interesting. This working draft aims to submit this content for publication around the beginning of 2025 in the yearly Tourette Syndrome Research Highlights series on F1000Research. The authors welcome article suggestions and thoughtful feedback from readers, who can add a comment by clicking on the rectangular comment box icon to the left of the LOG IN link at the top of this page. For private comments, you can reach us by email (andreas.hartmann@aphp.fr or kevin@wustl.edu).</ns3:p>

L-DOPA-induced dyskinesia (LID) is a significant and treatment-limiting complication in Parkinson's disease (PD) therapy, yet its mechanisms remain poorly understood. We used high-resolution mass spectrometry imaging to map brain-region-specific alterations of glycerophospholipids and sphingolipids in a female macaque model of PD with and without LID following chronic L-DOPA treatment. LID was associated with depletion of antioxidant plasmalogen phosphatidylcholines in the globus pallidus interna, claustrum, and precentral gyrus-regions critical for motor function-and elevations of polyunsaturated fatty acid-containing glycerophospholipids, indicative of increased membrane fluidity. This lipid profile differed from similarly treated non-dyskinetic animals, suggesting lipid composition mediates differential susceptibility to LID. Lipid alterations correlated strongly with dyskinesia severity, dopamine, and L-DOPA concentrations, supporting a mechanistic link between lipid metabolism, ne...

<ns3:p>We summarize research reports from 2024 relevant to Tourette syndrome, which the authors consider the most important or interesting. This working draft aims to submit this content for publication around the beginning of 2025 in the yearly Tourette Syndrome Research Highlights series on F1000Research. The authors welcome article suggestions and thoughtful feedback from readers, who can add a comment by clicking on the rectangular comment box icon to the left of the LOG IN link at the top of this page. For private comments, you can reach us by email (andreas.hartmann@aphp.fr or kevin@wustl.edu).</ns3:p>

ABSTRACT One of the main challenges in analyzing chemical messengers in the brain is the optimization of tissue sampling and preparation protocols. Limiting postmortem time and terminating enzyme activity is critical to identify low‐abundance neurotransmitters and neuropeptides. Here, we used a rapid and uniform conductive heat transfer stabilization method that was compared with a conventional fresh freezing protocol. Together with a selective chemical derivatization method and an optimized quantitation approach using deuterated internal standards, we spatially mapped neurotransmitters and their related metabolites by matrix‐assisted laser desorption/ionization mass spectrometry imaging (MALDI‐MSI) in rat brain tissue sections. Although the heat stabilization did not show differences in the levels of dopamine, norepinephrine, and serotonin, their related metabolites 3,4‐dihydroxyphenylacetaldehyde, 3,4‐dihydroxyphenylacetic acid, homovanillic acid, 3‐methoxy‐4‐hydroxyphenylacetaldehyd...

Inhibitors targeting amyloids formed by the human Islet Amyloid Polypeptide (hIAPP) are promising therapeutic candidates for type 2 diabetes. Peptide formulations derived from the nonamyloidogenic rat IAPP (rIAPP) sequence are currently used as hIAPP mimetics to support insulin therapy. rIAPP itself acts as a peptide inhibitor; yet, the structural-level consequences of such inhibition, particularly its impact on amyloid polymorphism, have not been studied in detail. Here, we conduct coaggregation experiments with varying rIAPP-to-hIAPP concentration ratios and employ high-resolution cryo-electron microscopy (Cryo-EM) to elucidate the polymorphism of the resulting fibril structures. Our results demonstrate that the polymorphism of hIAPP amyloids is highly sensitive to the electrostatic environment, which can be modulated by buffer composition, the concentration of the inhibitor, and cosolvents such as hexafluoroisopropanol (HFIP). Under native conditions, rIAPP associates with hIAPP but..."
"E. Coniavitis","","Professor","Computer Science","https://openalex.org/A5041774573","Interpatient variation of tumor radiosensitivity is rarely considered during the treatment planning process despite its known significance for the therapeutic outcome.

A review of the results on Higgs boson decays to leptons with the ATLAS detector at the Large Hadron Collider is presented. In the H→τ+τ− search, using the 8 TeV dataset, there is an excess of data over the background prediction, with an observed (expected) significance corresponding to 4.1σ (3.2σ). In the H→μ+μ− search, using approximately 25 fb−1 of pp collision data collected at 7 TeV and 8 TeV in 2011 and 2012, the data is consistent with the expected background and a 95% confidence level limit of 7.0 times the Standard Model prediction is placed on the signal strength, for a Higgs boson mass of 125.5 GeV.

A dedicated reconstruction algorithm to find decay vertices in the ATLAS muon spectrometer is presented. The algorithm searches the region just upstream of or inside the muon spectrometer volume for multi-particle vertices that originate from the decay of particles with long decay paths. The performance of the algorithm is evaluated using both a sample of simulated Higgs boson events, in which the Higgs boson decays to long-lived neutral particles that in turn decay to bb final states, and pp collision data at sqrt(s) = 7 TeV collected with the ATLAS detector at the LHC during 2011.

A search is presented for direct chargino production based on a disappearing-track signature using 20.3 fb−1 of proton-proton collisions at s=8 TeV collected with the ATLAS experiment at the LHC. In anomaly-mediated supersymmetry breaking (AMSB) models, the lightest chargino is nearly mass degenerate with the lightest neutralino and its lifetime is long enough to be detected in the tracking detectors by identifying decays that result in tracks with no associated hits in the outer region of the tracking system. Some models with supersymmetry also predict charginos with a significant lifetime. This analysis attains sensitivity for charginos with a lifetime between 0.1 and 10 ns, and significantly surpasses the reach of the LEP experiments. No significant excess above the background expectation is observed for candidate tracks with large transverse momentum, and constraints on chargino properties are obtained. In the AMSB scenarios, a chargino mass below 270 GeV is excluded at 95% confide..."
"D. Silvermyr","","Professor","Computer Science","https://openalex.org/A5048740401","The sPHENIX Time Projection Chamber Outer Tracker (TPOT) is a Micromegas based detector. It is a part of the sPHENIX experiment that aims to facilitate the calibration of the Time Projection Chamber, in particular the correction of the time-averaged and beam-induced distortions of the electron drift. This paper describes the detector mission, setup, construction, installation, commissioning and performance during the first year of sPHENIX data taking.

Abstract A large-volume Time Projection Chamber (TPC) is the main tracking and particle identification (PID) detector of the ALICE experiment at the CERN LHC. PID in the TPC is performed via specific energy-loss measurements (d E /d x ), which are derived from the average pulse-height distribution of ionization generated by charged-particle tracks traversing the TPC volume. During Runs 1 and 2, until 2018, the gas amplification stage was based on multiwire proportional chambers (MWPC). Signals from the MWPC show characteristic long negative tails after an initial positive peak due to the long ion drift times in the MWPC amplification region. This so-called ion tail can lead to a significant amplitude loss in subsequently measured signals, especially in the high-multiplicity environment of high-energy Pb-Pb collisions, which results in a degradation of the d E /d x resolution. A detailed study of the signal shapes measured with the ALICE TPC with the Ne-CO 2 (90-10) and Ar-CO 2 (90-10) ..."
"B. Åsman","","Professor","Computer Science","https://openalex.org/A5107824863","The ATLAS detector as installed in its experimental cavern at point 1 at CERN is described in this paper. A brief overview of the expected performance of the detector when the Large Hadron Collider begins operation is also presented.

The hadronic part of the electron structure function F2e has been measured for the first time, using e+e− data collected by the DELPHI experiment at LEP, at centre-of-mass energies of s=91.2–209.5GeV. The data analysis is simpler than that of the measurement of the photon structure function. The electron structure function F2e data are compared to predictions of phenomenological models based on the photon structure function. It is shown that the contribution of large target photon virtualities is significant. The data presented can serve as a cross-check of the photon structure function F2γ analyses and help in refining existing parameterisations.

The PreProcessor system of the ATLAS Level-1 Calorimeter Trigger (L1Calo) receives about 7200 analogue signals from the electromagnetic and hadronic components of the calorimetric detector system. Lateral division results in cells which are pre-summed to so-called Trigger Towers of size 0.1 × 0.1 along azimuth (ϕ) and pseudorapidity (η). The received calorimeter signals represent deposits of transverse energy."
"H. Herde","","Professor","Computer Science","https://openalex.org/A5091583777","Abstract At the end of Run 3 of the Large Hadron Collider (LHC), the accelerator complex will be upgraded to the High-Luminosity LHC (HL-LHC) in order to increase the total amount of data provided to its experiments. To cope with the increased rates of data, radiation, and pileup, the ATLAS detector will undergo a substantial upgrade, including a replacement of the Inner Detector with a future Inner Tracker, called the ITk. The ITk will be composed of pixel and strip sub-detectors, where the strips portion will be composed of 17,888 silicon strip detector modules. During the HL-LHC running period, the ITk will be cooled and warmed a number of times from about -35°C to room temperature as part of the operational cycle, including warm-ups during yearly shutdowns. To ensure ITk Strips modules are functional after these expected temperature changes, and to ensure modules are mechanically robust, each module must undergo ten thermal cycles and pass a set of electrical and mechanical criteri...

At the end of Run 3 of the Large Hadron Collider (LHC), the accelerator complex will be upgraded to the High-Luminosity LHC (HL-LHC) in order to increase the total amount of data provided to its experiments. To cope with the increased rates of data, radiation, and pileup, the ATLAS detector will undergo a substantial upgrade, including a replacement of the Inner Detector with a future Inner Tracker, called the ITk. The ITk will be composed of pixel and strip sub-detectors, where the strips portion will be composed of 17,888 silicon strip detector modules. During the HL-LHC running period, the ITk will be cooled and warmed a number of times from about ${-35}^\circ$C to room temperature as part of the operational cycle, including warm-ups during yearly shutdowns. To ensure ITk Strips modules are functional after these expected temperature changes, and to ensure modules are mechanically robust, each module must undergo ten thermal cycles and pass a set of electrical and mechanical criteri...

Abstract The inner detector of the present ATLAS experiment has been designed and developed to function in the environment of the present Large Hadron Collider (LHC). At the ATLAS Phase-II Upgrade, the particle densities and radiation levels will exceed current levels by a factor of ten. The instantaneous luminosity is expected to reach unprecedented values, resulting in up to 200 proton-proton interactions in a typical bunch crossing. The new detectors must be faster and they need to be more highly segmented. The sensors used also need to be far more resistant to radiation, and they require much greater power delivery to the front-end systems. At the same time, they cannot introduce excess material which could undermine tracking performance. For those reasons, the inner tracker of the ATLAS detector was redesigned and will be rebuilt completely. The ATLAS Upgrade Inner Tracker (ITk) consists of several layers of silicon particle detectors. The innermost layers will be composed of sili...

A bstract The Light Dark Matter eXperiment (LDMX) is an electron-beam fixed-target experiment designed to achieve comprehensive model independent sensitivity to dark matter particles in the sub-GeV mass region. An upgrade to the LCLS-II accelerator will increase the beam energy available to LDMX from 4 to 8 GeV. Using detailed GEANT4-based simulations, we investigate the effect of the increased beam energy on the capabilities to separate signal and background, and demonstrate that the veto methodology developed for 4 GeV successfully rejects photon-induced backgrounds for at least 2 × 10 14 electrons on target at 8 GeV."
"Tiny Jaarsma","","Professor","Computer Science","https://openalex.org/A5046767891","Aims Discussions about severe illness and the coming death do not often take place with patients with heart failure and their family. We therefore aimed to investigate how patients with end-stage heart failure and their family who discussed terminal illness and the imminence of death with a physician, experienced such communication, how they handled life emotionally and practically after said discussions, and if/how this changed over time. Methods A longitudinal interview study. Ten patients with end-stage heart failure and their closest kin were visited by a physician at home and discussed terminal illness during one visit and the imminence of death during another visit. They were interviewed three times about how they experienced the communication and how they handled life in this situation and in relation to the discussions. The interviews were analysed using qualitative thematic analysis by Braun and Clarke. Findings Two main themes and five subthemes were found. The first theme wa...

Abstract Background Exergaming is a promising intervention to decrease sedentary time in people with a chronic condition, such heart failure (HF). For implementing exergaming in healthcare, it is important to assess the strengths, weaknesses, opportunities and threats (SWOT). Purpose To describe the SWOT of implementing exergaming in healthcare experienced by patients’ representatives, healthcare professionals, game developers, people involved in healthcare regulations, and researchers. Methods The design was a qualitative interview study using deductive content analysis based on the SWOT framework. Purposeful and snowball sampling were used interchangeably to recruit study participants (patients’ representatives, healthcare professionals, game developers, people involved in healthcare regulations, and researchers). The interviews were conducted via the Zoom platform and their duration varied between 20 minutes and one hour. Results In total 24 people were interviewed (5 patients-repre...

Abstract Background Digital health interventions, including mobile applications, offer new opportunities for promoting physical activity among individuals with chronic conditions. However, adherence to such applications varies widely. Previous findings from a randomized controlled trial (RCT) evaluating a tele-yoga intervention indicated high participation in live-streamed group yoga sessions but low engagement with the accompanying app. The aim was to explore user experiences with a yoga app and to determine associations between adherence levels, demographic factors, and patient-reported outcomes. Methods This mixed-methods study included participants from a RCT evaluating tele-yoga (clinicaltrials.gov: NCT03703609). The intervention included live-streamed 60-minute group sessions twice weekly and individual 10-minute sessions five days per week via an app. Quantitative data on app usage during the 3-month intervention were logged, and demographic and baseline patient-reported outcome...

Abstract Background People with heart failure (HF) may experience symptoms such as shortness of breath or fatigue leading to avoiding physically demanding activities and becoming sedentary. Engaging in everyday life activities significantly enhances the quality of life and promote physical and emotional well-being. Yet little is known about the performance of everyday life activities in patients with HF. Purpose The aim is to identify important everyday life activities for patients with HF and assess ability to perform the activity and satisfaction with the performance Method This is a cross-sectional study on the baseline data in the ""Heart eXg-study,"" an ongoing randomized controlled trial. Performance of everyday life activities was assessed with the Canadian Occupational Performance Measure (COPM). In COPM activities perceived important to perform are identified and the ability to perform and satisfaction with performance are self-assessed. The ratings are made on a ten-point scale..."
"P. Adlarson","","Professor","Computer Science","https://openalex.org/A5077139969","A bstract A study on the Bose-Einstein correlations for triplets of same-sign pions is presented. The analysis is performed using proton-proton collisions at a centre-of-mass energy of $$ \sqrt{s} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> </mml:math> = 7 TeV, recorded by the LHCb experiment, corresponding to an integrated luminosity of 1.0 fb − 1 . For the first time, the results are interpreted in the core-halo model. The parameters of the model are determined in regions of charged-particle multiplicity. This measurement provides insight into the nature of hadronisation in terms of coherence, being consistent with the presence of coherent emission of pions.

Abstract The first observation of the $${{{{\varXi } ^0_{b}} \!\rightarrow {{J \hspace{-1.66656pt}/\hspace{-1.111pt}\psi }} {{\varXi } ^-} {{\pi } ^+} }}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> <mml:mn>0</mml:mn> </mml:msubsup> <mml:mspace/> <mml:mo>→</mml:mo> <mml:mrow> <mml:mi>J</mml:mi> <mml:mspace/> <mml:mo>/</mml:mo> <mml:mspace/> <mml:mi>ψ</mml:mi> </mml:mrow> <mml:msup> <mml:mrow> <mml:mi>Ξ</mml:mi> </mml:mrow> <mml:mo>-</mml:mo> </mml:msup> <mml:msup> <mml:mrow> <mml:mi>π</mml:mi> </mml:mrow> <mml:mo>+</mml:mo> </mml:msup> </mml:mrow> </mml:math> decay and the most precise measurement of the branching fraction of the $${{{\varLambda } ^0_{b}} \!\rightarrow {{J \hspace{-1.66656pt}/\hspace{-1.111pt}\psi }} {{\varXi } ^-} {{K} ^+} }$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:msubsup> <mml:mrow> <mml:mi>Λ</mml:mi> </mml:mrow> <mml:mi>b</mml:mi> ...

A bstract The ratio of prompt production cross-sections of ψ (2 S ) and J/ψ mesons in their dimuon final state is measured as a function of centrality, using data collected by the LHCb detector in PbPb collisions at $$ \sqrt{s_{\textrm{NN}}} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msqrt> <mml:msub> <mml:mi>s</mml:mi> <mml:mi>NN</mml:mi> </mml:msub> </mml:msqrt> </mml:math> = 5 . 02 TeV, for the first time in the forward rapidity region. The measured ratio shows no dependence on the collision centrality, and is compared to the latest theory predictions and to the recent measurements in literature.

A bstract A test of lepton universality between muons and electrons is performed using B + → K + ℓ + ℓ − decays (where ℓ = e , μ ), in the dilepton invariant-mass-squared region above 14.3 GeV 2 /c 4 . The data used for the measurement consists of beauty meson decays produced in proton-proton collisions, corresponding to an integrated luminosity of 9 fb − 1 , collected by the LHCb experiment between 2011 and 2018. The ratio of branching fractions for B + → K + μ + μ − and B + → K + e + e − decays is measured to be $$ {R}_K=1.0{8}_{-0.09}^{+0.11}{\left(\textrm{stat}\right)}_{-0.04}^{+0.04}\left(\textrm{syst}\right) $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msub> <mml:mi>R</mml:mi> <mml:mi>K</mml:mi> </mml:msub> <mml:mo>=</mml:mo> <mml:mn>1.0</mml:mn> <mml:msubsup> <mml:mn>8</mml:mn> <mml:mrow> <mml:mo>−</mml:mo> <mml:mn>0.09</mml:mn> </mml:mrow> <mml:mrow> <mml:mo>+</mml:mo> <mml:mn>0.11</mml:mn> </mml:mrow> </mml:msubsup> <mml:msubsup> <mml:mfenced> <mml:mtext>s..."
"M. Willander","","Professor","Computer Science","https://openalex.org/A5074258399","Expression of concern for ‘Electrochemical genosensor based on gold nanostars for the detection of Escherichia coli O157:H7 DNA’ by Nasrin Razmi et al. , Anal. Methods , 2022, 14 , 1562–1570, https://doi.org/10.1039/D2AY00056C.

While world energy consumption is rising every year, the development of clean and renewable energy sources becomes very important for keeping the standard of living and preserving the environment. Solar driven photoelectrochemical (PEC) water splitting to produce hydrogen and oxygen is a promising method to contribute to the energy increasing demand. The development of the photoelectrode is a key factor for improving the PEC performance. A new morphology of 3D CdS-branched ZnO nanorod array nanocomposite has successfully been synthesized via solution routes as a photoanode. The present nanocomposite provides the highest photocurrent density of 2.5 mA/cm2 at a potential 1.23 V vs. RHE, which is about 83.3 times compared to a photocurrent density of 0.03 mA/cm2 of the bare ZnO nanorod array photoelectrode. The boost of the PEC performance is improved due to the improvement of light absorption capacity, the enhanced energy band alignment (type-II heterostructure) promoting the charge tran...

Abstract Low temperature hydrothermal methods have been utilized to synthesize Hematite/Zinc oxide α ‐Fe 2 O 3 /ZnO composite nano‐heterojunction nanorods grown on FTO glass substrates while monitoring the effect of different concentrations of urea on the morphology of the composite nano‐heterojunction. X‐ray diffraction (XRD) and scanning electron microscopy (SEM) techniques were used for the structural characterization of the α ‐Fe 2 O 3 /ZnO different samples. UV‐visible spectroscopy was used for the characteristic absorbance versus wavelength of α ‐Fe 2 O 3 /ZnO composite nano‐heterojunction which shows an absorption edge from 400 to 560 nm. X‐ray photoelectron spectroscopy (XPS) technique was applied to study of chemical composition of the α ‐Fe 2 O 3 /ZnO and the obtained information demonstrated a pure phase α‐Fe 2 O 3 /ZnO has been achieved. The best efficiency among urea concentrations for the best composite nano‐heterojunction sample was achieved when using 0.2 M of urea. The..."
"Michel Foucault","","Professor","Computer Science","https://openalex.org/A5005989677","This book makes available, for the first time in English, lectures and interviews that Foucault gave in Japan in 1978, reconstructing their context, and isolating the question of their singular relevance for us today. In these forgotten lectures, in a free and often informal style, Foucault explores, together with his Japanese interlocutors, what it would mean to take up, from outside Europe, the questions he was raising at the time about Revolution and Enlightenment in the traditions of European critical thought. In a series of wide-ranging discussions, on sexuality and its history, non-Christian forms of spirituality, new forms of political movements, and the role of knowledge, power, and truth in them, Foucault examines these questions in relationship to Asia. He had hoped these questions, very much debated at the time in postwar Japan, would be the start of new forms of translation, publication, and exchange. At the heart of the lectures is thus a search for the creation of a new s..."
"J. Sollerman","","Professor","Computer Science","https://openalex.org/A5028904496","Abstract We present observations of SN 2023xgo, a transitional Type Ibn/Icn SN, from −5.6 to 63 days relative to r-band peak. Early spectra show C iii λ5696 emission like Type Icn SNe, shifting to Type Ibn features. The He i velocities (1800-10000 km s−1) and pseudo-equivalent widths are among the highest in the Ibn/Icn class. The light curve declines at 0.14mag d−1 until 30 days, matching SNe Ibn/Icn but slower than fast transients. SN 2023xgo is the faintest in our SN Ibn sample (Mr = −17.65 ± 0.04) but shows typical colour and host properties. Semi-analytical modelling of the light curve suggests a compact CSM shell (∼1012 − 1013 cm), mass-loss rate between 10−4 − 10−3 M⊙ yr−1 with CSM and ejecta masses of ∼0.22 and 0.12 M⊙, respectively. Post-maximum light-curve, spectral modelling favours a ∼3 M⊙ helium star progenitor with extended (∼1015 cm), stratified CSM (density exponent of 2.9) and mass-loss rate of 0.1 − 2.7 M⊙ yr−1. These two mass-loss regimes imply a radially varying CSM...

Abstract AT2022rze is a luminous, ambiguous transient located South-East of the geometric center of its host galaxy at redshift z = 0.08. The host appears to be formed by a merging galaxy system. The observed characteristics of AT2022rze are reminiscent of active galactic nuclei (AGN), tidal disruption events (TDEs), and superluminous supernovae (SLSNe). The transient reached a peak absolute magnitude of −20.2 ± 0.2 mag, showing a sharp rise (trise, 1/e = 27.5 ± 0.6 days) followed by a slow decline (tdec, 1/e = 382.9 ± 0.6). Its bumpy light curve and narrow Balmer lines indicate the presence of gas (and dust). Its light curve shows rather red colors, indicating that the transient could be affected by significant host extinction. The spectra reveal coronal lines, indicative of high-energy (X-ray/UV) emission. Archival data reveal no prior activity at this location, disfavoring a steady-state AGN, although an optical spectrum obtained prior to the transient is consistent with an AGN clas...

Abstract We investigate the optical shock emission from the Large Magellanic Cloud supernova remnant 0540–69.3 (SNR 0540) using MUSE integral-field-unit data from the VLT. The observations cover the spectral range 4650–9300 Å and provide a 1 × 1 arcmin2 field of view, encompassing nearly the entire remnant. We analyse the spatial and spectral properties of shock-related emission lines, and identify clumpy optical shock emission e.g. from [S ii] λλ6716,6731 doublet and the coronal [Fe xiv] λ5303 line (typically at radial velocities ≲ |100| km s−1 and ≲ |170| km s−1, respectively). These features trace the blast-wave shell seen in previous X-ray studies. Post-shock electron density estimates, based on the [S ii]-line ratio, reveal spatial variation, with the highest densities (∼104 cm−3) in the bright knots in the west, and lower densities (∼3 × 103 cm−3) in the east. The density in the north (southwest) appears significantly lower (higher) but remains unconstrained due to limited signal...

Abstract We present the optical discovery and multiwavelength follow-up observations of AT 2024kmq, a likely tidal disruption event (TDE) associated with a supermassive ( M BH ∼ 10 8 M ⊙ ) black hole in a massive galaxy at z = 0.192. The optical light curve of AT 2024kmq exhibits two distinct peaks: an early fast (timescale 1 day) and luminous ( M ≈ −20 mag) red peak, then a slower (timescale 1 month) blue peak with a higher optical luminosity ( M ≈ −22 mag) and featureless optical spectra. The second component is similar to the spectroscopic class of “featureless TDEs” in the literature, and during this second component we detect highly variable, luminous ( L X ≈ 10 44 erg s −1 ), and hard ( f ν ∝ ν −1.5 ) X-ray emission. Luminous (10 29 erg s −1 Hz −1 at 10 GHz) but unchanging radio emission likely arises from an underlying active galactic nucleus. The luminosity, timescale, and color of the early red optical peak can be explained by synchrotron emission, or alternatively by thermal ..."
"G. Ripellino","","Professor","Computer Science","https://openalex.org/A5055373831","A bstract This paper investigates the search for long-lived dark scalars from exotic Higgs boson decays at the Future Circular Collider in its e + e − stage, FCC-ee, considering an integrated luminosity of 10 . 8 ab −1 collected during the ZH run at a center-of-mass energy $$ \sqrt{s} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msqrt> <mml:mi>s</mml:mi> </mml:msqrt> </mml:math> = 240 GeV. The work considers Zh events where the Z boson decays leptonically and the Higgs boson h decays into two long-lived dark scalars s which further decay into bottom anti-bottom quark pairs. The analysis is performed using a parametrized simulation of the IDEA detector concept and targets dark scalar decays in the tracking volume, resulting in multiple displaced vertices in the final state. The sensitivity towards long-lived dark scalars at FCC-ee is estimated using an event selection requiring two opposite-charge, same-flavor leptons compatible with the Z boson, and at least two di...

This study presents a novel method for the definition of signal regions in searches for new physics at collider experiments. By leveraging multi-dimensional histograms with precise arithmetic and utilizing the SparkDensityTree library, it is possible to identify high-density regions within the available phase space, potentially improving sensitivity to very small signals. Inspired by a search for dark mesons at the ATLAS experiment, CMS open data is used for this proof-of-concept intentionally targeting an already excluded signal. Signal regions are defined based on density estimates of signal and background. These preliminary regions align well with the physical properties of the signal while effectively rejecting background events.

This paper investigates the search for long-lived dark scalars from exotic Higgs boson decays at the Future Circular Collider in its $e^+e^-$ stage, FCC-ee, considering an integrated luminosity of 10.8 $\text{ab}^{-1}$ collected during the ZH run at a center-of-mass energy $\sqrt{s}=240$ GeV. The work considers $Zh$ events where the $Z$ boson decays leptonically and the Higgs boson $h$ decays into two long-lived dark scalars $s$ which further decay into bottom anti-bottom quark pairs. The analysis is performed using a parametrized simulation of the IDEA detector concept and targets dark scalar decays in the tracking volume, resulting in multiple displaced vertices in the final state. The sensitivity towards long-lived dark scalars at FCC-ee is estimated using an event selection requiring two opposite-charge, same-flavor leptons compatible with the $Z$ boson, and at least two displaced vertices in the final state. The selection is seen to efficiently remove the Standard Model background...

This study presents a novel method for the definition of signal regions in searches for new physics at collider experiments, specifically those conducted at CERN's Large Hadron Collider. By leveraging multi-dimensional histograms with precise arithmetic and utilizing the SparkDensityTree library, it is possible to identify high-density regions within the available phase space, potentially improving sensitivity to very small signals. Inspired by an ongoing search for dark mesons at the ATLAS experiment, CMS open data is used for this proof-of-concept intentionally targeting an already excluded signal. Several signal regions are defined based on density estimates of signal and background. These preliminary regions align well with the physical properties of the signal while effectively rejecting background events. While not explored in this work, this method is also scalable, which makes it ideal for large datasets such as those expected at the high-luminosity upgrade of the LHC. Finally,..."
"Gerd Meyer","","Professor","Computer Science","https://openalex.org/A5068165268","The understanding of structure and bonding in intermetallic phases still lags behind that of molecular compounds. For that reason, exploring intermetallic phases and identifying structural patterns and relationships are particularly important for closing this knowledge gap. In particular, here we report on the addition of increasing amounts of platinum to ∼2:1 mixtures of tin and neodymium, which yields eight ternary Pt/Sn/Nd compounds, four of which have not been reported before. Interestingly, except for PtSnNd (1), all observed ternary phases of the system can be derived from the binary compounds Sn2Nd and Sn5Nd2 by adding Pt to the composition(s), as they lie on or close to two lines: Sn2Nd-Pt (Pt0.21(1)Sn2Nd (2), PtSn2Nd (3), Pt1.33Sn2Nd (4), Pt2-xSn2+xNd (x = 0.27(3), 5), and Pt3Sn2Nd (6)) or Sn5Nd2-Pt (Pt1.5Sn5-xNd2 (x = 0.16(2), 7) and Pt3Sn5Nd2-x (x = 0.161(8), 8)). While the introduction of increasing amounts of Pt to the binaries Sn2Nd and Sn5Nd2 leads to stepwise changes in...

An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures.

The rare earth (oxides), R2O3, may be converted into trihalides, RX3, by a number of different synthetic routes of which the ammonium halide route is inexpensive and easy to perform. It runs through ternary ammonium rare-earth halides, e.g. (NH4)3YCl6, and, with sufficient care, avoids the formation of oxide-halides, e.g. YOCl. Nevertheless the formation usually thought as a pitfall, can be a blessing, as the first synthesis of {OYb4}Cl6 attests. Again, there is a number of methods to reduce trihalides to lower oxidation states, most prominently to the divalent state. Binary dihalides, RX2, as well as ternaries such as ARX3, are either prepared by comproportionation or metallothermic reduction reactions, to name the two most prolific routes. Further reduction results in metal-rich halides, of which the most abundant are (complex) octahedral cluster halides, in most cases sequestering a main-group (E) or transition metal (T) atom to overcome the electron paucity of group 3 rare-earth me...

An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures.

An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures."
"Lars Hultman","","Professor","Computer Science","https://openalex.org/A5086073947","Achieving large two-dimensional (2D) sheets of any metal is challenging due to their tendency to coalescence or cluster into 3D shapes. Recently, single-atom-thick gold sheets, termed goldene, was reported. Here, we ask if goldene can be extended to include multiple layers. The answer is yes, and trilayer goldene is the magic number, for reasons of electronegativity. Experiments are made to synthesize the atomically laminated phase Ti 4 Au 3 C 3 through substitutional intercalation of Si layers in Ti 4 SiC 3 for Au. Density functional theory calculations suggest that it is energetically favorable to insert three layers of Au into Ti 4 SiC 3 , compared to inserting a monolayer, a bilayer, or more than three layers. Isolated trilayer goldene sheets, ~100 nanometers wide and 6.7 angstroms thick, were obtained by chemically etching the Ti 4 C 3 layers from Ti 4 Au 3 C 3 templates. Furthermore, trilayer goldene is found in both hcp and fcc forms, where the hcp is ~50 milli–electron volts pe...

Abstract Metallenes are presented for a new class of single-atom-thick two-dimensional (2D) metal sheets. It is motivated by a recent (2024) discovery of 2D gold, dubbed goldene, by selectively etching off Ti3C2 slabs from a Ti3AuC2 nanolaminate. This synthesis-derivative method bypasses the natural tendency for metals to form three-dimensional forms. Thus, 2D-materials’ research goes beyond ceramics with graphene as the most-known example. A range of noble and non-noble metals are now proposed for metallene preparation. Their exploration is motivated by the unique properties offered by 2D and nanostructured materials. Metallene’s ultimate high surface-to-volume ratio with abundant uncoordinated metal atoms makes them attractive for high-end applications, like in catalysis, sensing, electronics, and biomedicine. Challenges for scientific research and practical use, however, lie in scalable synthetic processes, sheet integrity, and transfer methods. Here, we review state-of-the-art for ..."
"Marie-Louise G. Wadenberg","","Professor","Computer Science","https://openalex.org/A5112766495","The acetylcholine esterase inhibitor/cholinergic nicotinic receptor (nAChR) allosteric modulator galantamine (Gal) is used against cognitive impairment in Alzheimer's disease. Negative/cognitive and psychotic symptom improvement in schizophrenia by adjunct Gal to antipsychotic drugs (APDs) has been reported. Cognitive symptoms in schizophrenia may involve brain prefrontal hypo-dopaminergia. Experimental data by others indicate nAChR involvement in animal pro-cognitive effects of Gal. The role of nAChRs in antipsychotic effects by Gal has, however, not been elucidated. Using the conditioned avoidance response (CAR) and the catalepsy tests for antipsychotic activity and extrapyramidal side-effect (EPS) liability, respectively, we here investigated the effects of adjunct Gal (1.25 mg/kg) to the typical APD haloperidol (Hal) (0.05 mg/kg), or the atypical APD risperidone (Ris) (0.2 mg/kg), in rats. Adjunct Gal significantly enhanced APD-like effects by low doses of Hal or Ris, but showed a ..."
"Andrei Khrennikov","","Professor","Computer Science","https://openalex.org/A5082978886","A physical model for phenomenological proto-consciousness as an intrinsic, primary property of matter is introduced. A fundamental particle is characterized as a classic-like system in physical space supplemented with an information space that endows the particle with the capacity of storing and processing incoming information which has causal power on the behaviour of the particle. These features transform the postulated initial random particles into information-theoretic Darwinian physical systems controlled by algorithms susceptible of evolution under natural selection. Consciousness is then defined as the elementary microscopic sentient process of subjectively experiencing a representation of the locations of the surrounding systems that is induced on matter by the irreversible erasure of dynamically superfluous stored information when the particle is measured. A physical description of the possible emergence of both quantum behaviour of matter in accordance with the conventional q...

In this work, we conduct a sentiment analysis of English-language reviews using a quantum-like (wave-based) model of text representation. This model is explored as an alternative to machine learning (ML) techniques for text classification and analysis tasks. Special attention is given to the problem of segmenting text into semantic units, and we illustrate how the choice of segmentation algorithm is influenced by the structure of the language. We investigate the impact of quantum-like semantic interference on classification accuracy and compare the results with those obtained using classical probabilistic methods. Our findings show that accounting for interference effects improves accuracy by approximately 15%. We also explore methods for reducing the computational cost of algorithms based on the wave model of text representation. The results demonstrate that the quantum-like model can serve as a viable alternative or complement to traditional ML approaches. The model achieves classifi...

This paper investigates the properties of the Gorini–Kossakowski–Sudarshan–Lindblad (GKSL) equation within the camel-like framework, with a focus on quantum correlations in the context of open quantum systems. Here, we compute quantum correlations such as quantum discord and quantum steering, analyzing their behavior under decoherence and environmental interaction for three sets of quantum states. Our results indicate that the sign of the entanglement entropy's derivative serves as an indicator of the system's drift toward classical or quantum information exchange—an insight with important implications for quantum error correction and dissipation processes in quantum thermal machines. \\ Moreover, we parametrize quantum states using both single-parameter and Bloch-sphere representations. The Bloch-sphere analysis is particularly developed by examining the topology of the quantum states as elements on the \(\mathbb{S}^2\) sphere, yielding a gradient map and a topological basin map which...

We proposed a unified principle of the universe and existence based on adaptive dynamics (Ando et al., AJIS 14, No.3, 2025). This principle posits the fundamental interconnection of all entities and events, akin to the concept of causation in Buddhism. Through scientific advancements, humanity has reached a stage where enlightenment—akin to Buddhahood—is potentially accessible to all. Thus, we advocate for a broad understanding of these principles and encourage efforts to navigate the challenges of our increasingly restricted and overpopulated world. Received: 27 April 2025 / Accepted: 26 June 2025 / Published: 08 July 2025"
"Erik G. Larsson","","Professor","Computer Science","https://openalex.org/A5043552696","Machine learning methods have been shown to be effective for weather forecasting, based on the speed and accuracy compared to traditional numerical models. While early efforts primarily concentrated on deterministic predictions, the field has increasingly shifted toward probabilistic forecasting to better capture the forecast uncertainty. Most machine learning-based models have been designed for global-scale predictions, with only limited work targeting regional or limited area forecasting, which allows more specialized and flexible modeling for specific locations. This work introduces Diffusion-LAM, a probabilistic limited area weather model leveraging conditional diffusion. By conditioning on boundary data from surrounding regions, our approach generates forecasts within a defined area. Experimental results on the MEPS limited area dataset demonstrate the potential of Diffusion-LAM to deliver accurate probabilistic forecasts, highlighting its promise for limited-area weather predicti...

Decentralized learning enables distributed agents to train a shared machine learning model through local computation and peer-to-peer communication. Although each agent retains its dataset locally, the communication of local models can still expose private information to adversaries. To mitigate these threats, local differential privacy (LDP) injects independent noise per agent, but it suffers a larger utility gap than central differential privacy (CDP). We introduce Whisper D-SGD, a novel covariance-based approach that generates correlated privacy noise across agents, unifying several state-of-the-art methods as special cases. By leveraging network topology and mixing weights, Whisper D-SGD optimizes the noise covariance to achieve network-wide noise cancellation. Experimental results show that Whisper D-SGD cancels more noise than existing pairwise-correlation schemes, substantially narrowing the CDP-LDP gap and improving model performance under the same privacy guarantees."
"Mauro Conti","","Professor","Computer Science","https://openalex.org/A5063847107","Convolutional neural networks (CNNs) are vulnerable to adversarial attacks in computer vision tasks. Current adversarial detections are ineffective against white-box attacks and inefficient when deep CNNs generate high-dimensional hidden features. This study proposes MeetSafe, an effective and scalable adversarial example (AE) detection against white-box attacks. MeetSafe identifies AEs using critical hidden features rather than the entire feature space. We observe a non-uniform distribution of Z-scores between clean samples and adversarial examples (AEs) among hidden features and propose two utility functions to select those most relevant to AEs. We process critical hidden features using feature engineering methods: local outlier factor (LOF), feature squeezing, and whitening, which estimate feature density relative to its k-neighbors, reduce redundancy, and normalize features. To deal with the curse of dimensionality and smooth statistical fluctuations in high-dimensional features, w..."
"Richard Bellman","","Professor","Computer Science","https://openalex.org/A5076973014","The Bellman"
"Weimin Chen","","Professor","Computer Science","https://openalex.org/A5020453892","Abstract The effects of Sb incorporation on the molecular beam epitaxial growth of GaInNAs triple quantum well (QW) core–multishell nanowires on Si(111) substrates were investigated. Sb was not directly incorporated into the QWs but was predominantly localized at the interface between the QWs and the adjacent GaAs(Sb) barrier, indicating significant Sb segregation with the concentration reaching approximately 1%, which was independently confirmed by optical measurements. Room-temperature photoluminescence measurements reveal a significant red shift of the QW emission peak from 1100 nm in Sb-free nanowires to 1250 nm in the Sb-containing structures. These findings suggest that Sb acts as an efficient surfactant.

Abstract Spin polarized excitons induced by spin injection from magnetic ion to a single quantum dot, has been considered as a basic unit of quantum information transfer between spin and photon for spin-photonic applications. However, this state-of-the-art technology has only been found with limited coupling strength and weak excitonic emission. Here, we demonstrate a spin-polarized self-trapped exciton naturally formed in the zero-dimensional lattice of cesium copper iodide. Upon excitation, the conversion from Cu + ion to spin-1/2 Cu 2+ ion results in an in-situ self-trapped exciton, which facilitates a local Jahn-Teller distortion and guarantees the strong spin-exciton coupling and near-unity excitonic emission efficiency. Consequently, a giant Zeeman splitting of −53 meV and an effective excitonic g-factor of −93.5 are observed from magneto-photoluminescence. More importantly, this nano-scale coupling can also be driven by an external electric field, which generates electroluminesc...

Due to its attractive electronic properties, the GaNAs alloy is considered a promising material for optoelectronic applications in the near-infrared spectral region. Unfortunately, nitrogen incorporation is also known to lead to material degradation due to the formation of non-radiative defects and strong band tailing effects caused by alloy disorder. In this study, we show that post-growth hydrogenation of GaNAs-based nanowires (NWs) can largely suppress these unwanted effects. First, we find that this treatment results in a more homogeneous electronic structure due to the passivation of nitrogen-related band tail states, without affecting the bandgap energy of the material. Additionally, hydrogenation reduces the density of quantum emitters that are spontaneously formed in dilute nitride NWs upon N incorporation. This leads to spectrally isolated emission lines from these emitters, which is important for creating high-purity single-photon sources. Finally, the treatment improves the ..."
"P. Christiansen","","Professor","Computer Science","https://openalex.org/A5021081970","The Large Hadron Collider (LHC) at CERN became operational in 2009 and has since then produced a plethora of physics results from proton–proton ( pp ) collisions. This short review covers results that relate to soft quantum chromodynamics (QCD) with a focus on nondiffractive physics at midrapidity. Most of the presented results are based on transverse momentum spectra and related derived observables, including multiplicity, the average transverse momentum, and various particle ratios. Additionally, the phenomenon of the observed ridge and its potential connection to the formation of a quark–gluon plasma in pp collisions are discussed. The goals of the review are to introduce the topics and provide references for scientists joining the LHC program and to highlight what we consider to be the most interesting results and open questions, to inspire novel measurements.

This corrects the article DOI: 10.1103/PhysRevLett.116.122301.

The PHENIX experiment measured the centrality dependence of two-pion Bose-Einstein correlation functions in √𝑠𝑁⁢𝑁=200GeV Au+Au collisions at the Relativistic Heavy Ion Collider at Brookhaven National Laboratory. The data are well represented by Lévy-stable source distributions. The extracted source parameters are the correlation-strength parameter 𝜆, the Lévy index of stability 𝛼, and the Lévy-scale parameter 𝑅 as a function of transverse mass 𝑚𝑇 and centrality. The 𝜆⁡(𝑚𝑇) parameter is constant at larger values of 𝑚𝑇, but decreases as 𝑚𝑇 decreases. The Lévy-scale parameter 𝑅⁡(𝑚𝑇) decreases with 𝑚𝑇 and exhibits proportionality to the length scale of the nuclear overlap region. The Lévy exponent 𝛼⁡(𝑚𝑇) is independent of 𝑚𝑇 within uncertainties in each investigated centrality bin, but shows a clear centrality dependence. At all centralities, the Lévy exponent 𝛼 is significantly different from that of Gaussian (𝛼=2) or Cauchy (𝛼=1) source distributions. Comparisons to the predictions of Mo...

A bstract This study provides an analysis of atmospheric neutrino oscillations at the ESSnuSB far detector facility. The prospects of the two cylindrical Water Cherenkov detectors with a total fiducial mass of 540 kt are investigated over 10 years of data taking in the standard three-flavor oscillation scenario. We present the confidence intervals for the determination of mass ordering, θ 23 octant as well as for the precisions on sin 2 θ 23 and $$ \left|\Delta {m}_{31}^2\right| $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mfenced> <mml:mrow> <mml:mi>Δ</mml:mi> <mml:msubsup> <mml:mi>m</mml:mi> <mml:mn>31</mml:mn> <mml:mn>2</mml:mn> </mml:msubsup> </mml:mrow> </mml:mfenced> </mml:math> . It is shown that mass ordering can be resolved by 3 σ CL (5 σ CL) after 4 years (10 years) regardless of the true neutrino mass ordering. Correspondingly, the wrong θ 23 octant could be excluded by 3 σ CL after 4 years (8 years) in the case where the true neutrino mass ordering is n...

In collisions between heavy nuclei, such as those at the Large Hadron Collider (LHC) at CERN, hydrodynamic models have successfully related measured azimuthal momentum anisotropies to the transverse shape of the collision region. For an elliptically shaped interaction area, the hydrodynamic pressure gradient is greater along the minor axis, resulting in increased particle momentum in that direction - a phenomenon known as positive elliptic flow. In this paper, we demonstrate that in smaller systems, such as proton-proton and peripheral ion-ion collisions, microscopic models for final state interactions, can produce anisotropies where the elliptic flow is negative - that is, the momentum is largest along the major axis, contrary to hydrodynamic predictions. We present results from two distinct microscopic models: one based on repulsion between string-like fields and another based on effective kinetic theory. Negative elliptic flow is a solid prediction of the string interaction model wh..."
"Stefano Moretti","","Professor","Computer Science","https://openalex.org/A5034676393","We study resonant production of pairs of Standard Model (SM-)like Higgs bosons, in the presence of new neutral Higgs states together with new colored scalars (stops or sbottoms) in loops within the Next-to-Minimal Supersymmetric Standard Model. This is used as a test case to prove that the Large Hadron Collider has sensitivity to a variety of effects stemming from interferences between resonant (heavy) Higgs diagrams and/or among these and nonresonant topologies involving loops of both tops and stops. These effects can alter significantly the naive description of individual <a:math xmlns:a=""http://www.w3.org/1998/Math/MathML"" display=""inline""><a:mrow><a:mi>s</a:mi></a:mrow></a:math>-channel Breit-Wigner resonances, leading to distortions of the latter, which, on the one hand, may mask their presence but, on the other hand, could enable one to extract features of the underlying new physics scenario. This last aspect is made possible through a decomposition of the <c:math xmlns:c=""http:/...

Expectations for an imminent new outburst of the recurrent symbiotic nova T CrB are mounting, initiated by the discovery in 2015 of a new enhanced mass-transfer phase (SAP), which is reminiscent of the one preceding the last recorded outburst in 1946. We aim to derive a robust estimate of the most important parameters describing the physical nature of T CrB, trace the accretion history onto its white dwarf, and account for the unexpected delay in the occurrence of the new outburst. In particular, the SAP prior to 1946 was brighter and followed by a nova eruption within six months from its conclusion. This time the 2015-2023 SAP has been fainter and although two years have passed since the end of this phase, no new eruption has taken place. Between 2005-2025, the period covering the SAP and the preceding quiescence, we collected a massive amount of photometric and spectroscopic observations at optical wavelengths. We analyzed these data together with the abundant ultraviolet (UV) observ...

This work investigates the discovery potential for singly produced vectorlike quarks (VLQs) <a:math xmlns:a=""http://www.w3.org/1998/Math/MathML"" display=""inline""><a:mi>T</a:mi></a:math> (<c:math xmlns:c=""http://www.w3.org/1998/Math/MathML"" display=""inline""><c:mi>Q</c:mi><c:mo>=</c:mo><c:mo>+</c:mo><c:mn>2</c:mn><c:mo>/</c:mo><c:mn>3</c:mn><c:mi>e</c:mi></c:math>) and <e:math xmlns:e=""http://www.w3.org/1998/Math/MathML"" display=""inline""><e:mi>Y</e:mi></e:math> (<g:math xmlns:g=""http://www.w3.org/1998/Math/MathML"" display=""inline""><g:mi>Q</g:mi><g:mo>=</g:mo><g:mo>−</g:mo><g:mn>4</g:mn><g:mo>/</g:mo><g:mn>3</g:mn><g:mi>e</g:mi></g:math>) decaying to <i:math xmlns:i=""http://www.w3.org/1998/Math/MathML"" display=""inline""><i:mi>W</i:mi><i:mi>b</i:mi></i:math> at future <k:math xmlns:k=""http://www.w3.org/1998/Math/MathML"" display=""inline""><k:mi>μ</k:mi><k:mi>p</k:mi></k:math> colliders with <m:math xmlns:m=""http://www.w3.org/1998/Math/MathML"" display=""inline""><m:msqrt><m:mi>s</m:mi></m:msqrt>...

Abstract We propose a method for probing CP-violation in the heavy (pseudo)scalar sector of an extended Higgs model, in which we make simultaneous use of the HVV ( $$V=W^\pm , Z$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mi>V</mml:mi> <mml:mo>=</mml:mo> <mml:msup> <mml:mi>W</mml:mi> <mml:mo>±</mml:mo> </mml:msup> <mml:mo>,</mml:mo> <mml:mi>Z</mml:mi> </mml:mrow> </mml:math> ) and $$Ht\bar{t}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mi>H</mml:mi> <mml:mi>t</mml:mi> <mml:mover> <mml:mrow> <mml:mi>t</mml:mi> </mml:mrow> <mml:mrow> <mml:mo>¯</mml:mo> </mml:mrow> </mml:mover> </mml:mrow> </mml:math> interactions of a heavy Higgs state H . The CP-even component of H can be probed through the tree-level HVV interaction, while the CP-odd component of H can be probed if the final $$t\bar{t}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mrow> <mml:mi>t</mml:mi> <mml:mover> <mml:mrow> <mml:mi>t</mml:mi> </mml:m...

A bstract Aiming to uncover the CP properties of spin-0 particle Dark Matter (DM), we explore a two-component DM scenario within the framework of 3-Higgs Doublet Models (3HDMs), a well-motivated set-up previously studied due to the complementarity of its collider and astrophysical probes. We devise benchmark points in which the two components of DM have same CP in one case and opposite CP in another. We then show several cross section distributions of observables at collider experiments where the two cases are clearly distinguishable."
"Mikael Skoglund","","Professor","Computer Science","https://openalex.org/A5041348422",""
"Per Hall","","Professor","Computer Science","https://openalex.org/A5053177403","Abstract Background BOADICEA is a widely used algorithm for predicting breast and ovarian cancer risks, using a combination of genetic and lifestyle, hormonal and reproductive risk factors. However, it has largely been developed using data from White/European individuals, limiting its applicability to other ethnicities. Here, we updated BOADICEA to provide ethnicity-specific risk estimates. Methods We utilised data from multiple sources to derive estimates for the distributions and effect sizes of risk factors in major UK ethnic groups (White, Black, South Asian, East Asian, and Mixed), along with ethnicity-specific population cancer incidences. We also developed a method for deriving adjusted polygenic scores for individuals of mixed genetic ancestry. Results The predicted average absolute risks were smaller in all non-White ethnic groups than in Whites, and the risk distributions were narrower. The proportion of women classified as at moderate or high risk of breast or ovarian cancer...

The role of germline genetics in adjuvant aromatase inhibitor (AI) treatment efficacy in ER-positive breast cancer is poorly understood. We employed a two-stage candidate gene approach to examine associations between survival endpoints and common germline variants in 753 endocrine resistance-related genes. For a discovery cohort, we screened the Breast Cancer Association Consortium database (n ≥ 90,000 cases) and retrieved 2789 AI-treated patients. Cox model-based analysis revealed 125 variants associated with overall, distant relapse-free, and relapse-free survival (p-value ≤ 1E-04). In validation analysis using five independent cohorts (n = 8857), none of the six selected candidates representing major linkage blocks at CELA2B/CASP9, NR1I2/GSK3B, LRP1B, and MIR143HG (CARMN) were validated. We discuss potential reasons for the failed validation and replication of published findings, including study/treatment heterogeneity and other limitations inherent to genomic treatment outcome stud...

Background BOADICEA is a widely used algorithm for predicting breast and ovarian cancer risks, using a combination of genetic and lifestyle/environmental risk factors. However, it has largely been developed using data from individuals of White ethnicity. Methods We utilised data from multiple sources to derive estimates for the distributions of risk factors and their effect sizes in major UK ethnic groups (White, Black, South Asian, East Asian, and Mixed). We combined these with ethnicity-specific population cancer incidences to update BOADICEA so that it provides ethnicity-specific risk estimates. We also developed and included a method for deriving adjusted polygenic scores for individuals of mixed genetic ancestry. Results The predicted average absolute risks were smaller in all non-White ethnic groups than in Whites, and the risk distributions were narrower. The proportion of women classified as at moderate or high risk of breast or ovarian cancer, according to national guidelines,..."
"Olli Kallioniemi","","Professor","Computer Science","https://openalex.org/A5050670008","As the non-coding genome remains poorly characterized in acute myeloid leukemia (AML), we aimed to identify and functionally characterize novel long non-coding RNAs (lncRNAs) relevant to AML biology and treatment. We first identified lncRNAs overexpressed in AML blasts and, among them, discovered a novel transcript, which we named myeloid and AML-associated intergenic long non-coding RNA (MALNC). MALNC is overexpressed in AML, particularly in cases with the PML-RARA fusion or IDH2R140/NPM1 co-mutations, and is associated with a distinct gene expression profile. Functional studies showed that MALNC knockout impairs AML cell proliferation and colony formation, enhances ATRA-induced differentiation, and sensitizes cells to arsenic trioxide. Transcriptomic analysis revealed that MALNC loss alters the expression of retinoic acid pathway genes, and chromatin binding studies showed that MALNC binds to genes related to the retinoic acid and Rho GTPase pathways. In conclusion, we have identifie...

Targeted monotherapies for cancer often fail due to inherent or acquired drug resistance. By aiming at multiple targets simultaneously, drug combinations can produce synergistic interactions that increase drug effectiveness and reduce resistance. Computational models based on the integration of omics data have been used to identify synergistic combinations, but predicting drug synergy remains a challenge. Here, we introduce Drug synergy Interaction Prediction (DIPx), an algorithm for personalized prediction of drug synergy based on biologically motivated tumor- and drug-specific pathway activation scores (PASs). We trained and validated DIPx in the AstraZeneca-Sanger (AZS) DREAM Challenge human cell-line dataset using two separate test sets: Test Set 1 comprised the combinations already present in the training set, while Test Set 2 contained combinations absent from the training set, thus indicating the model’s ability to handle novel combinations. The Spearman’s correlation coefficien...

Ovarian cancer (OC) is a leading cause of death of gynecological cancers in women. Poor patient response to treatment highlights the need to better understand how the tumor microenvironment affects OC progression. Growing evidence indicates the crucial role of non‐cancerous components, such as cancer‐associated fibroblasts, in establishing a complex network of cellular and molecular interactions, influencing cancer progression and response to treatment. Therefore, in this study, we sought to characterize the impact of fibroblasts on OC cell behavior and drug response. Using both direct and indirect cell co‐culture systems, we observed distinct changes in cancer cell proliferation, morphology, and secretome in the presence of fibroblasts. Furthermore, an imaging‐based high‐throughput drug screen of 528 oncology compounds revealed multiple drugs that showed altered efficacy in the co‐culture conditions, demonstrating the role of fibroblasts in driving cancer cell resistance to treatment....

Abstract Targeted monotherapies for cancer often fail due to inherent or acquired drug resistance. By aiming at multiple targets simultaneously, drug combinations can produce synergistic interactions that increase drug effectiveness and reduce resistance. Computational models based on the integration of omics data have been used to identify synergistic combinations, but predicting drug synergy remains a challenge. Here, we introduce DIPx, an algorithm for personalized prediction of drug synergy based on biologically motivated tumor- and drug-specific pathway activation scores (PASs). We trained and validated DIPx in the AstraZeneca-Sanger (AZS) DREAM Challenge dataset using two separate test sets: Test Set 1 comprised the combinations already present in the training set, while Test Set 2 contained combinations absent from the training set, thus indicating the model’s ability to handle novel combinations. The Spearman correlation coefficients between predicted and observed drug synergy ..."
"Bruno Latour","","Professor","Computer Science","https://openalex.org/A5054780327","Recorded 18 February 2013. Transcribed by Helen Bradstock.

Through our three-way collaboration we sought to understand Gaia and its political implications from the bottom-up and from within. Here we introduce that view of Gaia and how the dialogue between a philosopher (Bruno), a scientist (Tim), and a historian and philosopher of science (Séb) turned into a research programme. This sets in context a previously unpublished piece by Latour: ‘There is nothing simple in a feedback loop – or why goal function is not the problem of Gaia’."
"L. Barranco Navarro","","Professor","Computer Science","https://openalex.org/A5048186003","Among all the elementary particles that constitute the Standard Model of particle physics, the top quark can provide key information on fundamental interactions at the electroweak symmetry breaking scale and beyond. Effects of new physics can be parametrised in terms of an Effective Field Theory. This talk presents a selection of the most recent measurements provided by the ATLAS experiment from LHC accelerator in the top-quark sector that have a direct Effective Field Theory interpretation.

This thesis presents two analyses devoted to the search of new physics in single-top-quark events with data collected by the ATLAS detector at the LHC. The aim of the first analysis is to probe the Wtb vertex structure through the measurement of W-boson polarization observables using single-top-quark events produced in the t-channel. The dataset corresponds to proton-proton collision events at a center-of-mass energy of 8 TeV with a total integrated luminosity of 20.2/fb. Selected events contain one isolated lepton (electron or muon), large missing transverse momentum and exactly two jets, with one of them identified as likely to contain a b-hadron. Further selection requirements are applied to further separate t-channel single-top-quark events from background. The W boson polarization observables are extracted from asymmetries in angular distributions measured with respect to defined spin quantization axes. The asymmetry measurements are performed at parton level by correcting the obs...

This is the third out of five chapters of the final report [1] of the Workshop on Physics at HL-LHC, and perspectives on HE-LHC [2]. It is devoted to the study of the potential, in the search for Beyond the Standard Model (BSM) physics, of the High Luminosity (HL) phase of the LHC, defined as $3~\mathrm{ab}^{-1}$ of data taken at a centre-of-mass energy of $14~\mathrm{TeV}$, and of a possible future upgrade, the High Energy (HE) LHC, defined as $15~\mathrm{ab}^{-1}$ of data at a centre-of-mass energy of $27~\mathrm{TeV}$. We consider a large variety of new physics models, both in a simplified model fashion and in a more model-dependent one. A long list of contributions from the theory and experimental (ATLAS, CMS, LHCb) communities have been collected and merged together to give a complete, wide, and consistent view of future prospects for BSM physics at the considered colliders. On top of the usual standard candles, such as supersymmetric simplified models and resonances, considered f...

We study unconventional decays of the top-quark and the top-squark in the framework of SUSY models with broken R-parity. The model under study is the MSSM with an additional bilinear term that breaks R-parity. In this model the top-squark behaves similar to a third generation leptoquark. We demonstrate that existing Tevatron data on the top give rise to restrictions on the SUSY parameter space. In particular, we focus on scenarios where the tau-neutrino mass is smaller than 1 eV. We give an exclusion plot derived from the leptoquark searches at Tevatron."
"Axel Brandenburg","","Professor","Computer Science","https://openalex.org/A5023835521","Abstract The question of whether a dynamo can be triggered by gravitational collapse is of great interest, especially for the early Universe. Here, we employ supercomoving coordinates to study the magnetic field amplification from decaying turbulence during gravitational collapse. We perform 3D simulations and show that for large magnetic Reynolds numbers, there can be exponential growth of the comoving magnetic field with conformal time before the decay of turbulence impedes further amplification. The collapse dynamics only affect the nonlinear feedback from the Lorentz force, which diminishes more rapidly for shorter collapse times, allowing nearly kinematic continued growth. We confirm that helical turbulence is more efficient in driving dynamo action than nonhelical turbulence, but this difference decreases for larger collapse times. We also show that for nearly irrotational flows, dynamo amplification is still possible, but it is always associated with a growth of vorticity—even i...

Powerful lasers may be used in the future to produce magnetic fields that would allow us to study turbulent magnetohydrodynamic inverse cascade behaviour. This has so far only been seen in numerical simulations. In the laboratory, however, the produced fields may be highly anisotropic. Here, we present corresponding simulations to show that, during the turbulent decay, such a magnetic field undergoes spontaneous isotropisation. As a consequence, we find the decay dynamics to be similar to that in isotropic turbulence. We also find that an initially pointwise non-helical magnetic field is unstable and develops magnetic helicity fluctuations that can be quantified by the Hosking integral. It is a conserved quantity that characterises magnetic helicity fluctuations and governs the turbulent decay when the mean magnetic helicity vanishes. As in earlier work, the ratio of the magnetic decay time to the Alfvén time is found to be approximately $50$ in the helical and non-helical cases. At in...

Abstract Kinetic helicity is a fundamental characteristic of astrophysical turbulent flows. It is not only responsible for the generation of large-scale magnetic fields in the Sun, stars, and spiral galaxies, but it also affects turbulent diffusion, resulting in the dissipation of large-scale magnetic fields. Using the path integral approach for random helical velocity fields with a finite correlation time and large Reynolds numbers, we show that turbulent magnetic diffusion is reduced by the kinetic helicity, while the turbulent diffusivity of a passive scalar is enhanced by the helicity. The latter can explain the results of recent numerical simulations for forced helical turbulence. One of the crucial reasons for the difference between the kinetic helicity effect on magnetic and scalar fields is related to the helicity dependence of the correlation time of a turbulent velocity field.

Abstract We numerically study axion-U(1) inflation, focusing on the regime where the coupling between axions and gauge fields results in significant backreaction from the amplified gauge fields during inflation. These amplified gauge fields not only generate high-frequency gravitational waves (GWs), but also enhance spatial inhomogeneities in the axion field. GWs serve as key probe for constraining the coupling strength between the axion and gauge fields. We find that, when backreaction is important during inflation, the constraints on the coupling strength due to GW overproduction are relaxed compared to previous studies, in which backreaction matters only after inflation. Moreover, our results suggest that the probability density function (PDF) of axion fluctuations tends toward a Gaussian distribution even in cases where gauge field backreaction is important only after inflation. This aligns with previous studies where the same effect was observed for cases with strong backreaction ..."
"Y. V. Khotyaintsev","","Professor","Computer Science","https://openalex.org/A5053872042","Abstract Collisionless shocks can exhibit non‐stationary behavior even under steady upstream conditions, forming a complex transition region. Ion phase‐space holes, linked to shock self‐reformation and surface ripples, are a signature of this non‐stationarity. We statistically analyze their occurrence using 521 crossings of Earth's quasi‐perpendicular bow shock. Phase‐space holes appear in 65% of cases, though the actual rate may be higher as the holes may not be resolved during fast shock crossings. The occurrence rate peaks at 70% for shocks with Alfvén Mach numbers . These findings suggest that Earth's quasi‐perpendicular bow shock is predominantly non‐stationary.

Magnetospheric-ionospheric coupling studies often rely on multi-spacecraft conjunctions, which require accurate magnetic field mapping tools. For example, linking measurements from the magnetotail with those in the ionosphere involves determining when the orbital magnetic footpoint of THEMIS or MMS intersects with the footpoint of Swarm. The Tsyganenko models are commonly used for tracing magnetic field lines. In this study, we aim to analyze how the footpoint locations are impacted by the inputs parameters of these models, including solar wind conditions, geomagnetic activity, and the location in the magnetotail. A dataset of 2394 bursty bulk flows (BBFs) detected by MMS was mapped to Earth's ionosphere with six different Tsyganenko models. Approximately 90% of the ionospheric footpoints are concentrated within 70° +/- 5° magnetic latitude (MLAT) and +/- 3 hours of magnetic local time (MLT) around midnight, with a pronounced peak in the pre-midnight sector. The MLT position showed a d...

Abstract The evolution of the properties of short‐scale electrostatic waves across collisionless shocks remains an open question. We use a method based on the interferometry of the electric field measured aboard the magnetospheric multiscale spacecraft to analyze the evolution of the properties of electrostatic waves across four quasi‐perpendicular shocks, with and . Most of the analyzed wave bursts across all four shocks have a frequency in the plasma frame lower than the ion plasma frequency and a wavelength on the order of 20 Debye lengths . Their direction of propagation is predominantly field‐aligned upstream and downstream of the bow shock, while it is highly oblique within the shock transition region, which might indicate a shift in their generation mechanism. The similarity in wave properties between the analyzed shocks, despite their different shock parameters, indicates the fundamental nature of electrostatic waves for the dynamics of collisionless shocks.

Abstract Dipolarization events with inductive, radial electric fields are investigated with multi‐spacecraft analysis techniques. Observations by Magnetospheric Multiscale with separations around ion scales are used to study spatial and temporal variations of these events in the inner magnetosphere. force, magnetic pressure force, and tension force are compared based on the Taylor expansion method, which includes the curlometer technique. The magnetic pressure force, possibly related to energetic particles and magnetic flux transported from the magnetotail, tends to contribute to the force for the background components near the equator, while the tension force, related to Alfvén waves, contributes to the fluctuating components or outside the equator. The scale length is thousands of km for the background components, probably related to meso‐scale structures, while that length is tens to a thousand km for fluctuating components. The small‐scale fluctuations would be related to particle ..."
"Jeff Hearn","","Professor","Computer Science","https://openalex.org/A5049950427","In the world we live in today, the presence and claims of crisis abound – from climate change, financial and political crisis to depression, livelihoods and personal security crisis. There is a challenge to studying crisis due to the ways in which crisis as a notion, condition and experience refers to and operates at various societal levels. Further, different kinds of crisis can overlap and intersect with each other, and act as precursors or consequences of other crises, in what can be thought of as inter-crisis relations or chains of crises. This article makes an enquiry into how to develop more adequate analytical tools for understanding crisis as a multidimensional phenomenon. We ask how crisis can be conceptualised and what the analytical potentials of a distinct crisis perspective might be? In this article we suggest a multi- and interdisciplinary approach to bridge between traditionally separated realms. Our ambition is to present a case for the development of Interdisciplinary ...

The critique and conceptualisation of current policy and research on gender-based violence in higher education institutions (HEIs) and research performing organisations (RPOs) are matters of central importance. Building critically on recent European research and policy experience, and conceptual reflections arising from a large European multi-country research and innovation project, three key steps in critique and reconceptualising of gender-based violence in HEIs and RPOs are explicated. These are: first, clarification of differential definitions of and inclusions in gender-based violence in HEIs and RPOs; second, drawing on the recent European UniSAFE project survey and analysis of 42,000 university staff and student respondents in 46 institutions within 15 countries, differential contextualisations of prevalence and consequences, especially the need for multi-level and intersectional analysis of prevalence and consequences; and, third, engagement with ongoing theoretical and practic...

The digital age requires people of all ages to communicate and organise their lives through digital technologies. The project EQualCare (“Alone but connected? Digital (in)equalities in care work and generational relationships among older people living alone”) investigated how the growing population of older people living alone is man-aging this transition, how it shapes their (non-)digital social networks and what changes on local, regional, national and international levels need to be brought about to ensure (digital) equality. This white paper gives insight into the multi-method work that was done, summarises key findings, and provides recommendations for policy and practice. EQualCare was a cross-cultural comparison and collaboration across Finland, Ger-many, Latvia and Sweden, with Finland and Sweden as two countries advanced in the digitalisation of civic and private life and thus providing a helpful contrast to Germany and Latvia that are at different levels of digitalisation. Th..."
"Magnus Karlsson","","Professor","Computer Science","https://openalex.org/A5052801826","The study utilizes data of naturally occurring peer interactions in a Swedish preschool, to explore how dyadic affiliations (and exclusions) are created, played, out and transformed in small interacting groups of three children (aged 5) during free time play. The overall results point to the complex and shifting nature of children's (dyadic) relationships as children move in and out of dyads, which means that a child is not always excluded. It is shown how dyads can be built around relationship buildings as well as the evaluating skills of others in a specific activity, a formation that can be strengthened when reacting against a third-party seeking access. However, the results also show how dyadic affiliations can be renegotiated during a rather short period of time; third parties can benefit from “relational cracks” and tensions in the dyad as they restructure and challenge (dyadic) relationships and relative positions toward each other. Finally, the analysis reveals how part of thes...

Socialt arbete har diskuterats i Socialmedicinsk tidskrift sedan dess den först publicerades. I föreliggande text visas hur ämnet diskuterats med olika frekvens under olika perioder, och särskilt tre perioder under tiden 1924-2006 utmärker sig i detta fall. Här identifieras och karaktäriseras dessa tre perioder och exempel ges på hur det sociala arbetet diskuterats i tidskriften under dem. De historiska exempel som lyfts fram illustrerar en period där socialt arbete diskuterades framför allt i relation till vårdinsatser, en där ämnet socialt arbete etablerades och en där välfärdens sociala arbete på olika sätt kom att granskas."
"Nathalie Feiner","","Professor","Computer Science","https://openalex.org/A5004935819","<ns3:p>We present a genome assembly from an individual female <ns3:italic>Podarcis liolepis</ns3:italic> (Catalonian Wall Lizard; Chordata; Lepidosauria; Squamata; Lacertidae). The assembly contains two haplotypes with total lengths of 1 574.00 megabases and 1 478.08 megabases. Most of haplotype 1 (97.05%) is scaffolded into 20 chromosomal pseudomolecules, including the W and Z sex chromosomes. Haplotype 2 was assembled to scaffold level. The mitochondrial genome has also been assembled, with a length of 17.32 kilobases.</ns3:p>

<ns3:p>We present a genome assembly from an individual female <ns3:italic>Podarcis tiliguerta</ns3:italic> (Tyrrhenian Wall Lizard; Chordata; Lepidosauria; Squamata; Lacertidae). The assembly contains two haplotypes with total lengths of 1 462.31 megabases and 1 394.94 megabases. Most of haplotype 1 (99.26%) is scaffolded into 20 chromosomal pseudomolecules, including the W and Z sex chromosomes. Most of haplotype 2 (99.2%) is scaffolded into 18 chromosomal pseudomolecules. The mitochondrial genome has also been assembled, with a length of 17.19 kilobases.</ns3:p>

The vertebrate skull originates from two embryonic lineages, the mesoderm and the neural crest, offering a unique framework to study how developmental mechanisms connect phenotypic variation and evolutionary diversification. Using 3D geometric morphometrics, we analysed skull shape variation in lacertid lizards. Mesoderm- and neural crest-derived bones formed distinct, conserved modules at both micro- and macroevolutionary scales. In the common wall lizard ( Podarcis muralis ), rapid evolution of skull shape under sexual selection was primarily driven by neural crest-derived bones. While the primary axis of shape divergence in P. muralis aligned with a major axis of variation across lacertids, neural crest-derived bones exhibited slower evolutionary rates and lower morphological disparity than mesodermal-derived bones. We propose that this discrepancy between the role of the neural crest for skull evolution on micro- and macroevolution reflects constraints imposed by neural crest cell ...

Neural crest cells (NCCs) are a key component of the vertebrate body plan and contribute to a variety of different traits. However, their dynamic migratory behavior and spatiotemporal heterogeneity in the developing embryo pose significant challenges for their identification and isolation. Consequently, most studies of NCCs have been confined to model organisms with established transgenic tools. To overcome this limitation, we present a novel approach that combines antibody labelling with fluorescence activated cell sorting to enrich for NCCs and we demonstrate the approach in the common wall lizard ( Podarcis muralis ). Through microscopy, reverse transcription quantitative polymerase chain reaction and single-cell RNA sequencing, we show that the method enriches for NCCs as efficiently as methods relying on transgenic animals. Using this technique, we successfully characterise transcriptional profiles of NCCs in wall lizard embryos. We anticipate that this method can be applied to a ...

<ns5:p>We present a genome assembly from a female specimen of <ns5:italic>Podarcis vaucheri</ns5:italic> (Andalusian wall lizard; Chordata; Lepidosauria; Squamata; Lacertidae). The assembly contains two haplotypes with total lengths of 1,614.91 megabases and 1,510.74 megabases. Most of haplotype 1 (98.26%) is scaffolded into 20 chromosomal pseudomolecules, including the W and Z sex chromosomes. Most of haplotype 2 (98.63%) is scaffolded into 18 chromosomal pseudomolecules. The mitochondrial genome has also been assembled, with a length of 17.24 kilobases.</ns5:p>"
"Susanna C. Larsson","","Professor","Computer Science","https://openalex.org/A5090235061","Abstract Sleep problems and inadequate physical activity (PA) are associated with numerous adverse health outcomes. Most studies explored the influence of PA on sleep, but how sleep affects engagement in PA across adulthood remains less well investigated. This study examined the association between sleep traits and PA reported the following year in 51,247 Swedish women and men (55–93 years), who completed questionnaires regarding their sleep, PA, and other characteristics. Health status was assessed with data from the Swedish National Patient Registry. Odds ratios and 95% confidence intervals were estimated by binary logistic regression. In multivariable analysis, long sleep duration (≥ 9 h/night), sleep disturbance, and symptoms of sleep-disordered breathing (SDB) were associated with lower odds of engaging in walking or cycling, and exercising the following year. In addition, short sleep duration (< 7 h/night), sleep disturbance, and symptoms of SDB were linked to sedentary behavior ...

Peripheral arterial disease (PAD) is a major vascular complication associated with significant morbidity and mortality. While traditional cardiovascular risk factors such as smoking, hypertension, and diabetes are well established, emerging evidence suggests that alcohol consumption, alcoholic liver disease, and metabolic-associated steatotic liver disease may also contribute to PAD risk. This review synthesizes current epidemiological evidence linking alcohol intake, alcoholic liver disease, and metabolic-associated steatotic liver disease to PAD and explores potential mechanisms, including atherosclerosis, endothelial dysfunction, chronic inflammation, dyslipidemia, and coagulation abnormalities. Observational studies suggest a possible protective effect of light-to-moderate alcohol consumption though genetic studies challenge this notion. In addition, alcoholic liver disease and metabolic-associated steatotic liver disease are increasingly recognized as contributors to systemic vasc...

Alcohol is one of the most commonly consumed substances in the world, exhibiting complex relationships with multiple aspects of cardiovascular health and disease. The majority of the research on the topic is observational and therefore prone to bias and confounding. The available evidence suggests no risk to possible risk reduction when alcohol is consumed in low amounts (such as no more than 1 to 2 drinks a day) in regard to coronary artery disease, stroke, sudden death, and possibly heart failure. The risk associated with consuming 1 to 2 drinks a day on atrial fibrillation remains unknown. More randomized trials of low to moderate alcohol consumption are needed for more definitive conclusions. In stark contrast, heavier alcohol consumption such as binge drinking or consuming on average ≥3 drinks/d is consistently associated with worse outcomes in every cardiovascular disease entity studied. Considering the level of evidence, it remains unknown whether drinking is part of a healthy l...

Background Alcohol is a known carcinogen, yet the evidence for an association with pancreatic cancer risk is considered as limited or inconclusive by international expert panels. We examined the association between alcohol intake and pancreatic cancer risk in a large consortium of prospective studies. Methods and findings Population-based individual-level data was pooled from 30 cohorts across four continents, including Asia, Australia, Europe, and North America. A total of 2,494,432 participants without cancer at baseline (62% women, 84% European ancestries, 70% alcohol drinkers [alcohol intake ≥ 0.1 g/day], 47% never smokers) were recruited between 1980 and 2013 at the median age of 57 years and 10,067 incident pancreatic cancer cases were recorded. In age- and sex-stratified Cox proportional hazards models adjusted for smoking history, diabetes status, body mass index, height, education, race and ethnicity, and physical activity, pancreatic cancer hazard ratios (HR) and 95% confiden..."
"Feng Gao","","Professor","Computer Science","https://openalex.org/A5100729278","A mechanistic understanding of how molecular structure governs photoluminescence quantum yield (PLQY) in non-fullerene acceptors (NFAs) remains elusive, hindering further progress in organic solar cells. Here, we report a comprehensive study of nearly 100 organic semiconductors—primarily NFAs—with emission peaks spanning 550-1000 nm and PLQY values ranging from < 0.01 to > 0.70. We find that the commonly used structural, photophysical, and quantum descriptors fail to account for the observed variations. Instead, we identify the twisted intramolecular charge transfer (TICT) state as the primary quenching pathway. Through a combination of experimental spectroscopy, quantum calculations, and exciton decay dynamics, we show that TICT acts as a dark state on the first excited-state potential energy surface, leading to suppressed PLQY and multi-exponential decay. Crucially, suppressing TICT formation enables substantial enhancements in emission intensity. These results reveal a unifying mech...

Correction: Characterization of two proline-rich proteins involved in silicon deposition in Cucummis sativus

Perovskite materials have revolutionized optoelectronics by virtue of their tunable bandgaps, exceptional optoelectronic properties, and structural flexibility. Notably, the state-of-the-art performance of perovskite solar cells has reached 27%, making perovskite materials a promising candidate for next-generation photovoltaic technology. Although numerous reviews regarding perovskite materials have been published, the existing reviews generally focus on individual material systems (e.g., organic-inorganic hybrid perovskites) and specific optimizations in one particular optoelectronic application (e.g., stability engineering for solar cells), lacking a systematic overview of the progress and challenges across diverse perovskite types. This review breaks this limitation by providing a systematic overview of all perovskite categories used in solar cells classified by different criteria, including composition (organic-inorganic hybrid perovskites, all-inorganic perovskites, lead-free pero..."
"Pieter Jelle Visser","","Professor","Computer Science","https://openalex.org/A5029875703","Background The Models of Patient Engagement for Alzheimer's Disease (MOPEAD) project aimed to identify the most effective and cost-efficient recruitment model for detecting prodromal and mild Alzheimer's disease (AD) across five European countries. Objective To examine differences in cardiovascular risk factors and cognitive performance among countries and recruitment models using MOPEAD data. Methods Individuals aged 65–85 with a high risk for prodromal or mild AD were included. Four recruitment models were used: a web-based screening tool, an open house initiative (OHI), a primary care-based protocol for early detection of cognitive decline, and a tertiary care-based screening at a diabetologist clinic. Participants from Germany, Spain, the Netherlands, Sweden, and Slovenia were recruited. Cardiovascular risk factors were self-reported, and cognition was assessed using The Repeatable Battery for the Assessment of Neuropsychological Status (RBANS). Results A total of 414 individuals (...

The trajectories of core Alzheimer disease (AD) cerebrospinal fluid (CSF) biomarkers and the concurrent cognitive changes across the clinical spectrum remain unclear yet are important for clinical trial design. To map longitudinal CSF amyloid, tau, and cognitive trajectories along the clinical spectrum of AD and amyloid-negative controls. This longitudinal cohort study included participants with a minimum of 2 CSF samples from Alzheimer Centrum Amsterdam cohorts across the AD clinical spectrum (ie, abnormal amyloid levels at first visit, in different clinical stages) and cognitively normal controls with initially normal CSF markers from November 2003 to July 2019. The maximum follow-up period was 19.5 years (median [IQR], 2 [0-3] years). Data were analyzed from March 2024 to May 2025. AD biomarkers (ß-amyloid [Aß]1-42 to Aß1-40 ratio, total tau [t-tau], and phosphorylated tau [p-tau]) detected in serially collected CSF. CSF AD biomarkers were measured with Lumipulse G600II. Cognition w..."
"Nadhir Al‐Ansari","","Professor","Computer Science","https://openalex.org/A5070829177","The satellite-derived climatic variables offer extensive spatial and temporal coverage for research; however, their inherent biases can subsequently reduce their accuracy for water balance estimate. This study evaluates the effectiveness of bias correction in improving the Tropical Rainfall Measuring Mission (TRMM) rainfall and the Global Land Data Assimilation System (GLDAS) land surface temperature (LST) data and illustrates their long-term (2000–2019) hydrological assessment. The novelty lies in coupling the bias-corrected climate variables with the Thornthwaite–Mather water balance model as well as land use land cover (LULC) for improved predictive hydrological modeling. Bias correction significantly improved the agreement with ground observations, enhancing the R2 value from 0.89 to 0.96 for temperature and from 0.73 to 0.80 for rainfall, making targeted inputs ready to predict hydrological dynamics. LULC mapping showed a predominance of agricultural land (64.5%) in the area follo...

Abstract Conservation agriculture (CA) presents a promising substitute to the tillage-intensive rice–wheat cropping system (RWS) prevalent in the Indo-Gangetic plains (IGPs). In the northwestern IGPs, on-farm studies examining the impact of CA durations on soil properties and quality are limited. This study assessed the effects of CA practised for 2 (CA2), 4 (CA4), 8 (CA8), and 12 (CA12) years and conventional tillage (CT) on soil quality in the Nilokheri block of Haryana, India. The collected soil samples from 0–5 to 5–15 cm were analyzed for 22 different soil parameters, and a soil quality index (SQI) was developed using principal component analysis (PCA) for each scenario. The results showed that scenarios CA8 and CA12 had 9.8–10.7 and 11.1–11.3% lower bulk density, respectively, compared to CT. Mean weight diameter, saturated hydraulic conductivity, and water holding capacity were significantly higher in CA8 and CA12 over CT at both soil layers. Microbial biomass carbon and dehydro...

<title>Abstract</title> Climate change has assured the critical role of small dams in water resource management by enhancing local water storage, flood control, and irrigation, thereby improving resilience to climate variability. This study evaluates the effectiveness of one-arc-second digital elevation models (DEMs) in determining reservoir volume-elevation data, comparing these satellite-derived data with field survey data, which are known to be costly and time-consuming. Focused on small dams, the research compares field survey data with data derived from the one-arc-second DEM to address the lack of validation for these studies in small reservoirs. The methodology involves analyzing ten reservoirs in two different locations, namely Erbil and Sulaymaniyah governorates in Northern Iraq, using ArcGIS and Remote Sensing for digital elevation model processing. The volume-elevation data for the reservoirs are determined using ArcGIS. Consequently, evaluations of terrain metrics and sensi...

Abstract The monitoring of lotic ecosystems is an important issue. This study investigated the total petroleum hydrocarbons (THP) in the Tigris River within Baghdad City, Iraq, which is considered the ultimate water supply source of the city. The study included measurement of THP concentrations, distribution, and origins of total petroleum hydrocarbons (TPHs) in various matrices (water, sediment, and macrophyte) in Tigris River within Baghdad City, in addition to some environmental factors during two seasons (dry and wet) for October 2020 to April 2021. The sampling was collected from three sites along the river. Thirteen compounds were identified in the current investigation from total petroleum hydrocarbons (TPHs), including hexatriacontane, tetracontane, tetratetracontane, undecane, dodecane, hexane, nonane, tetradecane, hexadecane, eicosane, dortiacontane, decane, and octadecane. TPHs concentrations were arranged in the following order macrophyte > sediment > water, and the oil ref...

Real-time monitoring of canopy chlorophyll content is crucial for understanding crop growth and guiding precision agricultural management. The SPAD chlorophyll meter is a valuable tool for assessing nitrogen status in maize (Zea mays L.), a key cereal crop used for food, feed, and biofuels. Efficient nitrogen management is essential to maximize maize yield, particularly under varying water regimes. A study conducted over two years (2020-2021) utilized a strip plot design to investigate the spatiotemporal dynamics of SPAD readings and their correlation with maize yield under rainfed (M1) and irrigated (M2) conditions. Eight precision nitrogen management practices were implemented, including SPAD at sufficiency index and Green Seeker at response index, achieving ranges of 86-100% and 1.11-1.41, respectively. The findings revealed that irrigated maize produced significantly higher grain yields (6347 kg ha-1) compared to rainfed maize (5262 kg ha-1). The highest yield (9508.2 kg ha-1) was ..."
"Jari Tiihonen","","Professor","Computer Science","https://openalex.org/A5072366449","ABSTRACT Introduction Despite well‐known diagnostic and neurobiological overlaps between psychopathic traits and schizophrenia, it has remained unclear whether psychopathic traits increase the risk for later schizophrenia. Former studies have proven only a weak correlation between psychopathy and DSM axis I diagnoses. Methods We combined data from individuals who underwent forensic psychiatric evaluations (FPEs) at Niuvanniemi Hospital between 1984 and 1993 with the records from the Care Register for Health Care to examine the relationship between psychopathic traits, measured by the Psychopathy Checklist‐Revised (PCL‐R), and the development of schizophrenia following the evaluation. We conducted survival analyses using Kaplan–Meier estimates and Cox proportional hazards models, with a follow‐up period of up to 40 years. Mortality data were obtained from the National Death Registry. Statistical analyses were adjusted for age, sex, criminal responsibility, and substance abuse disorder a...

Brexpiprazole is a second-generation antipsychotic with multiple indications, including the treatment of schizophrenia. As a partial dopamine agonist, brexpiprazole differs from most other antipsychotics, yet uncertainties about its full mechanism of action have led to some ambiguity among prescribers. To address this gap, an international panel of psychiatric experts was organized and convened with funding from Otsuka Pharmaceutical Europe Ltd and H. Lundbeck A/S to discuss the safe and effective use of brexpiprazole across different stages of schizophrenia treatment. Brexpiprazole's pharmacological profile-characterized by balanced binding affinities across norepinephrine, dopamine, and serotonin receptors-contributes to its efficacy in multiple symptom domains, including agitation and negative symptoms. Its tolerable safety profile, marked by minimal activation and minimal sedation and a relatively low risk of long-term cardiometabolic concerns, further supports its clinical utility...

ABSTRACT Background Individuals with bipolar disorder face an elevated risk of premature death, often due to external causes such as accidental injuries, self‐harm, and substance‐related deaths. This study aimed to investigate the incidence of severe poisonings among individuals with bipolar disorder and to examine associated demographic and clinical factors. Methods We conducted a cohort study using data from national registers in Finland, measuring hospitalizations and deaths due to poisoning by medicines or illegal substances in 1996–2018. Cox proportional hazards regression models were used to assess associations between predictor variables and poisoning outcomes. Results The study population comprised 60,045 individuals aged 15–65 diagnosed with bipolar disorder in 1987–2018. During the study period, 13.1% ( N = 7872) of the population experienced at least one poisoning resulting in hospitalization or death. The age‐standardized rate of hospitalizations was 50.6 (95% CI, 49.5–51.7..."
"Bo Mattìasson","","Professor","Computer Science","https://openalex.org/A5041402257","In recent years, injectable cryogels have been frequently used in various biomedical applications such as tissue engineering, drug delivery, therapeutics, therapy, cell transplantation and immunotherapy. Cryogels, one of the biomaterials with injectable 3D scaffold structures, have important properties such as biocompatibility, physical resistance, macropore and sensitivity. In addition to being biocompatible, cryogels, consisting of three-dimensional (3D) scaffolds with physicochemical properties such as elasticity, hardness and biological degradation, provide a structural support environment for tissue development along with cell attachment. In addition, injectable cryogels can be loaded with therapeutic agents or cells in tissue engineering and clinical studies. The purpose of this review is to explain the physical and chemical properties of injectable cryogels consisting of three-dimensional scaffolding, after explaining the properties and synthesis stages of cryogels, and it is th...

Microbial contaminants are responsible for several infectious diseases, and they have been introduced as important potential food- and water-borne risk factors. They become a global burden due to their health and safety threats. In addition, their tendency to undergo mutations that result in antimicrobial resistance makes them difficult to treat. In this respect, rapid and reliable detection of microbial contaminants carries great significance, and this research area is explored as a rich subject within a dynamic state. Optical sensing serving as analytical devices enables simple usage, low-cost, rapid, and sensitive detection with the advantage of their miniaturization. From the point of view of microbial contaminants, on-site detection plays a crucial role, and portable, easy-applicable, and effective point-of-care (POC) devices offer high specificity and sensitivity. They serve as advanced on-site detection tools and are pioneers in next-generation sensing platforms. In this review,..."
"T. Ekelöf","","Professor","Computer Science","https://openalex.org/A5115589102","In this paper, we study scalar mediator induced nonstandard interactions (SNSIs) in the context of the ESSnuSB experiment. In particular, we study the capability of ESSnuSB to put bounds on the SNSI parameters and also study the impact of SNSIs in the measurement of the leptonic <a:math xmlns:a=""http://www.w3.org/1998/Math/MathML"" display=""inline""><a:mi>C</a:mi><a:mi>P</a:mi></a:math> phase <c:math xmlns:c=""http://www.w3.org/1998/Math/MathML"" display=""inline""><c:msub><c:mi>δ</c:mi><c:mrow><c:mi>C</c:mi><c:mi>P</c:mi></c:mrow></c:msub></c:math>. Existence of SNSIs modifies the neutrino mass matrix and this modification can be expressed in terms of three diagonal real parameters (<e:math xmlns:e=""http://www.w3.org/1998/Math/MathML"" display=""inline""><e:msub><e:mi>η</e:mi><e:mrow><e:mi>e</e:mi><e:mi>e</e:mi></e:mrow></e:msub></e:math>, <g:math xmlns:g=""http://www.w3.org/1998/Math/MathML"" display=""inline""><g:msub><g:mi>η</g:mi><g:mrow><g:mi>μ</g:mi><g:mi>μ</g:mi></g:mrow></g:msub></g:math>,...

The armed invasion of Ukraine by the Russian Federation has adversely affected the relations between Russia and Western countries. Among other aspects, it has put scientific cooperation and collaboration into question and changed the scientific landscape significantly. Cooperation between some Western institutions and their Russian and Belarusian partners were put on hold after February 24, 2022. The CERN Council decided at its meeting in December 2023 to terminate cooperation agreements with Russia and Belarus that date back a decade. CERN is an international institution with UN observer status, and has so far played a role in international cooperation which was independent of national political strategies. We argue that the Science4Peace idea still has a great value and scientific collaboration between scientists must continue, since fundamental science is by its nature an international discipline. A ban of scientists participating in international cooperation and collaboration is ag...

The ISOLDE Scientific Infrastructure at CERN offers a unique range of post-accelerated radioactive beams. The scientific program can be improved with the “Isolde Superconducting Recoil Separator” (ISRS), an innovative spectrometer able to deliver unprecedented (A, Z) resolution. In this paper we present an overview of the physics and ongoing technical developments.

The results of a search with the DELPHI Barrel RICH for Cherenkov rings having radii greater than those produced by ultrarelativistic particles are presented. The search for such anomalous rings is based on the data collected by the DELPHI Collaboration at CERN during the LEP1 and LEP2 periods. The DELPHI RICH detector was conceived for the identification of the stable and quasi-stable hadrons ($\pi/K/p$). The present analysis was made investigating electron-like particles. A subsample of events containing anomalous rings has been identified for which the probability that the reconstructed rings in a given event are due to fortuitous combinations of background hits is low ($10^{-3}$ or less). A detailed study of background sources capable of producing apparently anomalous rings has been done; it indicates that the background hypothesis has a low probability. Additional arguments against this hypothesis are provided by by a comparison of rates of events with single and double anomalous ...

The nature of b-quark jet hadronisation has been investigated using data taken at the Z peak by the DELPHI detector at LEP. Two complementary methods are used to reconstruct the energy of weakly decaying b-hadrons, $E_{\mathrm{B}}^{\mathrm{weak}}$ . The average value of $x^{\mathrm{weak}}_{\mathrm{B}} = E_{\mathrm{B}}^{\mathrm{weak}}/E_{\mathrm{beam}}$ is measured to be 0.699±0.011. The resulting $x^{\mathrm{weak}}_{\mathrm{B}}$ distribution is then analysed in the framework of two choices for the perturbative contribution (parton shower and Next to Leading Log QCD calculation) in order to extract measurements of the non-perturbative contribution to be used in studies of b-hadron production in other experimental environments than LEP. In the parton shower framework, data favour the Lund model ansatz and corresponding values of its parameters have been determined within PYTHIA 6.156 from DELPHI data: $$a= 1.84^{+0.23}_{-0.21}\quad\mbox{and}\quad b=0.642^{+0.073}_{-0.063}~\mathrm{GeV}^{-..."
"Anders Nilsson","","Professor","Computer Science","https://openalex.org/A5075231234","In this work, we introduce a modified dip-and-pull electrochemical X-ray photoelectron spectroscopy (ECXPS) approach that offers new mechanistic insight into the alkaline carbon monoxide reduction reaction (CORR) over a Cu(111) single crystal surface. We tackle two major unresolved questions in the CORR mechanism that persist in the literature. Firstly, we address the mechanism for methane formation on Cu(111) and show that the mechanism likely proceeds via atomic carbon, which subsequently couples, leading to the accumulation of amorphous carbon on the surface. Secondly, we provide insight into whether the mechanism for acetate formation occurs entirely on the surface or partially within the solution phase, showing that acetate is present on the surface, indicating a surface-based reaction. These insights into surface-based mechanisms provide a handle for designing future catalysts that can efficiently target the binding of specific intermediates. Furthermore, we expect that our modif...

Abstrakt In dieser Arbeit stellen wir einen modifizierten Ansatz der elektrochemischen Röntgen‐Photoelektronenspektroskopie (ECXPS) vor, der neue mechanistische Einblicke in die alkalische Kohlenmonoxid‐Reduktionsreaktion (CORR) an einer Cu(111)‐Einkristalloberfläche bietet. Wir befassen uns mit zwei wichtigen ungelösten Fragen zum CORR‐Mechanismus, die in der Literatur immer noch bestehen. Erstens befassen wir uns mit dem Mechanismus der Methanbildung auf Cu(111) und zeigen, dass der Mechanismus wahrscheinlich über atomaren Kohlenstoff abläuft, der anschließend koppelt, was zur Ansammlung von amorphem Kohlenstoff auf der Oberfläche führt. Zweitens geben wir Aufschluss darüber, ob der Mechanismus für die Acetatbildung vollständig an der Oberfläche oder teilweise in der Lösungsphase abläuft, indem wir zeigen, dass Acetat an der Oberfläche vorhanden ist, was auf eine oberflächenbasierte Reaktion hindeutet. Diese Einblicke in die auf der Oberfläche stattfindenden Mechanismen bieten einen ...

In this work, we introduce a modified dip-and-pull ECXPS approach that offers new mechanistic insight into the alkaline CORR over a Cu(111) single crystal surface. We tackle two major unresolved questions in the CORR mechanism that persist in the literature. Firstly, we address the mechanism for methane formation on Cu(111) and show that the mechanism likely proceeds via atomic carbon, which subsequently couples, leading to the accumulation of amorphous carbon on the surface. Secondly, we provide insight into whether the mechanism for acetate formation occurs entirely on the surface or partially within the solution phase, showing that acetate is present on the surface, indicating a surface-based reaction. These insights into surface-based mechanisms provide a handle for designing future catalysts that can efficiently target the binding of specific intermediates. Furthermore, we expect that our modified approach to dip-and-pull ECXPS - in which we have changed the electrode geometry, th..."
"Robin Room","","Professor","Computer Science","https://openalex.org/A5082884914","Cross sectional research has demonstrated that screening tool questions on frequency of alcohol consumption are a better predictor of dependence and harmful drinking in younger adults; questions about quantity per occasion are a better predictor in older adults. The aim of this study is to see if this relationship also holds longitudinally. A total of 9076 respondents aged 15 and over completed at least two waves of the longitudinal annual Household Income and Labour Dynamics in Australia survey 10 years apart between 2001-2010 and 2012-2020. Standardised scores from responses to questions on drinking quantity and frequency in the first survey were used to predict consumption 10 years later in groups stratified by age. Frequency of consumption was a significantly better predictor of future consumption than quantity in younger drinkers (aged < 36; β = 9.3, 95% confidence interval [CI] 8.6-10.0), than older drinkers (aged > 49; β = 5.1, 95% CI 4.8-5.5) while quantity was a better predict...

Abstract Introduction Despite successful public health campaigns, tobacco use persists as a major cause of preventable illness and death. While tobacco taxation is recognized as an effective control strategy, concerns remain about potential financial strain on lower socioeconomic groups. This study investigates the relationship between household tobacco expenditure and financial stress in Australia, a country with high tobacco taxes and declining smoking rates. Methods Household data from the 2015-16 Australian Household Expenditure Survey were analysed (N=10,036). Financial stress was measured using a scale based on nine self-reported indicators. Respondents were asked to report if their household had experienced any of these difficulties, e.g. inability to pay utility bills or going without meals. Negative binomial regression models assessed the association between tobacco expenditure share and financial stress, adjusting for sociodemographic factors, household wealth, and other expe..."
"Bertil Persson","","Professor","Computer Science","https://openalex.org/A5004175173","Idealet om at politikk skal være kunnskapsbasert står sterkt. Politikken henter kunnskap og forskning fra forskningssystemet, særlig fra instituttsektoren. Denne boka handler om forskningsinstitusjoner og forskningspolitikk, og mest om instituttsektoren. Det er institusjoner utenfor universiteter og høyskoler med forskning og utviklingsarbeid som hovedoppgave. Slike institutter finnes i mange land, men deres roller og antall har variert over tid og mellom land. Her handler det mest om norske forhold, men også om forskningsinstitutter i Danmark og Sverige. Boka gir ny innsikt om instituttsektorens roller og virke i Skandinavia. Den handler også om universiteter og høyskoler og forholdet mellom institusjonene. Ulike forvaltnings- og politikkområders forhold til forskningsinstitutter og bruk av forskning diskuteres. Boka har 15 forfattere og er flerfaglig. De fleste er statsvitere og historikere og kommer fra universiteter og høyskoler, og fra instituttsektoren. Mange har også egne erfari...

Background: Teledermoscopy (TDS) emerges as an efficient tool for diagnosing skin lesions. In Sweden, double reading is the standard of care, but risk factors for misdiagnosis or mismanagement using single reader evaluations (SRE) are not well-studied. This study aimed to assess the accuracy of SRE compared with the gold standard in TDS. Methods: This retrospective cohort study involved 1,997 TDS referrals sent from general practitioners to dermatologists in Stockholm, Sweden, selected based on dermoscopic diagnoses. All referrals underwent double reader evaluations (DRE). Each case was reassessed by a single external assessor, blinded to the DRE result. Based on predefined rules, a gold standard for the most correct diagnosis was established. Diagnostic accuracy and risk factors for misdiagnosis were evaluated. The trial was registered on ClinicalTrials.gov (ID NCT05033678). Results: Primary diagnosis by SRE agreed with the gold standard on benign-malignant classification in 84% of ca...

In the dawning era of artificial intelligence (AI), health care stands to undergo a significant transformation with the increasing digitalization of patient data. Digital imaging, in particular, will serve as an important platform for AI to aid decision making and diagnostics. A growing number of studies demonstrate the potential of automatic pre-surgical skin tumor delineation, which could have tremendous impact on clinical practice. However, current methods rely on having ground truth images in which tumor borders are already identified, which is not clinically possible. We report a novel approach where hyperspectral images provide spectra from small regions representing healthy tissue and tumor, which are used to generate prediction maps using artificial neural networks (ANNs), after which a segmentation algorithm automatically identifies the tumor borders. This circumvents the need for ground truth images, since an ANN model is trained with data from each individual patient, repres...

Diagnosing invasive cutaneous melanoma (CM) can be challenging due to subjectivity in distinguishing equivocal nevi, melanoma in situ and thin CMs. The underlying molecular mechanisms of progression from nevus to melanoma must be better understood. Identifying biomarkers for treatment response, diagnostics and prognostics is crucial. Using biomedical data from biobanks and population-based healthcare data, translational research can improve patient care by implementing evidence-based findings. The BioMEL biobank is a prospective, multicentre, large-scale biomedical database on equivocal nevi and all stages of primary melanoma to metastases. Its purpose is to serve as a translational resource, enabling researchers to uncover objective molecular, genotypic, phenotypic and structural differences in nevi and all stages of melanoma. The main objective is to leverage BioMEL to significantly improve diagnostics, prognostics and therapy outcomes of patients with melanoma. The BioMEL biobank co..."
"Jonas Bergquist","","Professor","Computer Science","https://openalex.org/A5067538945","Previous scientific explorations of kohl and other make-up substances from ancient Egypt have revealed a considerable diversity of materials and recipes used in different regions and time periods. However, samples from Sudanese Nubia have never been included in scientific investigations of make-up substances used along the Nile valley. For the first time, 24 samples of kohl and other cosmetics from Bronze Age Sudanese Lower Nubia (c. 2055–1070 BCE) were analysed using optical microscopy, GC-MS, SEM-EDS, ATR-FTIR and XRD. Beyond expanding our knowledge of make-up usage in the ancient Nile valley by including samples from Sudan, this study adds further depth to our understanding of make-up substances in ancient Northeast Africa by exploring samples from well-defined archaeological contexts. The multi-analytical approach presented here sheds light on the diversity of recipes used by various communities in the Middle Nile valley during the Bronze Age. Most samples are dominated by lead sul...

Myalgic encephalomyelitis (ME) is a chronic, multisystem illness characterized by post-exertional malaise (PEM) and cognitive dysfunction, yet the molecular mechanisms driving these hallmark symptoms remain unclear. This study investigated haptoglobin (Hp) as a potential biomarker of PEM severity and cognitive impairment in ME, with a focus on Hp phenotypes and structural proteoforms. A longitudinal case–control study was conducted in 140 ME patients and 44 matched sedentary healthy controls. In the discovery phase, global plasma proteomic profiling was performed in 61 ME patients and 20 controls before and after a standardized, non-invasive stress protocol in order to induce PEM. Associations between Hp levels, phenotype, and cognitive performance were assessed. In the validation phase, plasma Hp concentrations and proteoform composition were analyzed in an independent cohort of 89 ME patients and 24 controls using high-performance liquid chromatography (HPLC). ME patients demonstrate...

Abstract Background Critical Illness Myopathy (CIM) is a devastating consequence of modern critical care, causing dramatic loss of muscle mass and function in intensive care unit (ICU) patients. However, the loss in function by far exceeds the loss in muscle mass and myosin content, but the molecular mechanisms underlying the loss of force and myosin-expressing non-force (NF) generating fibers remain elusive. Objectives To explore the mechanisms underlying the compromised myosin function in ICU patients exposed to 12-day mechanical ventilation and immobilization. Methods Mass spectrometry-based proteomics and molecular dynamics simulations were used to explore the pathophysiology underlying the compromised muscle fiber function previously reported at the single muscle fiber level in six ICU patients on 12 th day compared with the 1 st day of mechanical ventilation and immobilization. Results Previous measurements revealed single muscle fiber size and specific force to be decreased by ∼...

Sodium-glucose co-transporter 2 inhibitors like canagliflozin (CFZ) have shown promise in preventing hyperinsulinemia-associated laminitis in horses, but data on pharmacokinetics, tolerability, and controlled studies are limited. This randomized, open-label, placebo-controlled, crossover study evaluated these aspects of CFZ treatment in eight healthy Standardbred mares. Each horse received single supratherapeutic oral doses of CFZ (1.8mg/kg or 3.6mg/kg) and placebo, with a two-week washout between treatments. A graded glucose infusion (GGI) was administered post-treatment to evaluate glucose and insulin responses. Plasma CFZ, glucose, insulin, urinary glucose, serum biochemistry, and urinalysis samples were collected over 72hours post-treatment. For CFZ 1.8mg/kg, median Cmax was 2623ng/mL, Tmax 2.2hours, and T1/2Z 21.8hours; for 3.6mg/kg, Cmax was 4975ng/mL, Tmax 2.8hours, and T1/2Z 23.0hours. The pharmacokinetics of CFZ displayed dose-proportionality across the two tested doses. Insul...

Abstract Background Myalgic encephalomyelitis / chronic fatigue syndrome (ME/CFS) is a multisystem disorder characterised by unrelenting fatigue, post-exertional malaise, and dysfunction across immune, nervous, metabolism, and endocrine systems. Given the broad role of steroid hormones in regulating these systems, this study investigated differences in the steroid metabolome and network dynamics between ME/CFS patients and matched controls. Methods Blood plasma steroid levels were quantified using Ultra-Performance Supercritical Fluid Chromatography- Tandem Mass Spectrometry (UPSFC-MS/MS) in ME/CFS patients ( n = 24) and age and gender matched controls ( n = 24). Group comparisons of absolute steroid concentrations were performed using Mann-Whitney U tests. Partial Spearman correlation networks were evaluated to examine direct associations between steroids within each group, and centrality metrics were used to evaluate structural differences. Steroid-steroid ratios were analysed to ref..."
"Petre Stoica","","Professor","Computer Science","https://openalex.org/A5007877889",""
"Henk Wymeersch","","Professor","Computer Science","https://openalex.org/A5033860704",""
"A. Hallgren","","Professor","Computer Science","https://openalex.org/A5052702737","The science program of the Radio Neutrino Observatory-Greenland (RNO-G) extends beyond particle astrophysics to include radioglaciology and, as we show herein, solar physics, as well. Impulsive solar flare observations not only permit direct measurements of light curves, spectral content, and polarization on time scales significantly shorter than most extant dedicated solar observatories, but also offer an extremely useful above-surface calibration source, with pointing precision of order tens of arc-minutes. Using the early RNO-G data from 2022-2023, observed flare characteristics are compared to well-established solar observatories. Also, a number of individual flares are used to highlight angular reconstruction and calibration methods. RNO-G observes signal excesses during solar flares reported by the solar-observing Callisto network and in coincidence with about 60% of the brightest excesses recorded by the SWAVES satellite, when the Sun is above the horizon for RNO-G. In these obs...

Abstract We recently reported on the radio-frequency attenuation length of cold polar ice at Summit Station, Greenland, based on bi-static radar measurements of radio-frequency bedrock echo strengths taken during the summer of 2021. Those data also allow studies of (a) the relative contributions of coherent (such as discrete internal conducting layers with sub-centimeter transverse scale) vs incoherent (e.g. bulk volumetric) scattering, (b) the magnitude of internal layer reflection coefficients, (c) limits on signal propagation velocity asymmetries (‘birefringence’) and (d) limits on signal dispersion in-ice over a bandwidth of ~100 MHz. We find that (1) attenuation lengths approach 1 km in our band, (2) after averaging 10 000 echo triggers, reflected signals observable over the thermal floor (to depths of ~1500 m) are consistent with being entirely coherent, (3) internal layer reflectivities are ≈–60 $\to$ –70 dB, (4) birefringent effects for vertically propagating signals are smalle...

Abstract The ARIANNA experiment is an Askaryan radio detector designed to measure high-energy neutrino induced cascades within the Antarctic ice. Ultra-high-energy neutrinos above 10 16 eV have an extremely low flux, so experimental data captured at trigger level need to be classified correctly to retain as much neutrino signal as possible. We first describe two new physics-based neutrino selection methods, or “cuts”, (the updown and dipole cut) that extend the previously published analysis to a specialized ARIANNA station with 8 antenna channels, which is double the number used in the prior analysis. For a standard trigger with a threshold signal to noise ratio at 4.4, the new cuts produce a neutrino efficiency of > 95% per station-year of operation, while rejecting 99.93% of the background (corresponding to 53 remaining experimental background events). When the new cuts are combined with a previously developed cut using neutrino waveform templates, all background is removed at no cha...

The Radio Neutrino Observatory in Greenland (RNO-G) is the only ultrahigh energy (UHE, ${\gtrsim}30$ PeV) neutrino monitor of the Northern sky and will soon be the world's most sensitive high-uptime detector of UHE neutrinos. Because of this, RNO-G represents an important piece of the multimessenger landscape over the next decade. In this talk, we will highlight RNO-G's multimessenger capabilities and its potential to provide key information in the search for the most extreme astrophysical accelerators. In particular, we will highlight opportunities enabled by RNO-G's unique field-of-view, its potential to constrain the sources of UHE cosmic rays, and its complementarity with IceCube at lower energies.

The Radio Neutrino Observatory in Greenland (RNO-G) is an array of radio detector stations which has been designed to study ultra-high energy (𝐸 ≳ $10^{18}$ eV) neutrinos. The experiment, when completed, will have the best sensitivity in this energy range and will yield a major advancement in our understanding of the sources and propagation of the highest energy cosmic rays. While RNO-G will be sensitive to primarily 𝐸 ≳ 100 PeV neutrinos, the optical-based detectors only have a large enough exposure to study up to ∼ 1–10 PeV, leaving a gap in the energy range between the two detection methods. For RNO-G, the energy threshold is set by our ability to distinguish the Askaryan pulses, created from neutrino interactions, from the irreducible background of thermal noise. Using modern machine learning techniques, an online trigger can be implemented to identify small-amplitude pulses from in-ice cascades and thereby decrease the energy threshold of RNO-G. Such an advancement will increase t..."
"J. G. Smith","","Professor","Computer Science","https://openalex.org/A5008638540","Type 2 diabetes is a growing global concern with serious complications, including kidney damage and cardiovascular morbidity and mortality. Monitoring albuminuria, which is associated with these complications, is crucial in optimal diabetes management. Gut microbiota composition has been suggested to impact albuminuria, but large studies with granular data are lacking. We investigated the relationship between 1002 gut microbial species, 1308 plasma metabolites and albuminuria in 752 participants with type 2 diabetes from the Swedish CArdioPulmonary BioImage Study. To determine the relative abundance of species, we employed deep shotgun metagenomic sequencing of fecal samples. Plasma metabolites were analyzed using mass spectrometry-based methods. We identified three species that were associated with albuminuria, including Sellimonas intestinalis, Eggerthellales sp., Ellagibacter isourolithinifaciens. Two of these species were replicated in an independent pre-diabetic population (n=3,42...

ABSTRACT Objective Although previous studies have shown reduced cardiovascular events following parathyroidectomy (PTX), it is unclear whether this extends to contemporary patients diagnosed and treated with milder disease than previously. The aim of this nation‐wide study was to determine the effect on cardiovascular events after PTX, and to comprehensively evaluate cardiovascular disease manifestations in patients with primary hyperparathyroidism, (pHPT). Design The cohort consisted of 5009 patients who underwent PTX and were identified from the Scandinavian Quality Register for Thyroid, Parathyroid and Adrenal Surgery. Patients were matched with 14,983 population controls. Methods Data was linked with the National Patient and Death Registries. Incidence rate ratios (IRRs) were estimated before and after PTX for recurrent events of acute myocardial infarction, stroke, transient ischemic attack (TIA), and first‐onset diagnoses of coronary artery disease, heart failure, aortic and mitr...

Abstract This collection gathers thirteen contributions by a number of historians, friends, colleagues and/or students of Jinty’s, who were asked to pick their favourite article by her and say a few words about it for an event held in her memory on 15 January 2025 at King’s College London. We offer this collection in print now for a wider audience not so much because it has any claim to be exhaustive or authoritative, but because taken all together these pieces seemed to add up to a useful retrospective on Jinty’s work, its wider context, and its impact on the field over the decades. We hope that, for those who know her work well already, this may be an opportunity to remember some of her classic (and a few less classic) articles, while at the same time serving as an accessible introduction to her research for anyone who knew her without necessarily knowing about her field, as well as for a new and younger generation of readers.

Heart failure (HF) is characterized by hemodynamic derangements that are likely to mediate systemic metabolic perturbations but limited data are available. Plasma metabolite profiling provides opportunities for comprehensive investigation of such perturbations. Here, we aimed to characterize plasma profiles that associate with HF and their relationship with central hemodynamics, symptom burden, and response to restoration of cardiac function by heart transplantation. Untargeted metabolite profiling was conducted with mass spectrometry in 2 independent case-control samples. In total, 89 of 797 studied metabolites were significantly associated with HF in both cohorts with concordant directionality. Amino acid, carbohydrate, and nucleotide metabolites were enriched for association with HF and were consistently increased in HF cases. A subset of patients with advanced HF subsequently underwent heart transplantation, after which 17 of the 89 metabolites returned significantly toward healthy..."
"Bo Edvardsson","","Professor","Computer Science","https://openalex.org/A5012900375",""
"Anders Berglund","","Professor","Computer Science","https://openalex.org/A5035616597","There is some evidence that mastocytosis patients are at increased risk of skin cancer. This study aimed to assess the risk of malignant melanoma (MM), melanoma in situ (Mis), and basal cell carcinoma (BCC). A dataset was generated by individual-level record linkages between Swedish population registers including the National Patient Register (NPR), the Swedish Cancer Register (SCR), and the Population Register (PR). Adult patients with a mastocytosis diagnosis between 2001 and 2018 were identified in the SCR and NPR. For each case, 5 mastocytosis-free comparators matched on age, sex, and county of residence were randomly chosen from the PR. Records of skin cancer were identified in the SCR and NPR. In total, the study encompassed 2,040 mastocytosis patients of whom 63 had a record of MM/Mis and 168 a record of BCC. Compared with comparators, the risk of MM/Mis was more than twofold higher (OR 2.39, 95% CI 1.8–3.2). Risk estimates for BCC were also elevated (OR 1.77, 95% CI 1.49–2.14)....

Abstract Background Evolution is a fundamental concept in biology education, recently emphasized in the Swedish curriculum for Year 4–6. However, teaching evolution poses challenges, necessitating innovative educational tools. This study explores the development and use of a comic book, Cats on the Run–A Dizzying Evolutionary Journey , designed to teach evolutionary concepts to young students through a narrative involving two modern-day house cats traveling through time and space. Results To explore what function the material has for students’ meaning making we analyze what students describe to have learned working with the comic Cats on the Run, and how aspects of the comic book are reflected in the students’ self-reported learning. The study involved 159 students from Grades 4–6 who used the comic book in their biology lessons. Analysis of student survey responses revealed that the students draw on the comic’s narrative and imagery as they report on learning about key evolutionary co...

Abstract Background Intrinsically resistant glioma stem cells (GSCs) in the setting of a highly immunosuppressive tumor microenvironment (TME) remain the most predominant phenomenon leading to unfavorable therapeutic outcomes in glioblastoma (GBM). Hence there is an unmet need for novel anti-GBM therapeutic paradigms that can effectively target GSCs while simultaneously reprogramming the TME. Methods In this study, we leverage evidence from SMAC mimetic screening to evaluate and characterize the anti-tumor and immune TME modulating impacts of the lead SMAC mimetic Xevinapant at the single cell level in GBM. We utilized viability assays and orthotopic human and murine GBM models to assess the survival impacts of Xevinapant on GSCs in vitro and in vivo. Moreover, we employed single-cell RNA sequencing (scRNA-seq) to investigate the modulation impact of Xevinapant on GBM TME. Lastly, we investigated drug combination synergies to address potential mechanisms of tolerance or resistance to X...

Background Papillary renal cell carcinoma (pRCC) is the second most common kidney cancer subtype, yet our understanding of its tumor immune microenvironment (TIME) remains limited. Objective We utilized multiplex immunofluorescence (mIF) and spatial transcriptomics (ST) to evaluate immune cell architecture in pRCC contrasted with clear cell RCC (ccRCC). Methods Localized RCC tumors (16 pRCC, 70 ccRCC) underwent mIF using markers for T cells, B cells, and tumor-associated macrophages (TAMs). Spatial data in both tumor and stromal compartments of the TIME were collected. A post hoc recurrence free survival analysis (RFS) was performed using Cox proportional hazard models. Single-cell ST was performed on a subset of samples, utilizing probes against 960 transcripts. Cell density, cell spatial clustering, and spatially varying gene expression were analyzed. Results Immune cell density was statistically lower in pRCC amongst functional CD8T cells, while cell clustering was higher amongst M2..."
"Lars Alfredsson","","Professor","Computer Science","https://openalex.org/A5071141746","Cardiovascular disease (CVD) is the leading cause of death in Europe, with myocardial infarction (MI) being one of its most severe manifestations. While many risk factors for CVD are well known, occupational exposures remain relatively understudied-especially in analyses that adjust for co-occurring workplace exposures. This study aimed to examine the association between occupational exposure to chemicals and particles and the risk of first-time MI. The cohort included all Swedish residents born between 1930 and 1990 who were employed between 1985 and 2013 and had no prior MI. Participants were followed from 1986 to 2017, and their occupational histories were linked to the Swedish Job Exposure Matrix (SweJEM) to estimate exposure to 31 chemicals and particles. MI cases were identified through national hospital discharge and cause of death registers. Using discrete time proportional hazards regression, we estimated gender-specific hazard ratios, adjusting for age, decision authority, ph...

The influence of body weight across the life course on multiple sclerosis (MS) progression remains incompletely understood. While excess body mass at diagnosis is associated with disability progression, it is unclear how early-life and adult BMI jointly affect long-term outcomes. We aimed to investigate the separate and combined effects of BMI at age 20 and at diagnosis on MS progression. We studied 2940 individuals with relapsing-onset MS from a population-based case-control study with prospective follow-up through the Swedish MS registry. BMI was calculated from self-reported weight at age 20 and at diagnosis. Outcomes included confirmed disability worsening (CDW), and time to reach EDSS 3 and EDSS 4. Cox regression and general linear models were used to examine associations between BMI and MS progression, including interaction terms. High BMI (> 28 kg/m2) at age 20 was associated with higher disability at diagnosis (β = 0.15, p = 0.0015), while BMI at diagnosis predicted increased r...

Abstract Background Rheumatoid arthritis (RA) is a chronic disease influenced by genetic and environmental factors, with viral infections potentially influencing the immune system and increase the risk of autoimmune diseases like RA. This study investigated the relationship between exposure to chikungunya and dengue infections, indicated by the presence of arboviral IgG antibodies, and risk of developing RA dichotomized by anti‐citrullinated protein antibody (ACPA) status. Methods Serum samples from the Malaysian Epidemiological Investigation of Rheumatoid Arthritis (MyEIRA) population‐based case‐control study involving 1235 RA cases and 1625 controls were assayed for IgG antibodies against chikungunya and dengue viruses. Positive results indicate previous exposure to the studied arboviral infections. Logistic regression and Mann–Whitney U analyses were performed to estimate the risk of developing ACPA‐positive/ACPA‐negative RA. Results We observed a low occurrence of chikungunya IgG a...

A better understanding of factors associated with multiple sclerosis (MS) disease activity and disability is needed. Given the strong link between comorbid depression and MS disease activity and disability, we aimed to determine whether the depression genetic burden, as modelled using its polygenic score, is associated with MS disease activity and disability worsening. In this cohort study, we used samples from neurologist-defined adult people with MS (PwMS) followed in clinical care or during a clinical trial from existing cohorts: Canada, the United States (US), and Sweden with extensive longitudinal phenotypes. We computed the depression polygenic score (PGS) and tested its association with annualized relapse rate and worsening disability. In the US cohort, we additionally explored the time to relapse, number of enhancing lesions, and confirmed Expanded Disability Status Scale (EDSS) worsening during the study period. We included 3,420 relapsing-onset PwMS of European genetic ancest...

Importance The implications of socioeconomic factors, including educational level, for multiple sclerosis (MS) progression remain unclear. Understanding whether educational level directly affects MS outcomes or is confounded by lifestyle risk factors and treatment choices could inform personalized care strategies. Objective To investigate the association between educational level and outcomes related to MS, including worsening of disability, cognition, and health-related quality of life, after adjusting for potential confounding factors or mediation by lifestyle factors and treatment. Design, Setting, and Participants This cohort study used data from 2 large, population-based case-control studies conducted in Sweden from April 2005 to December 2019 that used Swedish MS Registry data with detailed clinical and sociodemographic information. Patients with relapsing-onset MS aged 25 years or older at disease onset after 1995 were followed up from diagnosis until April 6, 2022, with a mean ..."
"Kerry S. Courneya","","Professor","Computer Science","https://openalex.org/A5047611713","ABSTRACT Purpose Wearable sensors that track physical activity in daily life may offer insights that help healthcare providers optimize care plans for individuals with cancer. Therefore, we examined the links between lower health-related fitness and worse patient-reported health and various step-based metrics. Methods The Alberta Moving Beyond Breast Cancer Study enrolled 1,528 women recently diagnosed with breast cancer and measured health-related fitness and patient-reported health outcomes near diagnosis, and one year later. Step counts and intensity (cadence, peak steps) were measured by activPAL® over seven days at baseline. We estimated cross-sectional associations (odds ratios (OR)) at baseline, and prospective associations between low baseline stepping and low fitness and poorer health at one year, adjusting for age, demographics, height, weight, and cancer diagnosis/treatment. Results At baseline 1,408 breast cancer survivors (mean age 56 yrs; early stage (90%)) provided valid...

Background: Current guidelines endorse the integration of exercise into cancer care. The diagnosis of cancer and its treatment, however, may introduce factors that make exercise engagement difficult, especially for individuals with advanced stages of disease. In this paper, we describe the baseline screening and triage process implemented for the Alberta Cancer Exercise (ACE) hybrid effectiveness-implementation study and share findings that highlight the multifaceted complexity of the process and the direct role of the clinical exercise physiologist (CEP). Methods: ACE was a hybrid effectiveness-implementation study examining the benefit of 12-week cancer-specific community-based exercise program. The ACE screening process was developed by integrating evidence-based guidelines with oncology rehabilitation expertise to ensure safe and standardized participation across cancer populations. The screening process involved four steps: (1) a pre-screen for high-risk cancers, (2) completion of...

Exercise is increasingly recognized by patients, clinicians, and allied health professionals globally as an important component of cancer care. In this paper, we provide a viewpoint on developments in exercise oncology over the past 4 decades leading up to the creation of the International Society of Exercise Oncology (ISEO). We briefly review research in adult and pediatric cancers from early foundation studies to larger randomized controlled trials published in mainstream oncology journals alongside critical work undertaken in exercise and cancer biological mechanisms. We also discuss potential strengths, weaknesses, opportunities, and threats facing ISEO in becoming a global forum for exercise oncology. Building on the foundational work undertaken over the past 4 decades by researchers, clinicians, and practitioners, ISEO provides an opportunity to support research, leverage collaborations and partnerships, facilitate education and training, increase awareness of exercise oncology, ...

Exercise oncology is a multidisciplinary field that encompasses research across the translational continuum. Some of the major disciplines contributing to the field include biology, immunology, physiology, psychology, behavioral science, epidemiology, and clinical oncology. Here, we provide a brief overview of the field under the headings of preclinical studies, observational studies, interventional outcome studies, interventional behavioral studies, dissemination and implementation studies, and childhood cancer studies. Preclinical studies have generally demonstrated that exercise can reduce tumor growth, primarily by modulating the tumor microenvironment. Observational studies have generally demonstrated that higher postdiagnosis exercise is associated with lower rates of mortality, however, most studies have not considered the combination and sequencing of exercise with other cancer treatments. Interventional outcome studies have consistently demonstrated strong evidence that aerobi...

Abstract Numerous exercise oncology trials have been completed, greatly informing exercise recommendations for patients with cancer. Exercise medicine can be administered in various types, doses and schedules at various timepoints. Advancing precision exercise medicine requires understanding of how the effects of different exercise interventions vary by characteristics of individual patients. The Predicting OptimaL cAncer RehabilItation and Supportive care (POLARIS) study provides an international infrastructure and shared database to perform pooled analyses of individual patient data (IPD) from multiple randomised controlled trials. This commentary aims to highlight the value of pooled IPD analyses, summarize key findings from published pooled IPD analyses on the effects of physical exercise on various outcomes, and provide guidance to advance precision exercise medicine for patients with cancer. POLARIS currently includes IPD from 52 exercise trials. Findings to date indicate that ex..."
"T Arndt","","Professor","Computer Science","https://openalex.org/A5055204464",""
"François R. Herrmann","","Professor","Computer Science","https://openalex.org/A5034367999","Background/Objectives: The prevalence of diabetes in very old people is rising sharply worldwide, due not only to obesity, nutritional and sedentary lifestyles, but also to aging per se. Diabetes is associated with a higher incidence of sarcopenia, malnutrition and physical disabilities. However, many age-specific issues in the clinical management of very old diabetic patients remain unstudied. Methods: This is a case–control prospective study including 162 very old hospitalized diabetic patients and 301 controls. We explored the impact of diabetes on the prevalence of sarcopenia according to the EWGSOP2 criteria, using Jamar handgrip to assess muscle strength, BIA-derived fat-free mass index to assess muscle mass, and the timed up and go test to assess physical performance. We also explored factors associated with sarcopenia in both groups in multiple logistic analysis. Results: Mean age was 84.8 ± 6.0 years. We found a prevalence of sarcopenia of 8.0% and 16.7% in the diabetic and th...

Introduction Hippocampal volume loss occurs physiologically with age, but an accelerated rate of volume loss is linked to neurodegenerative diseases. While evidence suggests that cross-sectional study designs tend to underestimate hippocampal atrophy rates compared to longitudinal approaches, few studies have directly examined the relationship between these two methods in the context of brain aging. This study aims to investigate the association between baseline hippocampal z-scores and hippocampal volume loss over time in a cohort of healthy older adults. Methods 182 healthy elderly subjects (mean age: 73.4 ± 3.5 years) who underwent structural Magnetic resonance imaging (MRI) at two timepoints (mean time between the scans 4.8 ± 1.0 years) were included. A subset of participants ( n = 103) also completed Positron emission tomography (PET) amyloid imaging. Hippocampal volumes were measured at baseline and follow-up using FreeSurfer (v7.1.1). Baseline volumes were adjusted for age and i...

People tend to consider others' perspective when judging their own (altercentric interference, AI) or other (egocentric interference, EI) divergent views. Borderline (BDL) and antisocial personalities are associated with significant changes in EI and AI. Combining the dot perspective-taking task with high-density EEG recordings, our study explores the correlations between EI and AI in cases with BDL diagnosis and court-ordered measures (BDL-COM; n = 14) compared to age-matched healthy controls (n = 24). In Inconsistent trials, controls displayed significant activation of brain generators, which was absent in BDL-COM patients. For the Self-Inconsistent stimuli (altercentric bias), controls showed increased activity in the left superior frontal gyrus between 58 and 74 ms and the left inferior frontal gyrus between 279 and 303 ms. Similar differences were observed for Other-Inconsistent stimuli (egocentric bias) in the precentral gyri and inferior frontal gyrus between 274 and 296 ms. The...

Obesity, defined by a body mass index (BMI) ≥ 30 kg/m2, is associated with higher mortality in the general population but shows a complex relationship with chronic diseases and critically illness. The aim of this study was to determine whether BMI predicted 30-day mortality (primary outcome) and intensive care unit (ICU) and hospital lengths of stay (LOS) (secondary outcome) in critically ill patients. This retrospective monocentric study encompassed adult patients admitted to the ICU of the Geneva University Hospitals between January 2010 and December 2022. They were categorized according to their BMI as underweight (<18.5 kg/m2), normal weight (18.5-24.9 kg/m2, reference category), overweight (25.0-29.9 kg/m2), obesity class I (30.0-34.9 kg/m2), class II (35.0-39.9) and class III (≥40.0 kg/m2). The association between BMI and outcomes was assessed by multivariate Cox Lasso (Least absolute shrinkage and selection operator) or linear Lasso regression models, adjusted for age, sex, Simp...

Low skeletal mass, often present at hospital admission, has been associated with poor prognoses. To explore the association between computed tomography (CT)-derived skeletal muscle mass at the lumbar level and short- and long-term mortality in critically ill patients. Following PRISMA 2020 guidelines, we included studies on critically ill adults (≥ 18 years) hospitalized in intensive care units (ICU) that measured CT-derived skeletal muscle mass at the lumbar vertebral level within ± 7 days of ICU admission. The primary outcome was mortality, categorized as short-term (including ICU, hospital, 28- and 30-day mortality) and long-term (> 30 days) mortality. MEDLINE and Embase databases were searched without date restrictions. Study screening was performed using Rayyan, data extraction was guided by a custom-designed tool, and quality assessment was performed using the JBI Cohort Study Checklist. A meta-analysis was conducted, focusing on studies that reported short- and long-term mortali..."
"Peter Svensson","","Professor","Computer Science","https://openalex.org/A5043756816","In this narrative review, the authors aimed to provide a focused overview, grounded in scientific literature, of the most common primary and secondary headaches frequently observed in patients with orofacial pain as well as orofacial conditions that may mimic primary headache disorders. In addition, they highlighted the clinically significant overlap between headaches and temporomandibular disorders (TMDs). Information was sourced from the International Classification of Headache Disorders, Third Edition, for headache diagnoses and from the International Classification of Orofacial Pain for orofacial pain diagnoses. Management guidelines were from the European Academy of Neurology. Data on the overlap between headache disorders and TMDs were drawn from a systematic review and observational studies. The authors provided a concise, practical, clinical guide for identifying and managing primary and secondary headaches commonly encountered in dental practice, which is grounded in establish...

Abstract The aim of this long-term follow-up study was to investigate the relationship between bite force, bruxism, and fractures of teeth and veneer porcelain of fixed dental prostheses. Patients previously assessed as probable bruxers (n = 30) and non-bruxers (n = 21), all rehabilitated with dental implant-supported restorations, underwent a clinical examination and measurement of maximum bite force. A univariate general linear model was used to compare regression lines showing the relationship between fractures and bite force. Bruxers had significantly higher maximum bite force ( p = 0.023) and higher proportion of tooth/veneer porcelain fractures per total number of tooth/prosthetic units ( p = 0.045). There was no significant difference in the relationship between frequency of tooth/veneer porcelain fractures and maximum bite force between probable bruxers and non-bruxers ( p = 0.054). However, there was a significant difference between probable bruxers and non-bruxers when the pe...

Bruxism is receiving increasing attention from both clinicians and researchers over the past decades. Recently, it has become clear that some aspects of the currently proposed, expert-driven bruxism definitions raise questions and cause confusion among clinicians, researchers, educators and patients. The aim of this report is threefold: (1) to provide the reader with a glossary of the existing definitions, (2) to discuss frequently asked questions regarding these definitions and (3) to suggest a road map for the next steps to be taken towards a better understanding of bruxism. A closed (invitation-only) full-day workshop at the 2024 General Session & Exhibition of the International Association for Dental, Oral and Craniofacial Research (IADR) convened international bruxism experts to discuss the current definitions. Insights from these discussions were compiled, analysed and summarised. The present report provides a glossary of the constituent terms of the currently proposed definition..."
"R. Poettgen","","Professor","Computer Science","https://openalex.org/A5107839818","Baryon number violation (BNV) in R-parity violating (RPV) supersymmetry is studied with a focus on Delta B = 2 processes which allow neutron–anti-neutron oscillations. Simplified RPV-SUSY models, including only the relevant superpartners and couplings, are considered. Constraints from flavour physics, searches at the Large Hadron Collider and searches at dedicated BNV experiments are quantified for the various scenarios at the TeV scale. It is also shown that a proposed neutron oscillation experiment at the European Spallation Source has a sensitivity to a mass scale for new physics that goes beyond all other experiments and up to the PeV scale for certain regions of parameter space.

Baryon number violation (BNV) in R-parity violating (RPV) supersymmetry is studied with a focus on ΔB = 2 processes which allow neutron-anti-neutron (n-n) oscillations. Simplified RPV-SUSY models, including only the relevant superpartners and couplings, are considered. Constraints from flavour physics, searches at the Large Hadron Collider and searches at dedicated BNV experiments are quantified for the various scenarios at the TeV scale. It is also shown that a proposed n-n experiment at the European Spallation Source has a sensitivity to a mass scale for new physics that goes beyond all other experiments and up to the PeV scale for certain regions of parameter space.

For the next run of the LHC, the ATLAS Level-1 trigger system will include topological information on trigger objects from the calorimeters and muon detectors. In order to supply coarse grained muon topological information, the existing MUCTPI (Muon-to-Central-Trigger-Processor Interface) system has been upgraded. The MIOCT (Muon Octant) module firmware has been then modified to extract, encode and send topological information through the existing MUCTPI electrical trigger outputs. The topological information from the muon detectors will be sent to the Level-1 Topological Trigger Processor (L1Topo) through the MUCTPI-to-Level-1-Topological-Processor (MuCTPiToTopo) interface. Examples of physics searches involving muons are: search for Lepton Flavour Violation, Bs-physics, Beyond the Standard Model (BSM) physics and others. This paper describes the modifications to the MUCTPI and its integration with the full trigger chain.

An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures.

An entry from the Inorganic Crystal Structure Database, the world’s repository for inorganic crystal structures. The entry contains experimental data from a crystal diffraction study. The deposited dataset for this entry is freely available from the joint CCDC and FIZ Karlsruhe Access Structures service and typically includes 3D coordinates, cell parameters, space group, experimental conditions and quality measures."
"Peter Lindqvist","","Professor","Computer Science","https://openalex.org/A5029991391","<title>Abstract</title> Magnetic reconnection, a fundamental energy conversion process, underpins a multitude of eruptive phenomena across the universe. Compared to the traditional standard magnetic reconnection with ion coupling, the recently discovered electron-only magnetic reconnection operates at strikingly diminutive spatial scales, imposing formidable observational constraints on resolving its intrinsic physical processes, most critically within its core region—near the X-line. Consequently, the intrinsic mechanism governing electron-only magnetic reconnection remains largely enigmatic. Leveraging high-resolution data from NASA’s MMS mission, we present unprecedented observations of two types of whistler waves in electron-only magnetic reconnection: right-handed whistler waves near the X-line, excited by perpendicular anisotropy electron distributions via second order cyclotron resonance, contrasted by left-handed whistler waves in the outflow jet. The outflow-associated whistle...

Abstract Dipolarization events with inductive, radial electric fields are investigated with multi‐spacecraft analysis techniques. Observations by Magnetospheric Multiscale with separations around ion scales are used to study spatial and temporal variations of these events in the inner magnetosphere. force, magnetic pressure force, and tension force are compared based on the Taylor expansion method, which includes the curlometer technique. The magnetic pressure force, possibly related to energetic particles and magnetic flux transported from the magnetotail, tends to contribute to the force for the background components near the equator, while the tension force, related to Alfvén waves, contributes to the fluctuating components or outside the equator. The scale length is thousands of km for the background components, probably related to meso‐scale structures, while that length is tens to a thousand km for fluctuating components. The small‐scale fluctuations would be related to particle ...

Abstract Wave‐particle interactions are essential for energy transport in the magnetosphere. In this study, we investigated an event during which electrons interact simultaneously with waves in different scales, using data from the Magnetospheric Multiscale mission. At the macroscale ( km), drift resonance between ultra‐low frequency (ULF) waves and 70–300 keV electrons is observed. At the microscale ( km), lower‐band chorus waves and electron cyclotron harmonic (ECH) waves are alternately generated, showing signatures of modulation by ULF waves. We found that compressional ULF waves affect the temperature anisotropy of 1–10 keV electrons, thereby periodically exciting chorus waves. Through linear instability analysis, we propose that ULF waves modulate ECH wave emissions by regulating the gradient of electron phase space density at the edge of the loss cone. Our results enhance the understanding of cross‐scale wave‐particle interactions, highlighting their importance in magnetospheric...

<title>Abstract</title> Chorus waves are electromagnetic emissions widely occurring in planetary (e.g., Earth, Mars, and Jupiter) environments and can cause hazardous space weather effects which threaten human safety and man-made electromagnetic devices. For decades, chorus waves have been intensively investigated, both experimentally and theoretically, in the context of planetary dipolar fields. However, the underlying electron dynamics which governs the wave generation and evolution, has not been well diagnosed by <italic>in situ</italic> observations, due to the limited capacity of previous spacecraft missions in geospace. Here we report unexpected observations of repetitive, falling-tone chorus waves in the terrestrial neutral sheet. Using unprecedently high-resolution data from NASA’s MMS mission, we present ultrafast measurements of electron dynamics and electric current within the waves, finding energy transfer from local electrons to the waves and development of electron hill i...

Abstract High‐speed electron flows (HSEFs) play a crucial role in the energy dissipation and conversion processes within the terrestrial magnetosphere and can drive various types of plasma waves and instabilities, affecting the electron‐scale dynamics. The existence, spatial distribution, and general properties of HSEFs in the Earth magnetotail are still unknown. In this study, we conduct a comprehensive survey of HSEFs in the Earth magnetotail, utilizing NASA's Magnetospheric Multiscale (MMS) mission observations from 2017 to 2021. A total of 642 events characterized by electron bulk speeds exceeding 5,000 km/s are identified. The main statistical properties are: (a) The duration of almost all HSEFs are less than 4 s, and the average duration is 0.74 s. (b) HSEFs exhibit a strong dawn‐dusk (30%–70%) asymmetry. (c) 39.6%, 29.0%, and 31.4% of the events are located in the plasma sheet, plasma sheet boundary layer (PSBL), and lobe region, respectively. (d) In the plasma sheet, HSEFs have..."
"Sverker Sörlin","","Professor","Computer Science","https://openalex.org/A5015853724","Abstract This article presents a new way of understanding Global Environmental Governance (GEG), historically and functionally. We outline a revised analytical framing, which connects the post-WWII moment of early globalizing conservation with the intensifying attempts to govern the human-earth relationship through an ever-growing assemblage of governable environmental objects and their quantifiable indicators as proxies . Our argument is as follows: (1) GEG has followed a trajectory of dispersal of actors, institutions, conceptual tools and responsibilities from the micro- and local scales to the planetary. We analyze how these trajectories unfold in three essential domains: Earth System science, sovereignty, and neoliberalization. (2) GEG is performative . The governance itself has created the dynamic environmental objects under governance. (3) In this way, GEG has normalized the environment as a policy object.

Global Environmental Governance (GEG) has been a growing phenomenon since the middle of the 20th century, although the concept itself and its acronym are more recent and were in fact rarely used before 2000. The early interest in GEG was much preoccupied with environmental diplomacy and international legal agreements from the Stockholm UN conference 1972 onwards. The addition of the ‘global’ reflected the general rise in awareness of the significance of globalization since the 1980s. The further growth, and the transformation of GEG, in earnest since the Millennium has been increasingly marked by yet another category, ‘the planetary’, mirroring the increasing influence of Earth System Science on environmental and climate discourse. This has affected both GEG and ‘the environment’ itself. The conceptual shifts, including the rising interest in the Anthropocene, reflected profound changes in the human-Earth relationship. To analyze these shifts, which is the aim of this paper, I will use..."
"M. Wolke","","Professor","Computer Science","https://openalex.org/A5110067108","The European Spallation Source, currently under construction in Lund, Sweden, is a multidisciplinary international laboratory. Once completed to full specifications, it will operate the world’s most powerful pulsed neutron source. Supported by a 3 million Euro Research and Innovation Action within the EU Horizon 2020 program, a design study (HighNESS) has been completed to develop a second neutron source located below the spallation target. Compared to the first source, designed for high cold and thermal brightness, the new source has been optimized to deliver higher intensity, and a shift to longer wavelengths in the spectral regions of cold (CN, 2–20 Å), very cold (VCN, 10–120 Å), and ultracold (UCN, >500 Å) neutrons. The second source comprises a large liquid deuterium moderator designed to produce CN and support secondary VCN and UCN sources. Various options have been explored in the proposed designs, aiming for world-leading performance in neutronics. These designs will enable the...

A key aim of the HighNESS project for the European Spallation Source is to enable cutting-edge particle physics experiments. This volume presents a conceptual design report for the NNBAR experiment. NNBAR would exploit a new cold lower moderator to make the first search in over thirty years for free neutrons converting to anti-neutrons. The observation of such a baryon-number-violating signature would be of fundamental significance and tackle open questions in modern physics, including the origin of the matter-antimatter asymmetry. This report shows the design of the beamline, supermirror focusing system, magnetic and radiation shielding, and anti-neutron detector necessary for the experiment. A range of simulation programs are employed to quantify the performance of the experiment and show how background can be suppressed. For a search with full background suppression, a sensitivity improvement of three orders of magnitude is expected, as compared with the previous search. Civil engin...

Tests of the T, CP and CPT symmetries in the neutral kaon system are performed by the direct comparison of the probabilities of a kaon transition process to its symmetry-conjugate. The exchange of in and out states required for a genuine test involving an antiunitary transformation implied by time-reversal is implemented exploiting the entanglement of K0K‾0 pairs produced at a ϕ-factory. A data sample collected by the KLOE experiment at DAΦNE corresponding to an integrated luminosity of about 1.7 fb−1 is analysed to study the Δt distributions of the ϕ→KSKL→π+π−π±e∓ν and ϕ→KSKL→π±e∓ν3π0 processes, with Δt the difference of the kaon decay times. A comparison of the measured Δt distributions in the asymptotic region Δt≫τS allows to test for the first time T and CPT symmetries in kaon transitions with a precision of few percent, and to observe CP violation with this novel method.

The NNBAR experiment for the European Spallation Source will search for free neutrons converting to antineutrons with an expected sensitivity improvement of three orders of magnitude compared to the last such search. This paper describes both the simulations of a key component for the experiment, the neutron optical reflector and the expected gains in sensitivity.

A bstract The ratio $$ \mathcal{R} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mi>R</mml:mi> </mml:math> = Γ( K S → πeν ) / Γ( K S → π + π − ) has been measured with a sample of 300 million K S mesons produced in ϕ → K L K S decays recorded by the KLOE experiment at the DAΦNE e + e − collider. K S → πeν events are selected by a boosted decision tree built with kinematic variables and time-of-flight measurements. Data control samples of K L → πeν decays are used to evaluate signal selection efficiencies. With 49647 ± 316 signal events we measure $$ \mathcal{R} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mi>R</mml:mi> </mml:math> = (1 . 0421 ± 0 . 0066 stat ± 0 . 0075 syst ) × 10 − 3 . The combination with our previous measurement gives $$ \mathcal{R} $$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:mi>R</mml:mi> </mml:math> = (1 . 0338 ± 0 . 0054 stat ± 0 . 0064 syst ) × 10 − 3 . From this value we derive the branching fractio..."
"M. Ughetto","","Professor","Computer Science","https://openalex.org/A5103120392","The limited amount of data available renders it challenging to characterize which biological processes are relevant to a rare disease. Hence, there is a need to leverage the knowledge of disease pathogenesis and treatment from the wider disease landscape to understand rare disease mechanisms. Furthermore, it is well understood that rare disease discoveries can inform the our knowledge of common diseases. In this paper, we introduce Dis2Vec (Disease to Vector), a new representation learning method for characterizing diseases with a focus on learning the underlying biological mechanisms, which is a step toward developing a foundation model for disease-association learning. Dis2Vec is trained on human genetic evidence and observed symptoms, and then evaluated through cross-modal transfer-learning scenarios based on a proposed drug association learning benchmark with drug targets (positive controls) and Orphanet Rare Disease Ontology (negative controls). Finally, we argue that clustering d...

Gold nanoparticles are used in a range of applications, but their properties depend on their shape, size, and polydispersity. A quick, easy, and accurate characterization of the particles is therefore of high importance, especially in flow synthesis settings where continuous monitoring of the characteristics is desired. Our hypothesis was that convolutional neural networks can be used to extract detailed information about structural parameters of gold nanoparticles from their UV–vis spectra, and we have shown that this is possible by predicting size distributions from in silico UV–vis spectra for colloidal gold with high accuracy. Here this was done for both spherical and rod-shaped gold nanoparticles. We also show that the addition of noise makes the prediction of diameter polydispersity more challenging, but the average diameter, and for rods also aspect ratio distribution, can be accurately predicted even with the highest evaluated level of noise. The model structure is promising an...

ABSTRACT Machine learning applications for the drug discovery pipeline have exponentially increased in the last few years. An example of these applications is the biological Knowledge Graph. These graphs represent biological entities and the relations between them based on existing knowledge. Graph machine learning models such as Graph Neural Networks can be applied on top of knowledge graphs to support the development of novel therapeutics. Nevertheless, Graph Neural Networks present an improved performance at the expense of complexity, becoming difficult to explain their decisions. State-of-the-art explanation algorithms for Graph Neural Networks focus on determining the most relevant subgraphs involved in their decision-making while considering graph elements (nodes and edges) as independent entities and ignoring any communities these graphs could present. We explore in this work the idea that graph community structure in biological Knowledge Graphs could provide a better grasp of t...

In this paper, we introduce ChemicalX, a PyTorch-based deep learning library designed for providing a range of state of the art models to solve the drug pair scoring task. The primary objective of the library is to make deep drug pair scoring models accessible to machine learning researchers and practitioners in a streamlined framework. The design of ChemicalX reuses existing high level model training utilities, geometric deep learning, and deep chemistry layers from the PyTorch ecosystem. Our system provides neural network layers, custom pair scoring architectures, data loaders, and batch iterators for end users. We showcase these features with example code snippets and case studies to highlight the characteristics of ChemicalX. A range of experiments on real world drug-drug interaction, polypharmacy side effect, and combination synergy prediction tasks demonstrate that the models available in ChemicalX are effective at solving the pair scoring task. Finally, we show that ChemicalX co..."
"Maria Strömme","","Professor","Computer Science","https://openalex.org/A5037359873","Covalent organic framework (COF) membranes hold significant promise for applications in separation, catalysis, and energy conversion; however, their industrial adoption has been hindered by the lack of scalable and efficient fabrication methods. Here, we present a fast, versatile, and broadly applicable strategy for fabricating free-standing and flexible COF membranes by casting precursor suspensions, followed by heat treatment under controlled humidity. This approach enables the fabrication of COF membranes with lateral dimensions up to several square decimeters and thicknesses that are tunable down to submicron levels within 1 h. It demonstrates remarkable versatility for producing a family of ketoenamine-linked COF membranes through the condensation of 1,3,5-triformylphloroglucinol with various amine monomers differing in length, side groups, and geometry. The resulting crack-free COF membranes exhibit high mechanical strength, with ultimate tensile strength up to 60 MPa and Young's...

Lignin is an aromatic biomacromolecule with many promising properties that can be beneficial to polymer blends. The main objective of this work was to investigate the processability, compatibility, and recyclability of lignin blends with poly(lactic acid). Two different commercial kraft lignins and a phenolated organosolv lignin were blended with poly(lactic acid) at various weight percentages, targeting high lignin content (30, 50, and 70 wt %). Obtained blends were used in additive manufacturing via fused deposition modeling. All obtained materials were thoroughly characterized by tensile tests, thermogravimetric analysis, differential scanning calorimetry, and 31P NMR. The recyclability of the polymer blend materials was evaluated by re-extruding them up to four times, and their printability was also assessed. The results showed that the material retained its mechanical properties relatively well for up to three cycles after which its tensile strength decreased by 30%. Phenolated or..."
"B. Lund-Jensen","","Professor","Computer Science","https://openalex.org/A5072437032","A summary is presented of ATLAS searches for gluinos and first- and second-generation squarks in final states containing jets and missing transverse momentum, with or without leptons or b-jets, in the root s = 8 TeV data set collected at the Large Hadron Collider in 2012. This paper reports the results of new interpretations and statistical combinations of previously published analyses, as well as a new analysis. Since no significant excess of events over the Standard Model expectation is observed, the data are used to set limits in a variety of models. In all the considered simplified models that assume R-parity conservation, the limit on the gluino mass exceeds 1150 GeV at 95% confidence level, for an LSP mass smaller than 100 GeV. Furthermore, exclusion limits are set for left-handed squarks in a phenomenological MSSM model, a minimal Supergravity/Constrained MSSM model, R-parity-violation scenarios, a minimal gauge-mediated supersymmetry breaking model, a natural gauge mediation mo...

The Sileye3/Alteino experiment is devoted to the investigation of the light flash phenomenon and particle composition of the cosmic ray spectrum inside the ISS. The particle detector is a silicon telescope consisting of eight planes, each divided into 32 strips. Data acquisition was initiated in 2002 in the Russian Pirs module. The data on nuclei from C to Fe in the energy range above about 60 MeV/n presented here were taken as part of the ESA Altcriss project [ 1] from late 2005 through 2007. Here we report on LET, from different locations and orientations, in both the Pirs and Zvezda modules. Taking solar modulation into account the results are in agreement with ALTEA measurements from USLab [ 2]. To convert the energy deposition in Si to the equivalent in water, the logarithmic relation between LET in Si and water adopted from [ 3]. In Fig. 1, the LET spectra in water for Alteino and ALTEA are compared with DOSTEL spectrum from 2001 [ 4], and we see a good overall agreement. We are ...

Rapidity gap cross sections measured with the ATLAS detector in pp collisions at sqrt(s) = 7 TeV

Search for excited leptons in proton-proton collisions at sqrt(s) = 7 TeV with the ATLAS detector

The ATLAS detector has been built to study the reactions produced by the Large Hadron Collider (LHC). ATLAS includes a system of liquid argon calorimeters for energy measurements. The electronics for amplifying, shaping, sampling, pipelining, and digitizing the calorimeter signals is implemented on a set of front-end electronic boards. The front-end boards are installed in crates mounted between the calorimeters, where they will be subjected to significant levels of radiation during LHC operation. As a result, all components used on the front-end boards had to be subjected to an extensive set of radiation qualification tests. This paper describes radiation-tolerant designs, radiation testing, and radiation qualification of the front-end readout system for the ATLAS liquid argon calorimeters."
"Ingmar Skoog","","Professor","Computer Science","https://openalex.org/A5058286274","Glial fibrillary acidic protein (GFAP) is a well-established biomarker of astrocytic activation associated with neurodegenerative diseases, neuroinflammatory disorders, and traumatic brain injury. With increasing interest in blood-based biomarkers, the need for analytically validated assays and reliable reference intervals is critical for routine clinical implementation. This study aimed to analytically validate the MSD S-Plex® GFAP immunoassay for plasma and to establish age-stratified reference intervals in an apparently healthy population. This study was conducted in two phases. First, key analytical validation parameters - including repeatability, intermediate precision, measurement range, interferences, and sample stability - were evaluated following Clinical and Laboratory Standards Institute (CLSI) and published protocol guidelines. Second, reference intervals were derived from 579 apparently healthy individuals aged 17-91 years using a right-sided non-parametric percentile meth...

Abstract Background The EAT-Lancet Commission has proposed a global reference diet aimed at promoting both human health and environmental sustainability. While adherence to this dietary pattern has been associated with reduced risks of chronic disease and lower environmental impact, concerns remain about its ability to meet nutritional requirements - particularly among older adults. The aim was to explore the association between adherence to the EAT-Lancet diet and nutrient intake and adequacy among 70-year-old adults in Gothenburg, Sweden. Methods This cross-sectional study included 861 participants from the Swedish population-based Gothenburg H70 Birth Cohort Study (mean age 70.5 years, 55% women). Dietary intake was assessed using a validated diet history interview, and adherence to the EAT-Lancet diet was scored based on 14 food components. Nutrient intake was evaluated against age- and sex-specific recommended intake (RI) levels. Cardiometabolic risk markers and biomarkers of nutr...

Abstract Background It is largely unknown how alcohol use affects the risk of Alzheimer`s disease (AD). Therefore, studies on the influence of alcohol use on cerebrospinal fluid (CSF) biomarkers for the earliest preclinical phase of AD are needed. Methods This was a cross-sectional cohort study. The sample ( n = 301) was derived from the 2014–2016 examinations of the Gothenburg H70 Birth Cohort Studies. The study cohort consisted of 301 70-year-old women and men, where of 246 cognitively unimpaired and 55 with mild cognitive deficits. Information on alcohol consumption (g/week and type of alcohol) was collected and CSF amyloid-β 1−42 (Aβ42), total-tau (T-tau), tau phosphorylated at threonine 181 (P-tau181), neurofilament light protein (NfL) and neurogranin (Ng) were measured. We tested the association between the CSF biomarkers and alcohol consumption types using correlation and linear regression, adjusting for possible confounders when necessary according to the performed sensitivity ..."
"J. U. Mjörnmark","","Professor","Computer Science","https://openalex.org/A5107856097",""
"R. Brenner","","Professor","Computer Science","https://openalex.org/A5107946798","•The 12-gene Oncotype DX Colon Recurrence Score® assay quantifies recurrence risk in mismatch repair-proficient stage II/III CC.•This real-world clinical practice study found that this assay provides independent prognostic information in these patients.•The study found that CT seems to confer clinical benefit in patients with high risk according to the assay.•Our results further validate the assay in such patients and support its use for guiding adjuvant treatment decisions. BackgroundThe 12-gene Oncotype DX Colon Recurrence Score® result quantifies the recurrence risk in stage II/III colon cancer (CC). This real-world study investigated stage II CC patients whose treatment decisions incorporated the Recurrence Score® (RS) result.Materials and methodsThis retrospective analysis of a prospectively designed cohort included all stage II, mismatch repair-proficient CC patients who underwent 12-gene testing through Clalit between January 2011 and December 2016 and had available data with a ...

In July 2018 an optimization run for the proposed charm cross section measurement for SHiP was performed at the CERN SPS. A heavy, moving target instrumented with nuclear emulsion films followed by a silicon pixel tracker was installed in front of the Goliath magnet at the H4 proton beamline. Behind the magnet, scintillating-fibre, drift-tube and RPC detectors were placed. The purpose of this run was to validate the measurement's feasibility, to develop the required analysis tools and fine-tune the detector layout. In this paper, we present the track reconstruction in the pixel tracker and the track matching with the moving emulsion detector. The pixel detector performed as expected and it is shown that, after proper alignment, a vertex matching rate of 87 % is achieved.

Abstract The operation at the Z-pole of the FCC-ee machine will deliver the highest possible instantaneous luminosities with the goal of collecting the largest Z boson datasets (Tera-Z), and enable a programme of standard model physics studies with unprecedented precision. The data acquisition and trigger systems of the FCC-ee experiments must be designed to be as unbiased and robust as possible, with the goal of containing the systematic uncertainties associated with these datasets at the smallest possible level, in order to not compromise the extremely small statistical uncertainties. In designing these experiments, we are confronted by questions on detector read-out speeds with an extremely tight material and power budget, trigger systems with a first hardware level or implemented exclusively on software, impact of background sources on event sizes, ultimate precision luminosity monitoring (to the $$10^{-5}$$ <mml:math xmlns:mml=""http://www.w3.org/1998/Math/MathML""> <mml:msup> <mml:...

The operation at the Z-pole of the FCC-ee machine will deliver the highest possible instantaneous luminosities with the goal of collecting the largest Z boson datasets (Tera-Z), and enable a programme of Standard Model physics studies with unprecedented precision. The data acquisition and trigger systems of the FCC-ee experiments must be designed to be as unbiased and robust as possible, with the goal of containing the systematic uncertainties associated with these datasets at the smallest possible level, in order to not compromise the extremely small statistical uncertainties. In designing these experiments, we are confronted by questions on detector readout speeds with an extremely tight material and power budget, trigger systems with a first hardware level or implemented exclusively on software, impact of background sources on event sizes, ultimate precision luminosity monitoring (to the $10^{-5} - 10^{-4}$ level), and sensitivity to a broad range of non-conventional exotic signatur...

Dark photons are hypothetical massive vector particles that could mix with ordinary photons. The simplest theoretical model is fully characterised by only two parameters: the mass of the dark photon m(gamma)D and its mixing parameter with the photon, epsilon. The sensitivity of the SHiP detector is reviewed for dark photons in the mass range between 0.002 and 10 GeV. Different productionmechanisms are simulated, with the dark photons decaying to pairs of visible fermions, including both leptons and quarks. Exclusion contours are presented and compared with those of past experiments. The SHiP detector is expected to have a unique sensitivity for m. D ranging between 0.8 and 3.3(-0.5)(+0.2) GeV, and epsilon(2) ranging between 10(-11) and 10(-17)."
"Nancy C. Andreasen","","Professor","Computer Science","https://openalex.org/A5091277767","Abstract Background Schizophrenia is a severe neuropsychiatric disorder accompanied by debilitating cognitive and psychosocial impairments over the course of the disease. As disease trajectories exhibit considerable inter-individual heterogeneity, early clinical and neurobiological predictors of long-term outcome are desirable for personalized treatment and care strategies. Methods In a naturalistic longitudinal approach, 381 schizophrenia patients from the Iowa Lon-gitudinal Study (ILS) cohort underwent an extensive characterization, including repeated magnetic resonance imaging (MRI) scans, over a mean surveillance period of 11.07 years. We explored whether pre-diagnostic markers, clinical markers at the first psychotic episode, or magnetic resonance imaging (MRI) measures at the onset of the disease were predictive of relapse or remission of specific symptom patterns later in life. Results We identified a set of clinical parameters - namely premorbid adjustment during adolescence, s...

16p11.2 copy number variations have been associated with neurodevelopmental disorders. Human induced pluripotent stem cells were generated from fibroblasts obtained from a patient diagnosed with schizophrenia with a 16p11.2 deletion. The generated cell line was further validated for its pluripotency and potential to differentiate into the three germ layers."
"Math Bollen","","Professor","Computer Science","https://openalex.org/A5046391029","Transmission network-expansion planning research requires reproducibility of results and comparability of research from various sources. This paper presents a process for modifying a published electricity network model so that the model can be used for exploration of transmission expansion planning problems for different load and generation profiles. Nodal voltages and branch currents are kept within performance limits by following the applicable planning codes, with reinforcements selected based on a defined strategy to achieve compliance with the applicable standards. The process can be applied to any published model and any set of planning standards to result in a base model that is suitably up to date and realistic for transmission network-expansion planning research. A case study is presented, whereby the process is followed for the “Nordic-32”—a popular reference model based on the Swedish transmission network of the 1980s—with the result being a reproducible and updatable model ...

Digital substation technology adhering to the IEC 61850 standard has provided several opportunities and flexibility for the rapid growth and complexity of the present and future electrical grid. The communication infrastructure allows complete interoperability between legacy and modern devices. The emergence of 5G wireless communication and its utilization in substation operation presents significant advantages in terms of cost and scalability, while also introducing challenges. This paper identifies research gaps in the literature and offers valuable insights for future analysis by providing a simulation study using an empirical latency dataset of a 5G network to illustrate three aspects of substation operational challenges: coordination of protection schemes, sequential reception of packet data streams, and time synchronization processes. The findings show a mean latency of 8.5 ms for the 5G network, which is significantly higher than that of a wired Ethernet network. The results als...

This paper examines the random nature of interharmonics generated by power converters connected to sustainable energy sources and loads, such as wind turbines, photovoltaic (PV) panels, and electric vehicles (EVs). Current research often overlooks the stochastic behavior of interharmonics and their impact on power system reliability and resilience, leading to gaps in effective modeling and mitigation strategies. Thus, this study examines a low-voltage installation with a PV panel, an EV and a microwave operating simultaneously, providing practical insights into real-world scenarios of interharmonic related disruptions and solutions for enhancing the reliability and resilience of sustainable energy grids. By leveraging real-time measurements of interharmonics, suitable probability distribution functions (PDFs) are initialized to develop a probabilistic model using Monte Carlo simulation. This enables the derivation of a time-domain aggregation model of interharmonics from multiple sourc...

Alternative solutions, instead of building new transmission lines, are needed to enable the fast electrification of sectors such as industry, transportation, and heating. This includes smart-grid technology allowing for an increase in hosting capacity without the need for building new transmission lines. The increase in hosting capacity beyond the firm hosting capacity is, with many of the schemes, related to the ability of the installation to tolerate curtailment. The paper gives an overview of different smart-grid schemes and how they potentially increase the hosting capacity. The connection of a large electrolyser installation for hydrogen production is used as an illustrative example introducing some of the issues."
