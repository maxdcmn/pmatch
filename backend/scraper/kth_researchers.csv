"name","email","title","research_area","profile_url","abstracts"
"Cyrille Artho","artho@kth.se","Associate professor","Cyrille Artho, Works for: Division of Theoretical Computer Science, E-mail: artho@kth.se, Telephone: +46 8 790 68 61, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/artho?l=en","[""Smart contracts are computer programs running on blockchains to implement Decentralized Applications. The absence of contract specifications hinders routine tasks, such as contract understanding and testing. In this work, we propose a specification mining approach to infer contract specifications from past transaction histories. Our approach derives high-level behavioral automata of function invocations, accompanied by program invariants statistically inferred from the transaction histories. We implemented our approach as tool SMCON and evaluated it on eleven well-studied Azure benchmark smart contracts and six popular real-world DApp smart contracts. The experiments show that SMCON mines reasonably accurate specifications that can be used to enhance symbolic analysis of smart contracts achieving higher code coverage and up to 56 % speedup, and facilitate DApp developers in maintaining high-quality documentation and test suites."", ""Smart contracts are computer programs running on blockchains to implement Decentralized Applications. The absence of contract specifications hinders routine tasks, such as contract understanding and testing. In this work, we propose a specification mining approach to infer contract specifications from past transaction histories. Our approach derives high-level behavioral automata of function invocations, accompanied by program invariants statistically inferred from the transaction histories. We implemented our approach as tool SMCON and evaluated it on eleven well-studied Azure benchmark smart contracts and six popular real-world DApp smart contracts. The experiments show that SMCON mines reasonably accurate specifications that can be used to enhance symbolic analysis of smart contracts achieving higher code coverage and up to 56 % speedup, and facilitate DApp developers in maintaining high-quality documentation and test suites.""]"
"Ricardo Baeza Yates","raby2@kth.se","Visiting professor","","https://www.kth.se/profile/raby2?l=en",""
"Musard Balliu","musard@kth.se","Associate professor","Musard Balliu, Works for: Division of Theoretical Computer Science, E-mail: musard@kth.se, Telephone: +46 8 790 68 22, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/musard?l=en","[""Securing communication in large scale distributed systems is an open problem. When multiple principals exchange sensitive information over a network, security and privacy issues arise immediately. For instance, in an online auction system we may want to ensure that no bidder knows the bids of any other bidder before the auction is closed. Such systems are typically interactive/reactive and communication is mostly asynchronous, lossy or unordered. Language-based security provides language mechanisms for enforcing end-to-end security. However, with few exceptions, previous research has mainly focused on relational or synchronous models, which are generally not suitable for distributed systems."", ""Securing communication in large scale distributed systems is an open problem. When multiple principals exchange sensitive information over a network, security and privacy issues arise immediately. For instance, in an online auction system we may want to ensure that no bidder knows the bids of any other bidder before the auction is closed. Such systems are typically interactive/reactive and communication is mostly asynchronous, lossy or unordered. Language-based security provides language mechanisms for enforcing end-to-end security. However, with few exceptions, previous research has mainly focused on relational or synchronous models, which are generally not suitable for distributed systems.""]"
"Benoit Baudry","baudry@kth.se","Visiting professor","Benoit Baudry, Works for: Division of Theoretical Computer Science, E-mail: baudry@kth.se, Telephone: +46 8 790 41 10, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/baudry?l=en","[""JavaScript packages are notoriously prone to bloat, a factor that significantly impacts the performance and maintainability of web applications. While web bundlers and tree-shaking can mitigate this issue in client-side applications, state-of-the-art techniques have limitations on the detection and removal of bloat in server-side applications. In this paper, we present the first study to investigate bloated dependencies within server-side JavaScript applications, focusing on those built with the widely used and highly dynamic CommonJS module system. We propose a trace-based dynamic analysis that monitors the OS file system, to determine which dependencies are not accessed during runtime. To evaluate our approach, we curate an original dataset of 91 CommonJS packages with a total of 50,488 dependencies. Compared to the state-of-the-art dynamic and static approaches, our trace-based analysis demonstrates higher accuracy in detecting bloated dependencies. Our analysis identifies 50.6% of the 50,488 dependencies as bloated: 13.8% of direct dependencies and 51.3% of indirect dependencies. Furthermore, removing only the direct bloated dependencies by cleaning the dependency configuration file can remove a significant share of unnecessary bloated indirect dependencies while preserving function correctness."", ""As all software, blockchain nodes are exposed to faults in their underlying execution stack. Unstable execution environments can disrupt the availability of blockchain nodes&#x0027; interfaces, resulting in downtime for users. This paper introduces the concept of N-Version Blockchain nodes. This new type of node relies on simultaneous execution of different implementations of the same blockchain protocol, in the line of Avizienis&#x0027; N-Version programming vision. We design and implement an N-Version blockchain node prototype in the context of Ethereum, called N-ETH. We show that N-ETH is able to mitigate the effects of unstable execution environments and significantly enhance availability under environment faults. To simulate unstable execution environments, we perform fault injection at the system-call level. Our results show that existing Ethereum node implementations behave asymmetrically under identical instability scenarios. N-ETH leverages this asymmetric behavior available in the diverse implementations of Ethereum nodes to provide increased availability, even under our most aggressive fault-injection strategies. We are the first to validate the relevance of N-Version design in the domain of blockchain infrastructure. From an industrial perspective, our results are of utmost importance for businesses operating blockchain nodes, including Google, ConsenSys, and many other major blockchain companies."", ""Mocking allows testing program units in isolation. A developer who writes tests with mocks faces two challenges: design realistic interactions between a unit and its environment; and understand the expected impact of these interactions on the behavior of the unit. In this paper, we propose to monitor an application in production to generate tests that mimic realistic execution scenarios through mocks. Our approach operates in three phases. First, we instrument a set of target methods for which we want to generate tests, as well as the methods that they invoke, which we refer to as mockable method calls. Second, in production, we collect data about the context in which target methods are invoked, as well as the parameters and the returned value for each mockable method call. Third, offline, we analyze the production data to generate test cases with realistic inputs and mock interactions. The approach is automated and implemented in an open-source tool called RICK. We evaluate our approach with three real-world, opensource Java applications. RICK monitors the invocation of 128 methods in production across the three applications and captures their behavior. Based on this captured data, RICK generates test cases that include realistic initial states and test inputs, as well as mocks and stubs. All the generated test cases are executable, and 52.4% of them successfully mimic the complete execution context of the target methods observed in production. The mock-based oracles are also effective at detecting regressions within the target methods, complementing each other in their fault-finding ability. We interview 5 developers from the industry who confirm the relevance of using production observations to design mocks and stubs. Our experimental findings clearly demonstrate the feasibility and added value of generating mocks from production interactions.""]"
"Claes Beckman","claesb@kth.se","Researcher","Claes Beckman, Works for: Division of Communication Systems, E-mail: claesb@kth.se, Telephone: +46 8 790 93 64, Unit address: Malvinas Väg 10","https://www.kth.se/profile/claesb?l=en","[""Theincrease in the number of cellular operating bands and theever-increasing criteria antenna size reduction has pushed the need touse switchable antenna structures. Further, the presence of a handin the close vicinity of the antenna causes deterioration inperformance. The authors present a switchable antenna technique, evaluate theeffect of the human hand on it and discuss anovel method to compensate for the mismatch. A penta-band switchableplanar inverted-F antenna covering 824–960 and 1710–2170 MHz has been usedin this study. The presence of the hand was observedto cause 2.0–6.0 dB loss in efficiency for the low band.Using the proposed extra-ground antenna switching method and the dynamicantenna matching, technique the total efficiency of the antenna canbe improved by 0.5–2.5 dB compared to the mismatched case"", ""The exact maximum likelihood estimator of the direct-to-scattered ratio of the Rician channel in a reverberation chamber is derived and its performance analysed. It is shown that the estimator obeys a non-central F-distribution, the mean value and variance of which are derived. For well stirred chambers, it is shown that the systematic error in the estimated ratio may be significant."", ""In the wake of the substantial financial commitments incurred by European and other UMTS network operators in the form of licence fees, licensees have turned to network sharing as a means of reducing the capital requirements needed to bring 3G services to market. The reception from European regulators has been mixed, due to concerns that this inhibits competition, slow buildout, or otherwise result in reduced consumer benefits. The authors believe that the generic product life cycle model provides insights that indicate that network sharing, within an appropriately constructed regulatory framework, is not a threat to vigorous competition in the 3G industry, and in fact is one of the keys to stimulating the development of advanced, ubiquitous, affordable services.""]"
"Slimane Ben Slimane","slimane@kth.se","Associate professor","Slimane Ben Slimane, Works for: Division of Communication Systems, E-mail: slimane@kth.se, Telephone: +46 8 790 93 53, Unit address: Malvinas Väg 10","https://www.kth.se/profile/slimane?l=en","[""As the standardization of network-assisted deviceto-device (D2D) communications by the 3 rd Generation Partnership Project progresses, the research community has started to explore the technology potential of new advanced features that will largely impact the performance of 5G networks. For 5G, D2D is becoming an integrative term of emerging technologies that take advantage of the proximity of communicating entities in licensed and unlicensed spectra. The European 5G research project Mobile and Wireless Communication Enablers for the 2020 Information Society (METIS) has identified advanced D2D as a key enabler for a variety of 5G services, including cellular coverage extension, social proximity and communicating vehicles. In this paper, we review the METIS D2D technology components in three key areas of proximal communications – network-assisted multi-hop, full-duplex, and multi-antenna D2D communications – and argue that the advantages of properly combining cellular and ad hoc technologies help to meet the challenges of the information society beyond 2020."", ""In recent years, the use of wireless systems in industrial applications has experienced spectacular growth. Unfortunately, industrial environments often present impulsive noise which degrades the reliability of wireless systems. OFDM is an enhanced technology used in industrial communication to monitor the work and movement of employees using high quality video. However, OFDM is sensitive to high amplitude impulsive noise because the noise energy spreads among all OFDM sub-carriers. This paper proposes a receiver structure consisting of two stages: a detector stage combining Fisher’s Quadratic discriminant and Gaussian Hypothesis techniques, and a suppression stage optimized by setting well defined thresholds. The receiver structure has been tested by simulations and measurements providing an increment in the probability of detection and improving the system performance.""]"
"Henrik Boström","bostromh@kth.se","Professor","Henrik Boström, Works for: Division of Software and Computer Systems, E-mail: bostromh@kth.se, Telephone: +46 8 790 43 06, Unit address: Kistagången 16","https://www.kth.se/profile/henbos?l=en","[""Local model-agnostic additive explanation techniques decompose the predicted output of a black-box model into additive feature importance scores. Questions have been raised about the accuracy of the produced local additive explanations. We investigate this by studying whether some of the most popular explanation techniques can accurately explain the decisions of linear additive models. We show that even though the explanations generated by these techniques are linear additives, they can fail to provide accurate explanations when explaining linear additive models. In the experiments, we measure the accuracy of additive explanations, as produced by, e.g., LIME and SHAP, along with the non-additive explanations of Local Permutation Importance (LPI) when explaining Linear and Logistic Regression and Gaussian naive Bayes models over 40 tabular datasets. We also investigate the degree to which different factors, such as the number of numerical or categorical or correlated features, the predictive performance of the black-box model, explanation sample size, similarity metric, and the pre-processing technique used on the dataset can directly affect the accuracy of local explanations."", ""Graph neural networks (GNNs) have become the standard approach for performing machine learning on graphs. Such models need large amounts of training data, however, in several graph classification and regression tasks, only limited training data is available. Unfortunately, due to the complex nature of graphs, common augmentation strategies employed in other settings, such as computer vision, do not apply to graphs. This work aims to improve the generalization ability of GNNs by increasing the size of the training set of a given problem. The new samples are generated using an iterative contrastive learning procedure that augments the dataset during the training, in a task-relevant approach, by manipulating the graph topology. The proposed approach is general, assumes no knowledge about the underlying architecture, and can thus be applied to any GNN. We provided a theoretical analysis regarding the equivalence of the proposed approach to a regularization technique. We demonstrate instances of our framework on popular GNNs, and evaluate them on several real-world benchmark graph classification datasets. The experimental results show that the proposed approach, in several cases, enhances the generalization of the underlying prediction models reaching in some datasets state-of-the-art performance."", ""Being able to understand the logic behind predictions or recommendations on the instance level is at the heart of trustworthy machine learning models. Inherently interpretable models make this possible by allowing inspection and analysis of the model itself, thus exhibiting the logic behind each prediction, while providing an opportunity to gain insights about the underlying domain. Another important criterion for trustworthiness is the model’s ability to somehow communicate a measure of confidence in every specific prediction or recommendation. Indeed, the overall goal of this paper is to produce highly informative models that combine interpretability and algorithmic confidence. For this purpose, we introduce conformal predictive distribution trees, which is a novel form of regression trees where each leaf contains a conformal predictive distribution. Using this representation language, the proposed approach allows very versatile analyses of individual leaves in the regression trees. Specifically, depending on the chosen level of detail, the leaves, in addition to the normal point predictions, can provide either cumulative distributions or prediction intervals that are guaranteed to be well-calibrated. In the empirical evaluation, the suggested conformal predictive distribution trees are compared to the well-established conformal regressors, thus demonstrating the benefits of the enhanced representation.""]"
"David Broman","dbro@kth.se","Professor","David Broman, Works for: Division of Software and Computer Systems, E-mail: dbro@kth.se, Telephone: +46 8 790 42 74, Unit address: Kistagången 16","https://www.kth.se/profile/dbro?l=en","[""Forms of synchrony can greatly simplify modeling, design, and verification of distributed systems. Thus, recent advances in clock synchronization protocols and their adoption hold promise for system design. However, these protocols synchronize the distributed clocks only within a certain tolerance, and there are transient phases while synchronization is still being achieved. Abstractions used for modeling and verification of such systems should accurately capture these imperfections that cause the system to only be “almost synchronized.” In this paper, we present approximate synchrony, a sound and tunable abstraction for verification of almost-synchronous systems. We show how approximate synchrony can be used for verification of both time synchronization protocols and applications running on top of them. We provide an algorithmic approach for constructing this abstraction for symmetric, almost-synchronous systems, a subclass of almost-synchronous systems. Moreover, we show how approximate synchrony also provides a useful strategy to guide state-space exploration. We have implemented approximate synchrony as a part of a model checker and used it to verify models of the Best Master Clock (BMC) algorithm, the core component of the IEEE 1588 precision time protocol, as well as the time-synchronized channel hopping protocol that is part of the IEEE 802.15.4e standard."", ""Forms of synchrony can greatly simplify modeling, design, and verification of distributed systems. Thus, recent advances in clock synchronization protocols and their adoption hold promise for system design. However, these protocols synchronize the distributed clocks only within a certain tolerance, and there are transient phases while synchronization is still being achieved. Abstractions used for modeling and verification of such systems should accurately capture these imperfections that cause the system to only be “almost synchronized.” In this paper, we present approximate synchrony, a sound and tunable abstraction for verification of almost-synchronous systems. We show how approximate synchrony can be used for verification of both time synchronization protocols and applications running on top of them. We provide an algorithmic approach for constructing this abstraction for symmetric, almost-synchronous systems, a subclass of almost-synchronous systems. Moreover, we show how approximate synchrony also provides a useful strategy to guide state-space exploration. We have implemented approximate synchrony as a part of a model checker and used it to verify models of the Best Master Clock (BMC) algorithm, the core component of the IEEE 1588 precision time protocol, as well as the time-synchronized channel hopping protocol that is part of the IEEE 802.15.4e standard.""]"
"Mats Brorsson","matsbror@kth.se","Professor","Mats Brorsson, Works for: Division of Software and Computer Systems, E-mail: matsbror@kth.se, Telephone: +46 8 790 41 21, Unit address: Kistagången 16","https://www.kth.se/profile/matsbror?l=en","[""As multicore systems evolve by increasing the number of parallel execution units, parallel programming models have been released to exploit parallelism in the applications. Task-based programming model uses task abstractions to specify parallel tasks and schedules tasks onto processors at runtime. In order to increase the efficiency and get the highest performance, it is required to identify which runtime configuration is needed and how processor cores must be shared among tasks. Exploring design space for all possible scheduling and runtime options, especially for large input data, becomes infeasible and requires statistical modeling. Regression-based modeling determines the effects of multiple factors on a response variable, and makes predictions based on statistical analysis. In this work, we propose a regression-based modeling approach to predict the task-based program performance for different scheduling parameters with variable data size. We execute a set of task-based programs by varying the runtime parameters, and conduct a systematic measurement for influencing factors on execution time. Our approach uses executions with different configurations for a set of input data, and derives different regression models to predict execution time for larger input data. Our results show that regression models provide accurate predictions for validation inputs with mean error rate as low as 6.3%, and 14% on average among four task-based programs."", ""In systems with complex many-core cache hierarchy, exploiting data locality can significantly reduce execution time and energy consumption of parallel applications. Locality can be exploited at various hardware and software layers. For instance, by implementing private and shared caches in a multi-level fashion, recent hardware designs are already optimised for locality. However, this would all be useless if the software scheduling does not cast the execution in a manner that promotes locality available in the programs themselves. Since programs for parallel systems consist of tasks executed simultaneously, task scheduling becomes crucial for the performance in multi-level cache architectures. This paper presents a heuristic algorithm for homogeneous multi-core systems called locality-aware task scheduling (LeTS). The LeTS heuristic is a work-conserving algorithm that takes into account both locality and load balancing in order to reduce the execution time of target applications. The working principle of LeTS is based on two distinctive phases, namely; working task group formation phase (WTG-FP) and working task group ordering phase (WTG-OP). The WTG-FP forms groups of tasks in order to capture data reuse across tasks while the WTG-OP determines an optimal order of execution for task groups that minimizes the reuse distance of shared data between tasks. We have performed experiments using randomly generated task graphs by varying three major performance parameters, namely: (1) communication to computation ratio (CCR) between 0.1 and 1.0, (2) application size, i.e., task graphs comprising of 50-, 100-, and 300-tasks per graph, and (3) number of cores with 2-, 4-, 8-, and 16-cores execution scenarios. We have also performed experiments using selected real-world applications. The LeTS heuristic reduces overall execution time of applications by exploiting inter-task data locality. Results show that LeTS outperforms state-of-the-art algorithms in amortizing inter-task communication cost.""]"
"Joel Brynielsson","joel@kth.se","Researcher","Joel Brynielsson, Works for: Division of Theoretical Computer Science, E-mail: joel@kth.se, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/joel?l=en","[""In recent years, the Swedish public sector has undergone rapid digitalization, while cybersecurity efforts have not kept even steps. This study investigates conditions for cybersecurity work at Swedish administrative authorities by examining organizational conditions at the authorities, what cybersecurity staff do to acquire the cyber situation awareness required for their role, as well as what experience cybersecurity staff have with incidents. In this study, 17 semi-structured interviews were held with respondents from Swedish administrative authorities. The results showed the diverse conditions for cybersecurity work that exist at the authorities and that a variety of roles are involved in that work. It was found that national-level support for cybersecurity was perceived as somewhat lacking. There were also challenges in getting access to information elements required for sufficient cyber situation awareness."", ""The financial sector relies heavily on information systems for business. This study sets out to investigate cyber situational awareness in the financial sector in Sweden, by examining what information elements that are needed for a common operational picture, and exploring how key actors perceive cyber-threats."", ""Simulator training is becoming increasingly important for training of time-critical and dynamic situations. Hence, how simulator training in such domains is planned, carried out and followed up becomes important. Based on a model prescribing such crucial aspects, ten decision-making training simulator facilities have been analyzed from an activity theoretical perspective. The analysis reveals several conflicts that exist between the training that is carried out and the defined training objectives. Although limitations in technology and organization are often alleviated by proficient instructors, it is concluded that there is a need for a structured approach to the design of training to be able to define the competencies and skills that ought to be trained along with relevant measurable training goals. Further, there is a need for a pedagogical model that takes the specifics of simulator training into account. Such a pedagogical model is needed to be able to evaluate the training, and would make it possible to share experiences and make comparisons between facilities in a structured manner.""]"
"Marco Chiesa","mchiesa@kth.se","Associate professor","Marco Chiesa, Works for: Division of Software and Computer Systems, E-mail: mchiesa@kth.se, Telephone: +46 8 790 44 29, Unit address: Kistagången 16","https://www.kth.se/profile/mchiesa?l=en","[""Data centers increasingly utilize commodity servers to deploy low-latency Network Functions (NFs). However, the emergence of multi-hundred-gigabit-per-second network interface cards (NICs) has drastically increased the performance expected from commodity servers. Additionally, recently introduced systems that store packet payloads in temporary off-CPU locations (e.g., programmable switches, NICs, and RDMA servers) further increase the load on NF servers, making packet processing even more challenging. This paper demonstrates existing bottlenecks and challenges of state-of-the-art stateful packet processing frameworks and proposes a system, called FAJITA, to tackle these challenges & accelerate stateful packet processing on commodity hardware. FAJITA proposes an optimized processing pipeline for stateful network functions to minimize memory accesses and overcome the overheads of accessing shared data structures while ensuring efficient batch processing at every stage of the pipeline. Furthermore, FAJITA provides a performant architecture to deploy high-performance network functions service chains containing stateful elements with different state granularities. FAJITA improves the throughput and latency of high-speed stateful network functions by ~2.43x compared to the most performant state-of-the-art solutions, enabling commodity hardware to process up to ~178 Million 64-B packets per second (pps) using 16 cores."", ""This paper explores opportunities to utilize Large Language Models (LLMs) to make network configuration human-friendly, simplifying the configuration of network devices & development of routing algorithms and minimizing errors. We design a set of benchmarks (NetConfEval) to examine the effectiveness of different models in facilitating and automating network configuration. More specifically, we focus on the scenarios where LLMs translate high-level policies, requirements, and descriptions (i.e., specified in natural language) into low-level network configurations & Python code. NetConfEval considers four tasks that could potentially facilitate network configuration, such as (i) generating high-level requirements into a formal specification format, (ii) generating API/function calls from high-level requirements, (iii) developing routing algorithms based on high-level descriptions, and (iv) generating low-level configuration for existing and new protocols based on input documentation. Learning from the results of our study, we propose a set of principles to design LLM-based systems to configure networks. Finally, we present two GPT-4-based prototypes to (i) automatically configure P4-enabled devices from a set of high-level requirements and (ii) integrate LLMs into existing network synthesizers."", ""Key-value data structures are an essential component of today's stateful packet processors such as load balancers, packet schedulers, and more. Realizing key-value data structures entirely in the data-plane of an ASIC switch would bring enormous energy savings. Yet, today's implementations are ill-suited for stateful packet processing as they support only a limited amount of flow-state insertions per second into these data structures. In this paper, we present SWITCHAROO, a mechanism for realizing key-value data structures on programmable ASIC switches that supports both high-frequency insertions and fast lookups entirely in the data plane. We show that SWITCHAROO can be realized on ASIC, supports millions of flow-state insertions per second with only limited amount of packet recirculation.""]"
"György Dán","gyuri@kth.se","Professor","György Dán, Works for: Division of Network and Systems Engineering, E-mail: gyuri@kth.se, Telephone: +46 8 790 42 53, Unit address: Teknikringen 33","https://www.kth.se/profile/gyuri?l=en","[""Fog computing is recognized as a promising approach for meeting the computational and delay requirements of a variety of emerging applications in the Internet of Things. This work presents a game theoretical treatment of the resource allocation problem in a fog computing system where wireless devices periodically generate computationally intensive tasks, and aim at minimizing their own cost."", ""Fog computing is recognized as a promising approach for meeting the computational and delay requirements of a variety of emerging applications in the Internet of Things. This work presents a game theoretical treatment of the resource allocation problem in a fog computing system where wireless devices periodically generate computationally intensive tasks, and aim at minimizing their own cost.""]"
"Mathias Ekstedt","mekstedt@kth.se","Professor","Mathias Ekstedt, Works for: Division of Network and Systems Engineering, E-mail: mekstedt@kth.se, Telephone: +46 8 790 68 67, Unit address: Teknikringen 33","https://www.kth.se/profile/mekstedt?l=en","[""Attack graphs quickly become large and challenging to understand and overview. As a means to ease this burden this paper presents an approach to introduce conceptual hierarchies of attack graphs. In this approach several attack steps are aggregated into abstract attack steps that can be given more comprehensive names. With such abstract attack graphs, it is possible to drill down, in several steps, to gain more granularity, and to move back up. The approach has been applied to the attack graphs generated by the cyber threat modeling tool securiCAD.""]"
"Carlo Fischione","carlofi@kth.se","Professor","Carlo Fischione, Works for: Division of Network and Systems Engineering, E-mail: carlofi@kth.se, Telephone: +46 73 632 25 61, Unit address: Teknikringen 33","https://www.kth.se/profile/carlofi?l=en","[""To meet the growing demands for connectivity and reliability in cellular networks, it is essential to ensure reliable quality of service (QoS) guarantees for end users. The integration of predictive QoS (pQoS) in cellular networks enables proactive fulfillment of QoS requirements for a diverse range of applications, including intelligent transportation systems. This study presents a pQoS framework in cellular networks, particularly for connected vehicles, that divides the road into segments, clusters them, and assigns a pQoS model to each cluster. By implementing this framework, we mitigate the concept drift of the pQoS model induced by variations in the propagation environment and interference. Each predictive cluster model is locally trained on vehicles traveling within the cluster boundaries using federated learning. A significant challenge is balancing the trade-off between the number of clusters, prediction accuracy, and communication overhead for updating local models. This trade-off suggests the novel problem of performing a joint optimization of the training and number of clusters. To address such difficult optimization, we propose an iterative approximate solution using proximal alternative minimization for which we provide convergence guarantees. Ultimately, by evaluations with real-world data, our numerical findings reveal that our proposed clustered predictive model reduces the mean absolute percentage error by 8%, and the mean absolute error by 7%, compared to conventional predictive approaches proposed by prior studies."", ""This paper investigates over-the-air computation (AirComp) over multiple-access time-varying channels, where devices with high mobility transmit their sensing data to a fusion center (FC) for averaging. To combat the Doppler shift induced by time-varying channels, each device adopts orthogonal time frequency space (OTFS) modulation. Our objective is minimizing the mean squared error (MSE) for the target function estimation. Due to the multipath time-varying channels, the OTFS-based AirComp not only suffers from noise but also interference. Specifically, we propose three schemes, namely S1, S2, and S3, for the target function estimation. S1 directly estimates the target function under the impacts of noise and interference. S2 mitigates the interference by introducing a zero padding-assisted OTFS. In S3, we propose an iterative algorithm to estimate the function in a matrix form. In the numerical results, we evaluate the performance of S1, S2, and S3 from the perspectives of MSE and computational complexity, and compare them with benchmarks. Specifically, compared to benchmarks, S3 outperforms them with a significantly lower MSE but incurs a higher computational complexity. In contrast, S2 demonstrates a reduction in both MSE and computational complexity. Lastly, S1 shows superior error performance at small SNR and reduced computational complexity."", ""Network digital twins (NDTs) are virtual representations of network assets and processes, such as in 5G or 6G networks, synchronized with physical properties. NDTs can be leveraged for network monitoring, automation, and optimization tasks. Simulations and predictions from the NDT can be provided to scout the feasibility of important processes from a network connectivity quality perspective. The real-time modeling of a key performance indicator (KPI) enables continuous network management. This article presents results and insights from a proof-of-concept NDT for an enterprise use case using real-time data from commercially available communication equipment. Neural networks, assisted by comprehensive feature selection and extraction, are integrated into the NDT to model the KPI, namely the downlink user throughput. KPI evaluations from the NDT are provided based on requests from the real-world generated demanding scenarios. The results provide general insights into an accurate real-time N DT that can support continuous network configuration updates.""]"
"Sarunas Girdzijauskas","sarunasg@kth.se","Professor","Sarunas Girdzijauskas, Works for: Division of Software and Computer Systems, E-mail: sarunasg@kth.se, Telephone: +46 8 790 41 75, Unit address: Kistagången 16","https://www.kth.se/profile/sarunasg?l=en","[""Decentralized Learning (DL) enables privacy-preserving collaboration among organizations or users to enhance the performance of local deep learning models. However, model aggregation becomes challenging when client data is heterogeneous, and identifying compatible collaborators without direct data exchange remains a pressing issue. In this paper, we investigate the effectiveness of various similarity metrics in DL for identifying peers for model merging, conducting an empirical analysis across multiple datasets with distribution shifts. Our research provides insights into the performance of these metrics, examining their role in facilitating effective collaboration. By exploring the strengths and limitations of these metrics, we contribute to the development of robust DL methods."", ""As the research community focuses on improving the reliability of deep learning, identifying out-of-distribution (OOD) data has become crucial. Detecting OOD inputs during test/prediction allows the model to account for discriminative features unknown to the model. This capability increases the model's reliability since this model provides a class prediction solely at incoming data similar to the training one. Although OOD detection is well-established in computer vision, it is relatively unexplored in other areas, like time series-based human activity recognition (HAR). Since uncertainty has been a critical driver for OOD in vision-based models, the same component has proven effective in time-series applications. In this work, we propose an ensemble-based temporal learning framework to address the OOD detection problem in HAR with time-series data. First, we define different types of OOD for HAR that arise from realistic scenarios. Then we apply our ensemble-based temporal learning framework incorporating uncertainty to detect OODs for the defined HAR workloads. This particular formulation also allows a novel approach to fall detection. We train our model on non-fall activities and detect falls as OOD. Our method shows state-of-The-Art performance in a fall detection task using much lesser data. Furthermore, the ensemble framework outperformed the traditional deep-learning method (our baseline) on the OOD detection task across all the other chosen datasets."", ""Ubiquitous self-tracking technologies have penetrated various aspects of our lives, from physical and mental health monitoring to fitness and entertainment. Yet, limited data exist on the association between in the wild large-scale physical activity patterns, sleep, stress, and overall health, and behavioral and psychological patterns due to challenges in collecting and releasing such datasets, including waning user engagement or privacy considerations. In this paper, we present the LifeSnaps dataset, a multi-modal, longitudinal, and geographically-distributed dataset containing a plethora of anthropological data, collected unobtrusively for the total course of more than 4 months by n = 71 participants. LifeSnaps contains more than 35 different data types from second to daily granularity, totaling more than 71 M rows of data. The participants contributed their data through validated surveys, ecological momentary assessments, and a Fitbit Sense smartwatch and consented to make these data available to empower future research. We envision that releasing this large-scale dataset of multi-modal real-world data will open novel research opportunities and potential applications in multiple disciplines.""]"
"Roberto Guanciale","robertog@kth.se","Associate professor","Roberto Guanciale, Works for: Division of Theoretical Computer Science, E-mail: robertog@kth.se, Telephone: +46 8 790 69 37, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/robertog?l=en","[""We present a tool-supported approach for the model-driven testing of message-passing applications. Our approach envisages choreographies as a particularly suited model to derive tests in order to tame the problems of correctness of distributed applications."", ""We present a tool-supported approach for the model-driven testing of message-passing applications. Our approach envisages choreographies as a particularly suited model to derive tests in order to tame the problems of correctness of distributed applications.""]"
"Dilian Gurov","dilian@kth.se","Professor","Dilian Gurov, Works for: Division of Theoretical Computer Science, E-mail: dilian@kth.se, Telephone: +46 8 790 81 98, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/dilian?l=en","[""Modern concurrent and distributed software is highly complex. Techniques to reason about the correct behaviour of such software are essential to ensure its reliability. To be able to reason about realistic programs, these techniques must be modular and compositional as well as practical by being supported by automated tools. However, many existing approaches for concurrency verification are theoretical and focus primarily on expressivity and generality. This paper contributes a technique for verifying behavioural properties of concurrent and distributed programs that balances expressivity and usability. The key idea of the approach is that program behaviour is abstractly modelled using process algebra, and analysed separately. The main difficulty is presented by the typical abstraction gap between program implementations and their models. Our approach bridges this gap by providing a deductive technique for formally linking programs with their process-algebraic models. Our verification technique is modular and compositional, is proven sound with Coq, and has been implemented in the automated concurrency verifier VerCors. Moreover, our technique is demonstrated on multiple case studies, including the verification of a leader election protocol."", ""Modern concurrent and distributed software is highly complex. Techniques to reason about the correct behaviour of such software are essential to ensure its reliability. To be able to reason about realistic programs, these techniques must be modular and compositional as well as practical by being supported by automated tools. However, many existing approaches for concurrency verification are theoretical and focus primarily on expressivity and generality. This paper contributes a technique for verifying behavioural properties of concurrent and distributed programs that balances expressivity and usability. The key idea of the approach is that program behaviour is abstractly modelled using process algebra, and analysed separately. The main difficulty is presented by the typical abstraction gap between program implementations and their models. Our approach bridges this gap by providing a deductive technique for formally linking programs with their process-algebraic models. Our verification technique is modular and compositional, is proven sound with Coq, and has been implemented in the automated concurrency verifier VerCors. Moreover, our technique is demonstrated on multiple case studies, including the verification of a leader election protocol.""]"
"Philipp Haller","phaller@kth.se","Associate professor","Philipp Haller, Works for: Division of Theoretical Computer Science, E-mail: phaller@kth.se, Telephone: +46 8 790 81 20, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/phaller?l=en","[""Implementing correct and deterministic parallel programs is challenging. Even though concurrency constructs exist in popular programming languages to facilitate the task of deterministic parallel programming, they are often too low level, or do not compose well due to underlying blocking mechanisms. In this paper, we present the design and implementation of a fundamental data structure for composable deterministic parallel dataflow computation through the use of functional programming abstractions. Additionally, we provide a correctness proof, showing that the implementation is linearizable, lock-free, and deterministic. Finally, we show experimental results which compare our FlowPool against corresponding operations on other concurrent data structures, and show that in addition to offering new capabilities, FlowPools reduce insertion time by 49 - 54% on a 4-core i7 machine with respect to comparable concurrent queue data structures in the Java standard library."", ""Implementing correct and deterministic parallel programs is challenging. Even though concurrency constructs exist in popular programming languages to facilitate the task of deterministic parallel programming, they are often too low level, or do not compose well due to underlying blocking mechanisms. In this paper, we present the design and implementation of a fundamental data structure for composable deterministic parallel dataflow computation through the use of functional programming abstractions. Additionally, we provide a correctness proof, showing that the implementation is linearizable, lock-free, and deterministic. Finally, we show experimental results which compare our FlowPool against corresponding operations on other concurrent data structures, and show that in addition to offering new capabilities, FlowPools reduce insertion time by 49 - 54% on a 4-core i7 machine with respect to comparable concurrent queue data structures in the Java standard library.""]"
"Markus Hidell","mahidell@kth.se","Associate professor","Markus Hidell, Works for: Division of Communication Systems, E-mail: mahidell@kth.se, Telephone: +46 8 790 42 67, Unit address: Malvinas Väg 10","https://www.kth.se/profile/mahidell?l=en","[""A sink is a node in a sensor network that functions as a gateway, and it gathers data from other nodes and sends the data over the Internet for further processing. However, a single sink cannot meet the demand of a sensor network for the Internet of Things (IoT) with heavy traffic. Although deploying multiple sinks can improve scalability and mitigate bottleneck problems, it is still challenging to use multiple sinks while keeping energy consumption down. Previous studies have addressed this issue using optimization techniques based on the assumption that the network converges to a steady state in terms of traffic load. We take a practical approach based on real-time changes in traffic load. This work introduces the concept of\ndynamic sinks\n, a sensor device that can serve as an on-demand sink. We identify suitable metrics for decision mechanisms to activate/deactivate dynamic sinks and investigate three decision schemes, namely\nautonomous\n,\ndelegated\n, and\ncentralized\nschemes. We also develop a protocol to disseminate the decisions. As a proof-of-concept, the dynamic sink is implemented in Contiki. Then, we evaluate trade-offs between packet delivery ratio (PDR) and energy consumption using emulated devices in the Cooja network simulator. The results show that setups with dynamic sinks can reduce energy consumption considerably at the expense of slightly lower PDR when compared to a setup with multiple permanent sinks."", ""One of the challenges in Delay Tolerant Wireless Sensor Networks (DT-WSN), is to handle situations where the available buffer space is insufficient- the buffer management problem. Although several buffer management algorithms have been proposed for DT-WSNs, to the best of our knowledge, there is no comprehensive study on the effects different factors have on their performance, and which evaluates the relative performance of these algorithms in different contexts. This paper evaluates in a fixed-factor factorial experiment the performance in terms of latency and Quality of Information (QoI) of four representative buffer management algorithms for DT-WSNs; two traditional, FiFO and Random, and two QoI-based algorithms- one proposed by Humber and Ngai and the SmartGap algorithm. The evaluation suggests that the buffer management algorithm in combination with employed routing protocol and the sensor node buffer sizes have a significant impact on latency, while the obtained QoI rather depends on the characteristics of the transported data and the routing protocol, provided a single-copy routing protocol is used. Moreover, the evaluation suggests that QoI-based buffer management algorithms do offer improved QoI, with an 31% improvement in MAE for SmartGap compared to FIFO. However, they do so at the expense of higher latency, with SmartGap giving a 60% higher latency than FIFO on average."", ""Telecommunication networks are over-provisioned with redundant resources in order to cope with traffic load during peak hours and to quickly recover from failures. However, much of the resources are underutilized during long periods of time, but still consuming full energy. With the growing concerns of energy waste and greenhouse gas emissions, the network design principles tend to shift towards allocation of resources on-demand for energy-efficiency. In this paper, we analyze and evaluate the performance of two different energy-saving techniques, namely energy saving topology control (ESTOP) and energy-efficient Ethernet (EEE). We investigate the energy-saving characteristics of ESTOP+EEE; the combination of ESTOP and EEE. The evaluation is conducted in OMNet++ with realistic and synthetic network topologies under varying traffic conditions. The results indicate that the combination has a significant potential for saving energy, compared to running ESTOP or EEE alone, but that the amount of energy savings depends on topology, traffic load, and the chosen target connectivity level for ESTOP. In particular, the results show that the target connectivity level needs to be carefully matched to the topology and the current traffic situation, suggesting that ESTOP+EEE would be suitable where the target connectivity level is dynamically adjusted according to traffic variations.""]"
"Johan Hoffman","jhoffman@kth.se","Professor","Johan Hoffman, Works for: Division of Computational Science and Technology, E-mail: jhoffman@kth.se, Telephone: +46 8 790 77 83, Unit address: Lindstedtsvägen 5","https://www.kth.se/profile/jhoffman?l=en","[""Simulations of blood flow in patient-specific models of heart ventricles is a rapidly developing field of research, showing promise to improve future treatment of heart diseases. Fluid-structure interaction simulation of the mitral valve, with its complex structure including leaflets, chordae tendineae, and papillary muscles, provides additional prospects as well as challenges to such models. In this study, we combine a patient-specific model of the left ventricle with an idealized unified continuum fluid-structure interaction model of the mitral valve, to simulate the intraventricular diastolic blood flow. To the best of our knowledge, no monolithic fluid-structure interaction model, without the need for remeshing, has ever been used before to simulate the native mitral valve within the left ventricle. The chordae tendineae are simulated as a region of porous medium, to partially hinder the flow. Simulation results from this model are compared to those of a model with the same patient-specific left ventricle, but with the mitral valve simply modeled as a time-variant inflow boundary condition. The blood flow is analyzed with the E-wave propagation index, and by use of the triple decomposition of the velocity gradient tensor, which decomposes the flow into rigid body rotational flow, shearing flow, and irrotational straining flow. The triple decomposition enables analysis of the formation of initially large dominant flow features, such as the E-wave jet and the vortex ring around it, and their subsequent decay into smaller turbulent flow structures. This analysis of the development of flow structures over the duration of diastole appears to be in general agreement with the theory of the stability of rotation, shear, and strain structures. Elevated shear levels are investigated, but are found only in limited amounts that do not indicate significant risks of thrombus formation or other blood damage, which is to be expected in this healthy ventricle. The highest shear levels are localized at the leaflets in the fluid-structure interaction model, and at the ventricle wall in the planar model. The computed E-wave propagation indices are 1.21 for the fluid-structure interaction model and 1.90 for the planar valve model, which indicates proper washout in the apical region with no significant risk of blood stasis that could lead to left ventricular thrombus formation."", ""Introduction: Aortic stiffness plays a critical role in the evolution of cardiovascular diseases, but the assessment requires specialized equipment. Photoplethysmography (PPG) and single-lead electrocardiogram (ECG) are readily available in healthcare and wearable devices. We studied whether a brief PPG registration, alone or in combination with single-lead ECG, could be used to reliably estimate aortic stiffness. Methods: A proof-of-concept study with simultaneous high-resolution index finger recordings of infrared PPG, single-lead ECG, and finger blood pressure (Finapres) was performed in 33 participants [median age 44 (range 21–66) years, 19 men] and repeated within 2 weeks. Carotid–femoral pulse wave velocity (cfPWV; two-site tonometry with SphygmoCor) was used as a reference. A brachial single-cuff oscillometric device assessed aortic pulse wave velocity (aoPWV; Arteriograph) for further comparisons. We extracted 136 established PPG waveform features and engineered 13 new with improved coupling to the finger blood pressure curve. Height-normalized pulse arrival time (NPAT) was derived using ECG. Machine learning methods were used to develop prediction models. Results: The best PPG-based models predicted cfPWV and aoPWV well (root-mean-square errors of 0.70 and 0.52 m/s, respectively), with minor improvements by adding NPAT. Repeatability and agreement were on par with the reference equipment. A new PPG feature, an amplitude ratio from the early phase of the waveform, was most important in modelling, showing strong correlations with cfPWV and aoPWV (r = −0.81 and −0.75, respectively, both P < 0.001). Conclusion: Using new features and machine learning methods, a brief finger PPG registration can estimate aortic stiffness without requiring additional information on age, anthropometry, or blood pressure. Repeatability and agreement were comparable to those obtained using non-invasive reference equipment. Provided further validation, this readily available simple method could improve cardiovascular risk evaluation, treatment, and prognosis."", ""The triple decomposition of a velocity gradient tensor provides an analysis tool in fluid mechanics by which the flow can be split into a sum of irrotational straining flow, shear flow, and rigid body rotational flow. In 2007, Kolar formulated an optimization problem to compute the triple decomposition [V. Kolar, \""Vortex identification: New requirements and limitations, \"" Int. J. Heat Fluid Flow 28, 638-652 (2007)], and more recently, the triple decomposition has been connected to the Schur form of the associated matrix. We show that the standardized real Schur form, which can be computed by state of the art linear algebra routines, is a solution to the optimization problem posed by Kolar. We also demonstrate why using the standardized variant of the real Schur form makes computation of the triple decomposition more efficient. Furthermore, we illustrate why different structures of the real Schur form correspond to different alignments of the coordinate system with the fluid flow and may, therefore, lead to differences in the resulting triple decomposition. Based on these results, we propose a new, simplified algorithm for computing the triple decomposition, which guarantees consistent results.""]"
"Hongyu Jin","hongyuj@kth.se","Researcher","Hongyu Jin, Works for: Division of Software and Computer Systems, E-mail: hongyuj@kth.se, Unit address: Kistagången 16","https://www.kth.se/profile/hongyuj?l=en","[""Standardized Vehicular Communication (VC), mainly Cooperative Awareness Messages (CAMs) and Decentralized Environmental Notification Messages (DENMs), is paramount to vehicle safety, carrying vehicle status information and reports of traffic/road-related events respectively. Broadcasted CAMs and DENMs are pseudonymously authenticated for security and privacy protection, with each node needing to have all incoming messages validated within an expiration deadline. This creates an asymmetry that can be easily exploited by external adversaries to launch a clogging Denial of Service (DoS) attack: each forged VC message forces all neighboring nodes to cryptographically validate it; at increasing rates, easy to generate forged messages gradually exhaust processing resources and severely degrade or deny timely validation of benign CAMs/DENMs. The result can be catastrophic when awareness of neighbor vehicle positions or critical reports are missed. We address this problem making the standardized VC pseudonymous authentication DoS-resilient. We propose efficient cryptographic constructs, which we term message verification facilitators, to prioritize processing resources for verification of potentially valid messages among bogus messages and verify multiple messages based on one signature verification. Any message acceptance is strictly based on public-key based message authentication/verification for accountability, i.e., non-repudiation is not sacrificed, unlike symmetric key based approaches. This further enables drastic misbehavior detection, also exploiting the newly introduced facilitators, based on probabilistic signature verification and cross-checking over multiple facilitators verifying the same message; while maintaining verification latency low even when under attack, trading off modest communication overhead. Our facilitators can also be used for efficient discovery and verification of DENM or any event-driven message, including misbehavior evidence used for our scheme. Even when vehicles are saturated by adversaries mounting a clogging DoS attack, transmitting high-rate bogus CAMs/DENMs, our scheme achieves an average 50 ms verification delay with message expiration ratio less than 1%- a huge improvement over the current standard that verifies every message signature in a First-Come First-Served (FCFS) manner and suffers from having 50% to nearly 100% of the received benign messages expiring."", ""Authenticated safety beacons in Vehicular Communication (VC) systems ensure awareness among neighboring vehicles. However, the verification of beacon signatures introduces significant processing overhead for resource-constrained vehicular On-Board Units (OBUs). Even worse in dense neighborhood or when a clogging Denial of Service (DoS) attack is mounted. The OBU would fail to verify for all received (authentic or fictitious) beacons. This could significantly delay the verifications of authentic beacons or even affect the awareness of neighboring vehicle status. In this paper, we propose an efficient cooperative beacon verification scheme leveraging efficient symmetric key based authentication on top of pseudonymous authentication (based on traditional public key cryptography), providing efficient discovery of authentic beacons among a pool of received authentic and fictitious beacons, and can significantly decrease waiting times of beacons in queue before their validations. We show with simulation results that our scheme can guarantee low waiting times for received beacons even in high neighbor density situations and under DoS attacks, under which a traditional scheme would not be workable. rights reserved."", ""Location-Based Services (LBSs) provide valuable services, with convenient features for mobile users. However, the location and other information disclosed through each query to the LBS erodes user privacy. This is a concern especially because LBS providers can be\nhonest-but-curious\n, collecting queries and tracking users’ whereabouts and infer sensitive user data. This motivated both\ncentralized\nand\ndecentralized\nlocation privacy protection schemes for LBSs: anonymizing and obfuscating LBS queries to not disclose exact information, while still getting useful responses. Decentralized schemes overcome disadvantages of centralized schemes, eliminating anonymizers, and enhancing users’ control over sensitive information. However, an insecure decentralized system could create serious risks beyond private information leakage. More so, attacking an improperly designed decentralized LBS privacy protection scheme could be an effective and low-cost step to breach user privacy. We address exactly this problem, by proposing security enhancements for mobile data sharing systems. We protect user privacy while preserving accountability of user activities, leveraging pseudonymous authentication with mainstream cryptography. We show our scheme can be deployed with off-the-shelf devices based on an experimental evaluation of an implementation in a static automotive testbed.""]"
"Pontus Johnson","pontusj@kth.se","Professor","Pontus Johnson, Works for: Division of Network and Systems Engineering, E-mail: pontusj@kth.se, Telephone: +46 8 790 68 25, Unit address: Teknikringen 33","https://www.kth.se/profile/pontusj?l=en","[""Software engineering is gravely hampered by immature practices. Specific problems include: The prevalence of fads more typical of the fashion industry than an engineering discipline; a huge number of methods and method variants, with differences little understood and artificially magnified; the lack of credible experimental evaluation and validation; and the split between industry practice and academic research."", ""Software engineering is gravely hampered by immature practices. Specific problems include: The prevalence of fads more typical of the fashion industry than an engineering discipline; a huge number of methods and method variants, with differences little understood and artificially magnified; the lack of credible experimental evaluation and validation; and the split between industry practice and academic research.""]"
"Viggo Kann","viggo@kth.se","Professor","Viggo Kann, Works for: Division of Theoretical Computer Science, E-mail: viggo@kth.se, Telephone: +46 8 790 62 92, Unit address: Lindstedtsvägen 5 Plan 5","https://www.kth.se/profile/viggo?l=en","[""Stakeholders and researchers in higher education have long debatedthe consequences of English-mediuminstruction (EMI); a key assumption of EMI isthat student’s academic learning through English should be at least as good aslearning through their first language (usually the national language). This studyaddressed the following question: “What is the impact from English-medium instructionon students’ academic performance in an online learning environment?”“Academic performance” was measured in two ways: number of correctlyanswered test questions and through-put/drop-out rate. The study adopted anexperimental design involving a large group (n = 2,263) randomized control studyin a programming course. Student participants were randomly allocated to anEnglish-medium version of the course (the intervention group) or a Swedishmediumversion of the course (the control group). The findings were that studentsenrolled on the English-medium version of the course answered statisticallysignificantly fewer test questions correctly; the EMI students also dropped outfromthe course to a statistically significantly higher degree compared to studentsenrolled on the Swedish version of the course. The conclusion of this study is thusthat EMI may, under certain circumstances, have negative consequences for students’academic performance."", ""This work spotlights the experiences from ten years of implementing sustainable development in all educational programs at a technical university. With a focus on the critical issue of involving more academics in the work, experiences are shared through an ethnographic account including focus group interviews. \""Sustainable development\"" has been perceived as both superficial and overwhelming; unclear yet somehow predetermined; it has been perceived to demand non-existent space in the curriculum; and it has challenged the academics regardless of the subjects' relatedness to sustainability. It is concluded that the evolution of a web of interconnected people, key academics, activities, norms and tools has contributed to an increased participation. The work for authenticity, reliability and feasibility, along with institution-wide and long-term academic development tools is presented."", ""Teaching assistants (TAs), students who assist the faculty, are widely used in computer science (CS) courses. Previous studies have, however, shown that TAs could be poorly prepared and need training. Particularly, an interview study has shown that one of the areas where the TAs experience uncertainty is when assessing students’ oral presentations of their lab assignments. Based on that result and by interviewing course coordinators, we have developed and offered training workshops about assessment in CS. We invited our TAs in the introductory CS courses to participate on a voluntary basis. By distributing pre-workshop surveys at the beginning of each semester, and post-workshop surveys at the end of the semesters, to both workshop attendees (50) and non-attendees (44), we studied how the TAs conducted the assessments and what impact the training workshop had on their self-reported practice.  Both surveys had 11 identical statements that the TAs were asked to rate on a 7-point Likert scale. We also conducted interviews with four workshop attendees and three non-attendees. The results showed a significant difference between the two groups in the post-workshop survey: workshop attendees disagreed more with the statement “I try to assess students' understanding rather than the program”, which is more in line with the instructions given. In addition, when comparing pre- and post-workshop answers, the workshop attendees stated that they were less inclined to ask for help, experienced that the lab instructions were not detailed enough, and were more inclined to ask questions that convinced them that the students had written the program themselves. In the control group, no significant differences between pre- and post-tests were found.""]"
"Gunnar Karlsson","gk@kth.se","Professor","Gunnar Karlsson, Works for: Division of Network and Systems Engineering, E-mail: gk@kth.se, Telephone: +46 8 790 42 57, Unit address: Teknikringen 33","https://www.kth.se/profile/gk?l=en","[""Accurate people count estimation, potentially in real-time, both for indoor and outdoor environments, is said to be of major importance in the smart cities of tomorrow. Application areas, such as public transportation, urban analytics, building automation, as well as disaster management are all expected to benefit if they were to have a better understanding of occupancy in public premises. A large body of work has been concentrated into providing people counting solutions based on images captured by surveillance cameras. However, image-based approaches are costly, as they require devoted hardware installations, and are often privacy intruding. Thus, academic and industry researchers are looking into alternative solutions for people counting. In this paper, we present a comprehensive study of non-image-based people counting techniques. Our goal with this paper is twofold: 1) to serve as an introduction to everyone interested in gaining a better understanding on non-image-based people counting techniques and 2) to serve as a guideline to practitioners interested in implementing and testing specific solutions in their everyday practice. To this end, we provide a novel classification of available approaches, and outline the requirements they need to meet. We further discuss in detail different academic solutions, and provide comparisons between them. Furthermore, we provide a discussion on available industrial approaches and compare them to academic proposals. Finally, we discuss open challenges and future directions in the field of non-image-based people counting."", ""The emerging device-to-device communication solutions and the abundance of mobile applications and services make opportunistic networking not only a feasible solution but also an important component of future wireless networks. Specifically, the distribution of locally relevant content could be based on the community of mobile users visiting an area, if long-term content survival can be ensured this way. In this article, we establish the conditions of content survival in such opportunistic networks, considering the user mobility patterns, as well as the time users keep forwarding the content, as the controllable system parameter."", ""Opportunistic networks are envisioned to complement traditional infrastructure-based communication by allowing mobile devices to communicate directly with each other when in communication range instead of via the cellular network. Due to their design, opportunistic networks are considered to be an appropriate communication means in both urban scenarios where the cellular network is overloaded, as well as in scenarios where infrastructure is not available, such as in sparsely populated areas and during disasters. However, after a decade of research, opportunistic networks have not yet been ubiquitously deployed. In this article we explore the reasons for their absence. We take a step back, and first question whether the use cases that are traditionally conjured to motivate opportunistic networking research are still relevant. We also discuss emerging applications that leverage the presence of opportunistic connectivity. Further, we look at past and current technical issues, and we investigate how upcoming technologies would influence the opportunistic networking paradigm. Finally, we outline some future directions for researchers in the field of opportunistic networking.""]"
"Dejan Kostic","dmk@kth.se","Professor","Dejan Kostic, Works for: Division of Software and Computer Systems, E-mail: dmk@kth.se, Telephone: +46 8 790 42 65, Unit address: Kistagången 16","https://www.kth.se/profile/dejanko?l=en","[""Recent control plane solutions in a software-defined network (SDN) setting assume physically distributed but logically centralized control instances: a distributed control plane (DCP). As networks become more heterogeneous with increasing amount and diversity of network resources, DCP deployment strategies must be both fast and flexible to cope with varying network conditions whilst fulfilling constraints. However, many approaches are too slow for practical applications and often address only bandwidth or delay constraints, while control-plane reliability is overlooked and control-traffic routability is not guaranteed. We demonstrate the capabilities of our optimization framework [1]-[3] for fast deployment of DCPs, guaranteeing routability in line with control service reliability, bandwidth and latency requirements. We show that our approach produces robust deployment plans under changing network conditions. Compared to state of the art solvers, our approach is magnitudes faster, enabling deployment of DCPs within minutes and seconds, rather than days and hours."", ""Recent control plane solutions in a software-defined network (SDN) setting assume physically distributed but logically centralized control instances: a distributed control plane (DCP). As networks become more heterogeneous with increasing amount and diversity of network resources, DCP deployment strategies must be both fast and flexible to cope with varying network conditions whilst fulfilling constraints. However, many approaches are too slow for practical applications and often address only bandwidth or delay constraints, while control-plane reliability is overlooked and control-traffic routability is not guaranteed. We demonstrate the capabilities of our optimization framework [1]-[3] for fast deployment of DCPs, guaranteeing routability in line with control service reliability, bandwidth and latency requirements. We show that our approach produces robust deployment plans under changing network conditions. Compared to state of the art solvers, our approach is magnitudes faster, enabling deployment of DCPs within minutes and seconds, rather than days and hours.""]"
